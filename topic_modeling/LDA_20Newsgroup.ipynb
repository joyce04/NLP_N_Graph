{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for 20Newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "dataset: http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "type(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = fetch_20newsgroups(subset='train', categories=['sci.crypt', \n",
    "                                                       'sci.electronics', \n",
    "                                                       'sci.med',\n",
    "                                                       'sci.space',\n",
    "                                                       'soc.religion.christian',\n",
    "                                                       'alt.atheism'\n",
    "                                                      ])\n",
    "train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train.data)\n",
    "train_df.columns = ['text']\n",
    "train_df['label'] = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: gerald.belton@ozonehole.com (Gerald Belton) \\nSubject: Food Dehydrators\\nDistribution: world\\nOrganization: Ozone Online Operations, Inc. - New Orleans, LA\\nReply-To: gerald.belton@ozonehole.com (Gerald Belton) \\nLines: 39\\n\\n&gt;Does anybody out there have one of those food dehydrators I've been seeing\\n&gt;all over late-night TV recently? I was wondering if they use forced air, hea\\n&gt;or both. If there's heat involved, anybody know what temperature they run at\\n&gt;My wife would like one and I'm not inclined to pay &gt;$100.00 for a box, a fan\\n&gt;and a heater. Seems to me you should be able to throw a dehydrator together\\n&gt;for just a few bucks. Heck, the technology is only what? 1,000 years old?\\n\\nYou can learn how to build a deyhdrator very easily from the book, \"The\\nHungry Hiker's Guide to Good Food,\" by Gretchen McHugh.  The heat source\\nis a 100 watt light bulb.  Basically, it's a vertical wooden box with\\nventilation holes in the top and bottom (lots of them, you want the air\\nto flow).  The light bulb goes in the bottom, and wire cake racks are\\nspaced every 6\" starting about 10\" above the bulb.\\n\\nOr, at a slightly higher cost in electricity, you can do what I do:  Use\\nyour oven.  (NOTE - I do this in an electric oven; some gas ovens may\\nnot have a low enough setting).  Put food to be dried on cookie tins or\\nracks in the oven.  Set oven to 140 degrees (the lowest setting on my\\noven - if yours goes down to 120 that's probably even better.)  Stick a\\nwooden spoon or something across the front corner of the oven and close\\nthe door on the spoon so that it stays open about an inch - this allows\\nfor airflow.  Leave the stuff in the oven for 6 to 8 hours; check it\\noften, since this dries it much faster than the dehydrator.  If you are\\nusing cookie sheets instead of racks, turn the stuff over halfway\\nthrough.\\n\\nIf you want more info, e-mail me since this isn't really the right sub\\nfor this stuff.\\n\\ngerald.belton@ozonehole.com\\n\\n\\n * SLMR 2.1 * I still miss my boss, but my aim is improving.\\n                       \\n----\\nThe Ozone Hole BBS * A Private Bulletin Board Service * (504)891-3142\\n3 Full Service Nodes * USRobotics 16.8K bps * 10 Gigs * 100,000 Files\\nSKYDIVE New Orleans! * RIME Network Mail HUB * 500+ Usenet Newsgroups\\nPlease route all questions or inquiries to:  postmaster@ozonehole.com\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: Donald Mackie &lt;Donald_Mackie@med.umich.edu&gt;\\nSubject: Re: REQUEST: Gyro (souvlaki) sauce\\nOrganization: UM Anesthesiology\\nLines: 11\\nDistribution: world\\nNNTP-Posting-Host: 141.214.86.38\\nX-UserAgent: Nuntius v1.1.1d9\\nX-XXDate: Fri, 23 Apr 93 14:56:04 GMT\\n\\nIn article &lt;1993Apr22.205341.172965@locus.com&gt; Michael Trofimoff,\\ntron@fafnir.la.locus.com writes:\\n&gt;Would anyone out there in 'net-land' happen to have an\\n&gt;authentic, sure-fire way of making this great sauce that\\n&gt;is used to adorn Gyro's and Souvlaki?\\n\\nI'm not sure of the exact recipe, but I'm sure acidophilus is one of\\nthe major ingredients.   :-)\\n\\nDon Mackie - his opinions\\nUM Anesthesiology will disavow\\n</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: bdunn@cco.caltech.edu (Brendan Dunn)\\nSubject: Re: Amusing atheists and agnostics\\nOrganization: California Institute of Technology, Pasadena\\nLines: 8\\nNNTP-Posting-Host: punisher.caltech.edu\\n\\nThanks to whoever posted this wonderful parody of people who post without \\nreading the FAQ!  I was laughing for a good 5 minutes.  Were there any \\nparts of the FAQ that weren't mentioned?  I think there might have been one\\nor two...\\n\\nPlease don't tell me this wasn't a joke.  I'm not ready to hear that yet...\\n\\nBrendan\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guncer@enuxha.eas.asu.edu (Selim Guncer )\\nSubject: Re: Islam &amp; Dress Code for women\\nOrganization: Arizona State University, Tempe, AZ\\nLines: 53\\n\\nIn article &lt;16BA7103C3.I3150101@dbstu1.rz.tu-bs.de&gt; I3150101@dbstu1.rz.tu-bs.de (Benedikt Rosenau) writes:\\n&gt;In article &lt;1993Apr5.091258.11830@monu6.cc.monash.edu.au&gt;\\n&gt;darice@yoyo.cc.monash.edu.au (Fred Rice) writes:\\n&gt; \\n&gt;&gt;(2) Do women have souls in Islam?\\n&gt;&gt;\\n&gt;&gt;People have said here that some Muslims say that women do not have\\n&gt;&gt;souls.  I must admit I have never heard of such a view being held by\\n&gt;&gt;Muslims of any era.  I have heard of some Christians of some eras\\n&gt;&gt;holding this viewpoint, but not Muslims.  Are you sure you might not be\\n&gt;&gt;confusing Christian history with Islamic history?\\n&gt;&gt;\\n&gt; \\n&gt;Yes, it is supposed to have been a predominant view in the Turkish\\n&gt;Caliphate.\\n&gt; \\n\\nI am not aware of any \"Turkish Caliphate\" viewpoint on this. Can you\\nreference?\\n\\nHowever, I found a quote due to Imam Ali, whom the Shias follow:\\n\\n\"Men, never obey your women in any way whatsoever. Never let them give their\\nadvice on any matter whatsoever, even those of everyday life. Indeed, allow\\nthem freely to give advice on anything and they will fritter away one's\\nwealth and disobey the wishes of the owner of this wealth.\\n  We see them without religion, when, alone, they are left to their own\\ndevices; they are lacking in both pity and virtue when their carnal\\ndesires are at stake. It is easy to enjoy them, but they cause great\\nanxiety. The most virtious among them are libertines. But the most\\ncorrupt are whores. Only those of them whom age has deprived of any\\ncharm are untainted by vice. They have three qualities particular to\\nmiscreants; they complain of being oppressed, whereas it is they\\nwho oppress; they make oaths, whereas they are lying; they pretend\\nto refuse men's solicitations, whereas they desire them most ardently.\\nLet us beg the help of God to emerge victorious from their evil deeds.\\nAnd preserve us in any case from their good ones.\"\\n\\n(Quote from Mas'ud al-Qanawi, ref. A. Bouhdiba, Sexuality in Islam, \\np. 118).\\n\\nI wouldn't consider this quote as being exemplary of the Islamic (TM)\\nviewpoint though.  For all we know, the prophet's cousin and\\nthe Fourth Khalif Hazret-i Ali may have said this after a frustrating \\nnight with a woman.\\n\\nSelim Guncer\\n\\n--\\nSelim E. Guncer               | Jaca negra, luna grande,\\nCSSER-ASU                     | y aceitunas en mi alforja.\\n(602)-965-4096                | Aunque sepa los caminos\\nguncer@enuxha.eas.asu.edu     | yo nunca llegare a Cordoba.. (FGL)\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: habs@panix.com (Harry Shapiro)\\nSubject: Re: The [secret] source of that announcement\\nOrganization: PANIX Public Access Unix, NYC\\nLines: 65\\n\\nIn &lt;1r1om5$c5m@slab.mtholyoke.edu&gt; jbotz@mtholyoke.edu (Jurgen Botz)\\nwrites:\\n\\n&gt;Even more interesting: the SMTP server at csrc.ncsl.nist.gov no longer\\n&gt;recognizes the 'expn' and 'vrfy' commands...\\n\\n&gt;   telnet csrc.ncsl.nist.gov smtp\\n&gt;   Trying 129.6.54.11...\\n&gt;   Connected to csrc.ncsl.nist.gov.\\n&gt;   Escape character is '^]'.\\n&gt;   220 first.org sendmail 4.1/NIST ready at Tue, 20 Apr 93 17:01:34 EDT\\n&gt;   expn clipper\\n&gt;   500 Command unrecognized\\n\\n&gt;Seems like sombody didn't like your snooping around, Marc.\\n\\nThen it is a good thing we already have this:\\n\\nThe csspub mailing list: csspab@mail-gw.ncsl.nist.gov, and address on\\nthe clipper mailing list, seems to contain basically the members of\\nthe NIST security board.\\n\\nIn addition to the names already posted, their true names are as\\nfollows:\\n\\nburrows@ecf = James Burrows a director of NIST's National Computer\\nSystems Laboratory\\n\\nmcnulty@ecf = F. Lynn McNulty an associate director for computer\\nsecurity at the National Institute of Standards and Technology's\\nComputer Systems Laboratory\\n\\nGangemi@dockmaster.ncsc.mil = Gaetano Gangemi is director of the\\nsecure systems program at Wang Laboratories Inc. He wrote: Computer\\nSecurity Basics by Deborah Russell and G. T.  Gangemi, Sr. -1991,\\nO'Reilly and Associates\\n\\nslambert@cgin.cto.citicorp.com = Sandra Lambert is vice-president of\\ninformation security at Citibank, N.A.\\n\\nlipner@mitre.org = Lipner is Mitre Corp.'s director of information\\nsystems.\\n\\ngallagher@dockmaster.ncsc.mil = Patrick Gallagher, director of the\\nNational Security Agency's National Computer Security Center and a\\nsecurity board member\\n\\nwalker@tis.com = Stephen Walker a computer security expert and\\npresident of Trusted Information Systems, Inc. in Glenwood, Md\\n\\nwillis@rand.org = Willis H.  Ware a the Rand Corp.  executive who\\nchairs the security board.\\n\\nwhitehurst@vnet.ibm.com = William Whitehurst is a security board\\nmember and director of IBM Corp.'s data security programs.\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      habs@panix.com\\nList Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      habs@panix.com\\nList Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
       "0  From: gerald.belton@ozonehole.com (Gerald Belton) \\nSubject: Food Dehydrators\\nDistribution: world\\nOrganization: Ozone Online Operations, Inc. - New Orleans, LA\\nReply-To: gerald.belton@ozonehole.com (Gerald Belton) \\nLines: 39\\n\\n>Does anybody out there have one of those food dehydrators I've been seeing\\n>all over late-night TV recently? I was wondering if they use forced air, hea\\n>or both. If there's heat involved, anybody know what temperature they run at\\n>My wife would like one and I'm not inclined to pay >$100.00 for a box, a fan\\n>and a heater. Seems to me you should be able to throw a dehydrator together\\n>for just a few bucks. Heck, the technology is only what? 1,000 years old?\\n\\nYou can learn how to build a deyhdrator very easily from the book, \"The\\nHungry Hiker's Guide to Good Food,\" by Gretchen McHugh.  The heat source\\nis a 100 watt light bulb.  Basically, it's a vertical wooden box with\\nventilation holes in the top and bottom (lots of them, you want the air\\nto flow).  The light bulb goes in the bottom, and wire cake racks are\\nspaced every 6\" starting about 10\" above the bulb.\\n\\nOr, at a slightly higher cost in electricity, you can do what I do:  Use\\nyour oven.  (NOTE - I do this in an electric oven; some gas ovens may\\nnot have a low enough setting).  Put food to be dried on cookie tins or\\nracks in the oven.  Set oven to 140 degrees (the lowest setting on my\\noven - if yours goes down to 120 that's probably even better.)  Stick a\\nwooden spoon or something across the front corner of the oven and close\\nthe door on the spoon so that it stays open about an inch - this allows\\nfor airflow.  Leave the stuff in the oven for 6 to 8 hours; check it\\noften, since this dries it much faster than the dehydrator.  If you are\\nusing cookie sheets instead of racks, turn the stuff over halfway\\nthrough.\\n\\nIf you want more info, e-mail me since this isn't really the right sub\\nfor this stuff.\\n\\ngerald.belton@ozonehole.com\\n\\n\\n * SLMR 2.1 * I still miss my boss, but my aim is improving.\\n                       \\n----\\nThe Ozone Hole BBS * A Private Bulletin Board Service * (504)891-3142\\n3 Full Service Nodes * USRobotics 16.8K bps * 10 Gigs * 100,000 Files\\nSKYDIVE New Orleans! * RIME Network Mail HUB * 500+ Usenet Newsgroups\\nPlease route all questions or inquiries to:  postmaster@ozonehole.com\\n                                                                                                                                                                                                                                                                                 \n",
       "1  From: Donald Mackie <Donald_Mackie@med.umich.edu>\\nSubject: Re: REQUEST: Gyro (souvlaki) sauce\\nOrganization: UM Anesthesiology\\nLines: 11\\nDistribution: world\\nNNTP-Posting-Host: 141.214.86.38\\nX-UserAgent: Nuntius v1.1.1d9\\nX-XXDate: Fri, 23 Apr 93 14:56:04 GMT\\n\\nIn article <1993Apr22.205341.172965@locus.com> Michael Trofimoff,\\ntron@fafnir.la.locus.com writes:\\n>Would anyone out there in 'net-land' happen to have an\\n>authentic, sure-fire way of making this great sauce that\\n>is used to adorn Gyro's and Souvlaki?\\n\\nI'm not sure of the exact recipe, but I'm sure acidophilus is one of\\nthe major ingredients.   :-)\\n\\nDon Mackie - his opinions\\nUM Anesthesiology will disavow\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "2  From: bdunn@cco.caltech.edu (Brendan Dunn)\\nSubject: Re: Amusing atheists and agnostics\\nOrganization: California Institute of Technology, Pasadena\\nLines: 8\\nNNTP-Posting-Host: punisher.caltech.edu\\n\\nThanks to whoever posted this wonderful parody of people who post without \\nreading the FAQ!  I was laughing for a good 5 minutes.  Were there any \\nparts of the FAQ that weren't mentioned?  I think there might have been one\\nor two...\\n\\nPlease don't tell me this wasn't a joke.  I'm not ready to hear that yet...\\n\\nBrendan\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "3  From: guncer@enuxha.eas.asu.edu (Selim Guncer )\\nSubject: Re: Islam & Dress Code for women\\nOrganization: Arizona State University, Tempe, AZ\\nLines: 53\\n\\nIn article <16BA7103C3.I3150101@dbstu1.rz.tu-bs.de> I3150101@dbstu1.rz.tu-bs.de (Benedikt Rosenau) writes:\\n>In article <1993Apr5.091258.11830@monu6.cc.monash.edu.au>\\n>darice@yoyo.cc.monash.edu.au (Fred Rice) writes:\\n> \\n>>(2) Do women have souls in Islam?\\n>>\\n>>People have said here that some Muslims say that women do not have\\n>>souls.  I must admit I have never heard of such a view being held by\\n>>Muslims of any era.  I have heard of some Christians of some eras\\n>>holding this viewpoint, but not Muslims.  Are you sure you might not be\\n>>confusing Christian history with Islamic history?\\n>>\\n> \\n>Yes, it is supposed to have been a predominant view in the Turkish\\n>Caliphate.\\n> \\n\\nI am not aware of any \"Turkish Caliphate\" viewpoint on this. Can you\\nreference?\\n\\nHowever, I found a quote due to Imam Ali, whom the Shias follow:\\n\\n\"Men, never obey your women in any way whatsoever. Never let them give their\\nadvice on any matter whatsoever, even those of everyday life. Indeed, allow\\nthem freely to give advice on anything and they will fritter away one's\\nwealth and disobey the wishes of the owner of this wealth.\\n  We see them without religion, when, alone, they are left to their own\\ndevices; they are lacking in both pity and virtue when their carnal\\ndesires are at stake. It is easy to enjoy them, but they cause great\\nanxiety. The most virtious among them are libertines. But the most\\ncorrupt are whores. Only those of them whom age has deprived of any\\ncharm are untainted by vice. They have three qualities particular to\\nmiscreants; they complain of being oppressed, whereas it is they\\nwho oppress; they make oaths, whereas they are lying; they pretend\\nto refuse men's solicitations, whereas they desire them most ardently.\\nLet us beg the help of God to emerge victorious from their evil deeds.\\nAnd preserve us in any case from their good ones.\"\\n\\n(Quote from Mas'ud al-Qanawi, ref. A. Bouhdiba, Sexuality in Islam, \\np. 118).\\n\\nI wouldn't consider this quote as being exemplary of the Islamic (TM)\\nviewpoint though.  For all we know, the prophet's cousin and\\nthe Fourth Khalif Hazret-i Ali may have said this after a frustrating \\nnight with a woman.\\n\\nSelim Guncer\\n\\n--\\nSelim E. Guncer               | Jaca negra, luna grande,\\nCSSER-ASU                     | y aceitunas en mi alforja.\\n(602)-965-4096                | Aunque sepa los caminos\\nguncer@enuxha.eas.asu.edu     | yo nunca llegare a Cordoba.. (FGL)\\n   \n",
       "4  From: habs@panix.com (Harry Shapiro)\\nSubject: Re: The [secret] source of that announcement\\nOrganization: PANIX Public Access Unix, NYC\\nLines: 65\\n\\nIn <1r1om5$c5m@slab.mtholyoke.edu> jbotz@mtholyoke.edu (Jurgen Botz)\\nwrites:\\n\\n>Even more interesting: the SMTP server at csrc.ncsl.nist.gov no longer\\n>recognizes the 'expn' and 'vrfy' commands...\\n\\n>   telnet csrc.ncsl.nist.gov smtp\\n>   Trying 129.6.54.11...\\n>   Connected to csrc.ncsl.nist.gov.\\n>   Escape character is '^]'.\\n>   220 first.org sendmail 4.1/NIST ready at Tue, 20 Apr 93 17:01:34 EDT\\n>   expn clipper\\n>   500 Command unrecognized\\n\\n>Seems like sombody didn't like your snooping around, Marc.\\n\\nThen it is a good thing we already have this:\\n\\nThe csspub mailing list: csspab@mail-gw.ncsl.nist.gov, and address on\\nthe clipper mailing list, seems to contain basically the members of\\nthe NIST security board.\\n\\nIn addition to the names already posted, their true names are as\\nfollows:\\n\\nburrows@ecf = James Burrows a director of NIST's National Computer\\nSystems Laboratory\\n\\nmcnulty@ecf = F. Lynn McNulty an associate director for computer\\nsecurity at the National Institute of Standards and Technology's\\nComputer Systems Laboratory\\n\\nGangemi@dockmaster.ncsc.mil = Gaetano Gangemi is director of the\\nsecure systems program at Wang Laboratories Inc. He wrote: Computer\\nSecurity Basics by Deborah Russell and G. T.  Gangemi, Sr. -1991,\\nO'Reilly and Associates\\n\\nslambert@cgin.cto.citicorp.com = Sandra Lambert is vice-president of\\ninformation security at Citibank, N.A.\\n\\nlipner@mitre.org = Lipner is Mitre Corp.'s director of information\\nsystems.\\n\\ngallagher@dockmaster.ncsc.mil = Patrick Gallagher, director of the\\nNational Security Agency's National Computer Security Center and a\\nsecurity board member\\n\\nwalker@tis.com = Stephen Walker a computer security expert and\\npresident of Trusted Information Systems, Inc. in Glenwood, Md\\n\\nwillis@rand.org = Willis H.  Ware a the Rand Corp.  executive who\\nchairs the security board.\\n\\nwhitehurst@vnet.ibm.com = William Whitehurst is a security board\\nmember and director of IBM Corp.'s data security programs.\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      habs@panix.com\\nList Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      habs@panix.com\\nList Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n                                                                                                                          \n",
       "\n",
       "   label  \n",
       "0  2      \n",
       "1  3      \n",
       "2  0      \n",
       "3  0      \n",
       "4  1      "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing\n",
    "- tokenization : \n",
    "  - split the text into sentences and then sentences into words\n",
    "  - lower case words\n",
    "  - remove punctuation\n",
    "  - remove all stopwords\n",
    "- Lemmatization : \n",
    " - verbs in past and future tenses are changed into present\n",
    "- Stemmazation :\n",
    " - words are reduced to their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/grace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: (Gerald Belton) \\nSubject: Food Dehydrators\\nDistribution: world\\nOrganization: Ozone Online Operations, Inc. - New Orleans, LA\\nReply-To: (Gerald Belton) \\nLines: 39\\n\\n&gt;Does anybody out there have one of those food dehydrators I've been seeing\\n&gt;all over late-night TV recently? I was wondering if they use forced air, hea\\n&gt;or both. If there's heat involved, anybody know what temperature they run at\\n&gt;My wife would like one and I'm not inclined to pay &gt;$100.00 for a box, a fan\\n&gt;and a heater. Seems to me you should be able to throw a dehydrator together\\n&gt;for just a few bucks. Heck, the technology is only what? 1,000 years old?\\n\\nYou can learn how to build a deyhdrator very easily from the book, \"The\\nHungry Hiker's Guide to Good Food,\" by Gretchen McHugh.  The heat source\\nis a 100 watt light bulb.  Basically, it's a vertical wooden box with\\nventilation holes in the top and bottom (lots of them, you want the air\\nto flow).  The light bulb goes in the bottom, and wire cake racks are\\nspaced every 6\" starting about 10\" above the bulb.\\n\\nOr, at a slightly higher cost in electricity, you can do what I do:  Use\\nyour oven.  (NOTE - I do this in an electric oven; some gas ovens may\\nnot have a low enough setting).  Put food to be dried on cookie tins or\\nracks in the oven.  Set oven to 140 degrees (the lowest setting on my\\noven - if yours goes down to 120 that's probably even better.)  Stick a\\nwooden spoon or something across the front corner of the oven and close\\nthe door on the spoon so that it stays open about an inch - this allows\\nfor airflow.  Leave the stuff in the oven for 6 to 8 hours; check it\\noften, since this dries it much faster than the dehydrator.  If you are\\nusing cookie sheets instead of racks, turn the stuff over halfway\\nthrough.\\n\\nIf you want more info, e-mail me since this isn't really the right sub\\nfor this stuff.\\n\\n\\n\\n * SLMR 2.1 * I still miss my boss, but my aim is improving.\\n                       \\n----\\nThe Ozone Hole BBS * A Private Bulletin Board Service * (504)891-3142\\n3 Full Service Nodes * USRobotics 16.8K bps * 10 Gigs * 100,000 Files\\nSKYDIVE New Orleans! * RIME Network Mail HUB * 500+ Usenet Newsgroups\\nPlease route all questions or inquiries to:</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: Donald Mackie Subject: Re: REQUEST: Gyro (souvlaki) sauce\\nOrganization: UM Anesthesiology\\nLines: 11\\nDistribution: world\\nNNTP-Posting-Host: 141.214.86.38\\nX-UserAgent: Nuntius v1.1.1d9\\nX-XXDate: Fri, 23 Apr 93 14:56:04 GMT\\n\\nIn article Michael Trofimoff,\\nwrites:\\n&gt;Would anyone out there in 'net-land' happen to have an\\n&gt;authentic, sure-fire way of making this great sauce that\\n&gt;is used to adorn Gyro's and Souvlaki?\\n\\nI'm not sure of the exact recipe, but I'm sure acidophilus is one of\\nthe major ingredients.   :-)\\n\\nDon Mackie - his opinions\\nUM Anesthesiology will disavow\\n</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: (Brendan Dunn)\\nSubject: Re: Amusing atheists and agnostics\\nOrganization: California Institute of Technology, Pasadena\\nLines: 8\\nNNTP-Posting-Host: punisher.caltech.edu\\n\\nThanks to whoever posted this wonderful parody of people who post without \\nreading the FAQ!  I was laughing for a good 5 minutes.  Were there any \\nparts of the FAQ that weren't mentioned?  I think there might have been one\\nor two...\\n\\nPlease don't tell me this wasn't a joke.  I'm not ready to hear that yet...\\n\\nBrendan\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: (Selim Guncer )\\nSubject: Re: Islam &amp; Dress Code for women\\nOrganization: Arizona State University, Tempe, AZ\\nLines: 53\\n\\nIn article (Benedikt Rosenau) writes:\\n&gt;In article (Fred Rice) writes:\\n&gt; \\n&gt;&gt;(2) Do women have souls in Islam?\\n&gt;&gt;\\n&gt;&gt;People have said here that some Muslims say that women do not have\\n&gt;&gt;souls.  I must admit I have never heard of such a view being held by\\n&gt;&gt;Muslims of any era.  I have heard of some Christians of some eras\\n&gt;&gt;holding this viewpoint, but not Muslims.  Are you sure you might not be\\n&gt;&gt;confusing Christian history with Islamic history?\\n&gt;&gt;\\n&gt; \\n&gt;Yes, it is supposed to have been a predominant view in the Turkish\\n&gt;Caliphate.\\n&gt; \\n\\nI am not aware of any \"Turkish Caliphate\" viewpoint on this. Can you\\nreference?\\n\\nHowever, I found a quote due to Imam Ali, whom the Shias follow:\\n\\n\"Men, never obey your women in any way whatsoever. Never let them give their\\nadvice on any matter whatsoever, even those of everyday life. Indeed, allow\\nthem freely to give advice on anything and they will fritter away one's\\nwealth and disobey the wishes of the owner of this wealth.\\n  We see them without religion, when, alone, they are left to their own\\ndevices; they are lacking in both pity and virtue when their carnal\\ndesires are at stake. It is easy to enjoy them, but they cause great\\nanxiety. The most virtious among them are libertines. But the most\\ncorrupt are whores. Only those of them whom age has deprived of any\\ncharm are untainted by vice. They have three qualities particular to\\nmiscreants; they complain of being oppressed, whereas it is they\\nwho oppress; they make oaths, whereas they are lying; they pretend\\nto refuse men's solicitations, whereas they desire them most ardently.\\nLet us beg the help of God to emerge victorious from their evil deeds.\\nAnd preserve us in any case from their good ones.\"\\n\\n(Quote from Mas'ud al-Qanawi, ref. A. Bouhdiba, Sexuality in Islam, \\np. 118).\\n\\nI wouldn't consider this quote as being exemplary of the Islamic (TM)\\nviewpoint though.  For all we know, the prophet's cousin and\\nthe Fourth Khalif Hazret-i Ali may have said this after a frustrating \\nnight with a woman.\\n\\nSelim Guncer\\n\\n--\\nSelim E. Guncer               | Jaca negra, luna grande,\\nCSSER-ASU                     | y aceitunas en mi alforja.\\n(602)-965-4096                | Aunque sepa los caminos\\n    | yo nunca llegare a Cordoba.. (FGL)\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: (Harry Shapiro)\\nSubject: Re: The [secret] source of that announcement\\nOrganization: PANIX Public Access Unix, NYC\\nLines: 65\\n\\nIn (Jurgen Botz)\\nwrites:\\n\\n&gt;Even more interesting: the SMTP server at csrc.ncsl.nist.gov no longer\\n&gt;recognizes the 'expn' and 'vrfy' commands...\\n\\n&gt;   telnet csrc.ncsl.nist.gov smtp\\n&gt;   Trying 129.6.54.11...\\n&gt;   Connected to csrc.ncsl.nist.gov.\\n&gt;   Escape character is '^]'.\\n&gt;   220 first.org sendmail 4.1/NIST ready at Tue, 20 Apr 93 17:01:34 EDT\\n&gt;   expn clipper\\n&gt;   500 Command unrecognized\\n\\n&gt;Seems like sombody didn't like your snooping around, Marc.\\n\\nThen it is a good thing we already have this:\\n\\nThe csspub mailing list: and address on\\nthe clipper mailing list, seems to contain basically the members of\\nthe NIST security board.\\n\\nIn addition to the names already posted, their true names are as\\nfollows:\\n\\n= James Burrows a director of NIST's National Computer\\nSystems Laboratory\\n\\n= F. Lynn McNulty an associate director for computer\\nsecurity at the National Institute of Standards and Technology's\\nComputer Systems Laboratory\\n\\n= Gaetano Gangemi is director of the\\nsecure systems program at Wang Laboratories Inc. He wrote: Computer\\nSecurity Basics by Deborah Russell and G. T.  Gangemi, Sr. -1991,\\nO'Reilly and Associates\\n\\n= Sandra Lambert is vice-president of\\ninformation security at Citibank, N.A.\\n\\n= Lipner is Mitre Corp.'s director of information\\nsystems.\\n\\n= Patrick Gallagher, director of the\\nNational Security Agency's National Computer Security Center and a\\nsecurity board member\\n\\n= Stephen Walker a computer security expert and\\npresident of Trusted Information Systems, Inc. in Glenwood, Md\\n\\n= Willis H.  Ware a the Rand Corp.  executive who\\nchairs the security board.\\n\\n= William Whitehurst is a security board\\nmember and director of IBM Corp.'s data security programs.\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      List Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      List Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "0  From: (Gerald Belton) \\nSubject: Food Dehydrators\\nDistribution: world\\nOrganization: Ozone Online Operations, Inc. - New Orleans, LA\\nReply-To: (Gerald Belton) \\nLines: 39\\n\\n>Does anybody out there have one of those food dehydrators I've been seeing\\n>all over late-night TV recently? I was wondering if they use forced air, hea\\n>or both. If there's heat involved, anybody know what temperature they run at\\n>My wife would like one and I'm not inclined to pay >$100.00 for a box, a fan\\n>and a heater. Seems to me you should be able to throw a dehydrator together\\n>for just a few bucks. Heck, the technology is only what? 1,000 years old?\\n\\nYou can learn how to build a deyhdrator very easily from the book, \"The\\nHungry Hiker's Guide to Good Food,\" by Gretchen McHugh.  The heat source\\nis a 100 watt light bulb.  Basically, it's a vertical wooden box with\\nventilation holes in the top and bottom (lots of them, you want the air\\nto flow).  The light bulb goes in the bottom, and wire cake racks are\\nspaced every 6\" starting about 10\" above the bulb.\\n\\nOr, at a slightly higher cost in electricity, you can do what I do:  Use\\nyour oven.  (NOTE - I do this in an electric oven; some gas ovens may\\nnot have a low enough setting).  Put food to be dried on cookie tins or\\nracks in the oven.  Set oven to 140 degrees (the lowest setting on my\\noven - if yours goes down to 120 that's probably even better.)  Stick a\\nwooden spoon or something across the front corner of the oven and close\\nthe door on the spoon so that it stays open about an inch - this allows\\nfor airflow.  Leave the stuff in the oven for 6 to 8 hours; check it\\noften, since this dries it much faster than the dehydrator.  If you are\\nusing cookie sheets instead of racks, turn the stuff over halfway\\nthrough.\\n\\nIf you want more info, e-mail me since this isn't really the right sub\\nfor this stuff.\\n\\n\\n\\n * SLMR 2.1 * I still miss my boss, but my aim is improving.\\n                       \\n----\\nThe Ozone Hole BBS * A Private Bulletin Board Service * (504)891-3142\\n3 Full Service Nodes * USRobotics 16.8K bps * 10 Gigs * 100,000 Files\\nSKYDIVE New Orleans! * RIME Network Mail HUB * 500+ Usenet Newsgroups\\nPlease route all questions or inquiries to:                                                                                                                                                                                           \n",
       "1  From: Donald Mackie Subject: Re: REQUEST: Gyro (souvlaki) sauce\\nOrganization: UM Anesthesiology\\nLines: 11\\nDistribution: world\\nNNTP-Posting-Host: 141.214.86.38\\nX-UserAgent: Nuntius v1.1.1d9\\nX-XXDate: Fri, 23 Apr 93 14:56:04 GMT\\n\\nIn article Michael Trofimoff,\\nwrites:\\n>Would anyone out there in 'net-land' happen to have an\\n>authentic, sure-fire way of making this great sauce that\\n>is used to adorn Gyro's and Souvlaki?\\n\\nI'm not sure of the exact recipe, but I'm sure acidophilus is one of\\nthe major ingredients.   :-)\\n\\nDon Mackie - his opinions\\nUM Anesthesiology will disavow\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "2  From: (Brendan Dunn)\\nSubject: Re: Amusing atheists and agnostics\\nOrganization: California Institute of Technology, Pasadena\\nLines: 8\\nNNTP-Posting-Host: punisher.caltech.edu\\n\\nThanks to whoever posted this wonderful parody of people who post without \\nreading the FAQ!  I was laughing for a good 5 minutes.  Were there any \\nparts of the FAQ that weren't mentioned?  I think there might have been one\\nor two...\\n\\nPlease don't tell me this wasn't a joke.  I'm not ready to hear that yet...\\n\\nBrendan\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "3  From: (Selim Guncer )\\nSubject: Re: Islam & Dress Code for women\\nOrganization: Arizona State University, Tempe, AZ\\nLines: 53\\n\\nIn article (Benedikt Rosenau) writes:\\n>In article (Fred Rice) writes:\\n> \\n>>(2) Do women have souls in Islam?\\n>>\\n>>People have said here that some Muslims say that women do not have\\n>>souls.  I must admit I have never heard of such a view being held by\\n>>Muslims of any era.  I have heard of some Christians of some eras\\n>>holding this viewpoint, but not Muslims.  Are you sure you might not be\\n>>confusing Christian history with Islamic history?\\n>>\\n> \\n>Yes, it is supposed to have been a predominant view in the Turkish\\n>Caliphate.\\n> \\n\\nI am not aware of any \"Turkish Caliphate\" viewpoint on this. Can you\\nreference?\\n\\nHowever, I found a quote due to Imam Ali, whom the Shias follow:\\n\\n\"Men, never obey your women in any way whatsoever. Never let them give their\\nadvice on any matter whatsoever, even those of everyday life. Indeed, allow\\nthem freely to give advice on anything and they will fritter away one's\\nwealth and disobey the wishes of the owner of this wealth.\\n  We see them without religion, when, alone, they are left to their own\\ndevices; they are lacking in both pity and virtue when their carnal\\ndesires are at stake. It is easy to enjoy them, but they cause great\\nanxiety. The most virtious among them are libertines. But the most\\ncorrupt are whores. Only those of them whom age has deprived of any\\ncharm are untainted by vice. They have three qualities particular to\\nmiscreants; they complain of being oppressed, whereas it is they\\nwho oppress; they make oaths, whereas they are lying; they pretend\\nto refuse men's solicitations, whereas they desire them most ardently.\\nLet us beg the help of God to emerge victorious from their evil deeds.\\nAnd preserve us in any case from their good ones.\"\\n\\n(Quote from Mas'ud al-Qanawi, ref. A. Bouhdiba, Sexuality in Islam, \\np. 118).\\n\\nI wouldn't consider this quote as being exemplary of the Islamic (TM)\\nviewpoint though.  For all we know, the prophet's cousin and\\nthe Fourth Khalif Hazret-i Ali may have said this after a frustrating \\nnight with a woman.\\n\\nSelim Guncer\\n\\n--\\nSelim E. Guncer               | Jaca negra, luna grande,\\nCSSER-ASU                     | y aceitunas en mi alforja.\\n(602)-965-4096                | Aunque sepa los caminos\\n    | yo nunca llegare a Cordoba.. (FGL)\\n   \n",
       "4  From: (Harry Shapiro)\\nSubject: Re: The [secret] source of that announcement\\nOrganization: PANIX Public Access Unix, NYC\\nLines: 65\\n\\nIn (Jurgen Botz)\\nwrites:\\n\\n>Even more interesting: the SMTP server at csrc.ncsl.nist.gov no longer\\n>recognizes the 'expn' and 'vrfy' commands...\\n\\n>   telnet csrc.ncsl.nist.gov smtp\\n>   Trying 129.6.54.11...\\n>   Connected to csrc.ncsl.nist.gov.\\n>   Escape character is '^]'.\\n>   220 first.org sendmail 4.1/NIST ready at Tue, 20 Apr 93 17:01:34 EDT\\n>   expn clipper\\n>   500 Command unrecognized\\n\\n>Seems like sombody didn't like your snooping around, Marc.\\n\\nThen it is a good thing we already have this:\\n\\nThe csspub mailing list: and address on\\nthe clipper mailing list, seems to contain basically the members of\\nthe NIST security board.\\n\\nIn addition to the names already posted, their true names are as\\nfollows:\\n\\n= James Burrows a director of NIST's National Computer\\nSystems Laboratory\\n\\n= F. Lynn McNulty an associate director for computer\\nsecurity at the National Institute of Standards and Technology's\\nComputer Systems Laboratory\\n\\n= Gaetano Gangemi is director of the\\nsecure systems program at Wang Laboratories Inc. He wrote: Computer\\nSecurity Basics by Deborah Russell and G. T.  Gangemi, Sr. -1991,\\nO'Reilly and Associates\\n\\n= Sandra Lambert is vice-president of\\ninformation security at Citibank, N.A.\\n\\n= Lipner is Mitre Corp.'s director of information\\nsystems.\\n\\n= Patrick Gallagher, director of the\\nNational Security Agency's National Computer Security Center and a\\nsecurity board member\\n\\n= Stephen Walker a computer security expert and\\npresident of Trusted Information Systems, Inc. in Glenwood, Md\\n\\n= Willis H.  Ware a the Rand Corp.  executive who\\nchairs the security board.\\n\\n= William Whitehurst is a security board\\nmember and director of IBM Corp.'s data security programs.\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      List Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      List Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n                                                                                                                                                                                                                                             \n",
       "\n",
       "   label  \n",
       "0  2      \n",
       "1  3      \n",
       "2  0      \n",
       "3  0      \n",
       "4  1      "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "train_df.text = train_df.text.map(lambda x: re.sub('^From:[ ()a-zA-Z.,\\n]*Subject:', '', x))\n",
    "\n",
    "#remove emails\n",
    "train_df.text = train_df.text.map(lambda x: re.sub('\\S*@\\S*\\s?', '', x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "STOP_WORDS = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True): #deacc = removes punctuations\n",
    "        if token not in STOP_WORDS and len(token)>2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result#' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 4 ms, total: 13.4 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#process the text, save the results as processed_docs\n",
    "processed_docs = pd.DataFrame()\n",
    "processed_docs = pd.concat([train_df, train_df.text.map(preprocess)], axis=1)\n",
    "processed_docs.columns = ['text', 'label', 'vect']\n",
    "processed_docs['text_vect'] = processed_docs.vect.map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>vect</th>\n",
       "      <th>text_vect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: (Gerald Belton) \\nSubject: Food Dehydrators\\nDistribution: world\\nOrganization: Ozone Online Operations, Inc. - New Orleans, LA\\nReply-To: (Gerald Belton) \\nLines: 39\\n\\n&gt;Does anybody out there have one of those food dehydrators I've been seeing\\n&gt;all over late-night TV recently? I was wondering if they use forced air, hea\\n&gt;or both. If there's heat involved, anybody know what temperature they run at\\n&gt;My wife would like one and I'm not inclined to pay &gt;$100.00 for a box, a fan\\n&gt;and a heater. Seems to me you should be able to throw a dehydrator together\\n&gt;for just a few bucks. Heck, the technology is only what? 1,000 years old?\\n\\nYou can learn how to build a deyhdrator very easily from the book, \"The\\nHungry Hiker's Guide to Good Food,\" by Gretchen McHugh.  The heat source\\nis a 100 watt light bulb.  Basically, it's a vertical wooden box with\\nventilation holes in the top and bottom (lots of them, you want the air\\nto flow).  The light bulb goes in the bottom, and wire cake racks are\\nspaced every 6\" starting about 10\" above the bulb.\\n\\nOr, at a slightly higher cost in electricity, you can do what I do:  Use\\nyour oven.  (NOTE - I do this in an electric oven; some gas ovens may\\nnot have a low enough setting).  Put food to be dried on cookie tins or\\nracks in the oven.  Set oven to 140 degrees (the lowest setting on my\\noven - if yours goes down to 120 that's probably even better.)  Stick a\\nwooden spoon or something across the front corner of the oven and close\\nthe door on the spoon so that it stays open about an inch - this allows\\nfor airflow.  Leave the stuff in the oven for 6 to 8 hours; check it\\noften, since this dries it much faster than the dehydrator.  If you are\\nusing cookie sheets instead of racks, turn the stuff over halfway\\nthrough.\\n\\nIf you want more info, e-mail me since this isn't really the right sub\\nfor this stuff.\\n\\n\\n\\n * SLMR 2.1 * I still miss my boss, but my aim is improving.\\n                       \\n----\\nThe Ozone Hole BBS * A Private Bulletin Board Service * (504)891-3142\\n3 Full Service Nodes * USRobotics 16.8K bps * 10 Gigs * 100,000 Files\\nSKYDIVE New Orleans! * RIME Network Mail HUB * 500+ Usenet Newsgroups\\nPlease route all questions or inquiries to:</td>\n",
       "      <td>2</td>\n",
       "      <td>[gerald, belton, subject, food, dehydr, distribut, world, organ, ozon, onlin, oper, new, orlean, repli, gerald, belton, line, anybodi, food, dehydr, see, late, night, recent, wonder, use, forc, air, hea, heat, involv, anybodi, know, temperatur, run, wife, like, inclin, pay, box, fan, heater, abl, throw, dehydr, buck, heck, technolog, year, old, learn, build, deyhdrat, easili, book, hungri, hiker, guid, good, food, gretchen, mchugh, heat, sourc, watt, light, bulb, basic, vertic, wooden, box, ventil, hole, lot, want, air, flow, light, bulb, go, wire, cake, rack, space, start, bulb, slight, higher, cost, electr, use, oven, note, electr, oven, gas, oven, low, set, food, ...]</td>\n",
       "      <td>gerald belton subject food dehydr distribut world organ ozon onlin oper new orlean repli gerald belton line anybodi food dehydr see late night recent wonder use forc air hea heat involv anybodi know temperatur run wife like inclin pay box fan heater abl throw dehydr buck heck technolog year old learn build deyhdrat easili book hungri hiker guid good food gretchen mchugh heat sourc watt light bulb basic vertic wooden box ventil hole lot want air flow light bulb go wire cake rack space start bulb slight higher cost electr use oven note electr oven gas oven low set food dri cooki tin rack oven set oven degre lowest set oven go probabl better stick wooden spoon corner oven close door spoon stay open inch allow airflow leav stuff oven hour check dri faster dehydr cooki sheet instead rack turn stuff halfway want info mail isn right sub stuff slmr miss boss aim improv ozon hole bbs privat bulletin board servic servic nod usrobot bps gig file skydiv new orlean rime network mail hub usenet newsgroup rout question inquiri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: Donald Mackie Subject: Re: REQUEST: Gyro (souvlaki) sauce\\nOrganization: UM Anesthesiology\\nLines: 11\\nDistribution: world\\nNNTP-Posting-Host: 141.214.86.38\\nX-UserAgent: Nuntius v1.1.1d9\\nX-XXDate: Fri, 23 Apr 93 14:56:04 GMT\\n\\nIn article Michael Trofimoff,\\nwrites:\\n&gt;Would anyone out there in 'net-land' happen to have an\\n&gt;authentic, sure-fire way of making this great sauce that\\n&gt;is used to adorn Gyro's and Souvlaki?\\n\\nI'm not sure of the exact recipe, but I'm sure acidophilus is one of\\nthe major ingredients.   :-)\\n\\nDon Mackie - his opinions\\nUM Anesthesiology will disavow\\n</td>\n",
       "      <td>3</td>\n",
       "      <td>[donald, macki, subject, request, gyro, souvlaki, sauc, organ, anesthesiolog, line, distribut, world, nntp, post, host, userag, nuntius, xxdate, fri, apr, gmt, articl, michael, trofimoff, write, net, land, happen, authent, sure, way, make, great, sauc, adorn, gyro, souvlaki, sure, exact, recip, sure, acidophilus, major, ingredi, macki, opinion, anesthesiolog, disavow]</td>\n",
       "      <td>donald macki subject request gyro souvlaki sauc organ anesthesiolog line distribut world nntp post host userag nuntius xxdate fri apr gmt articl michael trofimoff write net land happen authent sure way make great sauc adorn gyro souvlaki sure exact recip sure acidophilus major ingredi macki opinion anesthesiolog disavow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: (Brendan Dunn)\\nSubject: Re: Amusing atheists and agnostics\\nOrganization: California Institute of Technology, Pasadena\\nLines: 8\\nNNTP-Posting-Host: punisher.caltech.edu\\n\\nThanks to whoever posted this wonderful parody of people who post without \\nreading the FAQ!  I was laughing for a good 5 minutes.  Were there any \\nparts of the FAQ that weren't mentioned?  I think there might have been one\\nor two...\\n\\nPlease don't tell me this wasn't a joke.  I'm not ready to hear that yet...\\n\\nBrendan\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>[brendan, dunn, subject, amus, atheist, agnost, organ, california, institut, technolog, pasadena, line, nntp, post, host, punish, caltech, edu, thank, post, wonder, parodi, peopl, post, read, faq, laugh, good, minut, part, faq, weren, mention, think, tell, wasn, joke, readi, hear, brendan]</td>\n",
       "      <td>brendan dunn subject amus atheist agnost organ california institut technolog pasadena line nntp post host punish caltech edu thank post wonder parodi peopl post read faq laugh good minut part faq weren mention think tell wasn joke readi hear brendan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: (Selim Guncer )\\nSubject: Re: Islam &amp; Dress Code for women\\nOrganization: Arizona State University, Tempe, AZ\\nLines: 53\\n\\nIn article (Benedikt Rosenau) writes:\\n&gt;In article (Fred Rice) writes:\\n&gt; \\n&gt;&gt;(2) Do women have souls in Islam?\\n&gt;&gt;\\n&gt;&gt;People have said here that some Muslims say that women do not have\\n&gt;&gt;souls.  I must admit I have never heard of such a view being held by\\n&gt;&gt;Muslims of any era.  I have heard of some Christians of some eras\\n&gt;&gt;holding this viewpoint, but not Muslims.  Are you sure you might not be\\n&gt;&gt;confusing Christian history with Islamic history?\\n&gt;&gt;\\n&gt; \\n&gt;Yes, it is supposed to have been a predominant view in the Turkish\\n&gt;Caliphate.\\n&gt; \\n\\nI am not aware of any \"Turkish Caliphate\" viewpoint on this. Can you\\nreference?\\n\\nHowever, I found a quote due to Imam Ali, whom the Shias follow:\\n\\n\"Men, never obey your women in any way whatsoever. Never let them give their\\nadvice on any matter whatsoever, even those of everyday life. Indeed, allow\\nthem freely to give advice on anything and they will fritter away one's\\nwealth and disobey the wishes of the owner of this wealth.\\n  We see them without religion, when, alone, they are left to their own\\ndevices; they are lacking in both pity and virtue when their carnal\\ndesires are at stake. It is easy to enjoy them, but they cause great\\nanxiety. The most virtious among them are libertines. But the most\\ncorrupt are whores. Only those of them whom age has deprived of any\\ncharm are untainted by vice. They have three qualities particular to\\nmiscreants; they complain of being oppressed, whereas it is they\\nwho oppress; they make oaths, whereas they are lying; they pretend\\nto refuse men's solicitations, whereas they desire them most ardently.\\nLet us beg the help of God to emerge victorious from their evil deeds.\\nAnd preserve us in any case from their good ones.\"\\n\\n(Quote from Mas'ud al-Qanawi, ref. A. Bouhdiba, Sexuality in Islam, \\np. 118).\\n\\nI wouldn't consider this quote as being exemplary of the Islamic (TM)\\nviewpoint though.  For all we know, the prophet's cousin and\\nthe Fourth Khalif Hazret-i Ali may have said this after a frustrating \\nnight with a woman.\\n\\nSelim Guncer\\n\\n--\\nSelim E. Guncer               | Jaca negra, luna grande,\\nCSSER-ASU                     | y aceitunas en mi alforja.\\n(602)-965-4096                | Aunque sepa los caminos\\n    | yo nunca llegare a Cordoba.. (FGL)\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>[selim, guncer, subject, islam, dress, code, women, organ, arizona, state, univers, temp, line, articl, benedikt, rosenau, write, articl, fred, rice, write, women, soul, islam, peopl, say, muslim, women, soul, admit, hear, view, hold, muslim, era, hear, christian, era, hold, viewpoint, muslim, sure, confus, christian, histori, islam, histori, yes, suppos, predomin, view, turkish, caliph, awar, turkish, caliph, viewpoint, refer, quot, imam, ali, shia, follow, men, obey, women, way, whatsoev, let, advic, matter, whatsoev, everyday, life, allow, freeli, advic, fritter, away, wealth, disobey, wish, owner, wealth, religion, leav, devic, lack, piti, virtu, carnal, desir, stake, easi, enjoy, caus, great, anxieti, virtious, libertin, ...]</td>\n",
       "      <td>selim guncer subject islam dress code women organ arizona state univers temp line articl benedikt rosenau write articl fred rice write women soul islam peopl say muslim women soul admit hear view hold muslim era hear christian era hold viewpoint muslim sure confus christian histori islam histori yes suppos predomin view turkish caliph awar turkish caliph viewpoint refer quot imam ali shia follow men obey women way whatsoev let advic matter whatsoev everyday life allow freeli advic fritter away wealth disobey wish owner wealth religion leav devic lack piti virtu carnal desir stake easi enjoy caus great anxieti virtious libertin corrupt whore age depriv charm untaint vice qualiti particular miscreant complain oppress oppress oath lie pretend refus men solicit desir ardent let beg help god emerg victori evil deed preserv case good one quot mas qanawi ref bouhdiba sexual islam wouldn consid quot exemplari islam viewpoint know prophet cousin fourth khalif hazret ali say frustrat night woman selim guncer selim guncer jaca negra luna grand csser asu aceituna alforja aunqu sepa los camino nunca llegar cordoba fgl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: (Harry Shapiro)\\nSubject: Re: The [secret] source of that announcement\\nOrganization: PANIX Public Access Unix, NYC\\nLines: 65\\n\\nIn (Jurgen Botz)\\nwrites:\\n\\n&gt;Even more interesting: the SMTP server at csrc.ncsl.nist.gov no longer\\n&gt;recognizes the 'expn' and 'vrfy' commands...\\n\\n&gt;   telnet csrc.ncsl.nist.gov smtp\\n&gt;   Trying 129.6.54.11...\\n&gt;   Connected to csrc.ncsl.nist.gov.\\n&gt;   Escape character is '^]'.\\n&gt;   220 first.org sendmail 4.1/NIST ready at Tue, 20 Apr 93 17:01:34 EDT\\n&gt;   expn clipper\\n&gt;   500 Command unrecognized\\n\\n&gt;Seems like sombody didn't like your snooping around, Marc.\\n\\nThen it is a good thing we already have this:\\n\\nThe csspub mailing list: and address on\\nthe clipper mailing list, seems to contain basically the members of\\nthe NIST security board.\\n\\nIn addition to the names already posted, their true names are as\\nfollows:\\n\\n= James Burrows a director of NIST's National Computer\\nSystems Laboratory\\n\\n= F. Lynn McNulty an associate director for computer\\nsecurity at the National Institute of Standards and Technology's\\nComputer Systems Laboratory\\n\\n= Gaetano Gangemi is director of the\\nsecure systems program at Wang Laboratories Inc. He wrote: Computer\\nSecurity Basics by Deborah Russell and G. T.  Gangemi, Sr. -1991,\\nO'Reilly and Associates\\n\\n= Sandra Lambert is vice-president of\\ninformation security at Citibank, N.A.\\n\\n= Lipner is Mitre Corp.'s director of information\\nsystems.\\n\\n= Patrick Gallagher, director of the\\nNational Security Agency's National Computer Security Center and a\\nsecurity board member\\n\\n= Stephen Walker a computer security expert and\\npresident of Trusted Information Systems, Inc. in Glenwood, Md\\n\\n= Willis H.  Ware a the Rand Corp.  executive who\\nchairs the security board.\\n\\n= William Whitehurst is a security board\\nmember and director of IBM Corp.'s data security programs.\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      List Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      List Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>[harri, shapiro, subject, secret, sourc, announc, organ, panix, public, access, unix, nyc, line, jurgen, botz, write, interest, smtp, server, csrc, ncsl, nist, gov, longer, recogn, expn, vrfi, command, telnet, csrc, ncsl, nist, gov, smtp, tri, connect, csrc, ncsl, nist, gov, escap, charact, org, sendmail, nist, readi, tue, apr, edt, expn, clipper, command, unrecogn, like, sombodi, like, snoop, marc, good, thing, csspub, mail, list, address, clipper, mail, list, contain, basic, member, nist, secur, board, addit, name, post, true, name, follow, jam, burrow, director, nist, nation, system, laboratori, lynn, mcnulti, associ, director, secur, nation, institut, standard, technolog, system, laboratori, gaetano, gangemi, director, ...]</td>\n",
       "      <td>harri shapiro subject secret sourc announc organ panix public access unix nyc line jurgen botz write interest smtp server csrc ncsl nist gov longer recogn expn vrfi command telnet csrc ncsl nist gov smtp tri connect csrc ncsl nist gov escap charact org sendmail nist readi tue apr edt expn clipper command unrecogn like sombodi like snoop marc good thing csspub mail list address clipper mail list contain basic member nist secur board addit name post true name follow jam burrow director nist nation system laboratori lynn mcnulti associ director secur nation institut standard technolog system laboratori gaetano gangemi director secur system program wang laboratori write secur basic deborah russel gangemi reilli associ sandra lambert vice presid inform secur citibank lipner mitr corp director inform system patrick gallagh director nation secur agenc nation secur center secur board member stephen walker secur expert presid trust inform system glenwood willi ware rand corp execut chair secur board william whitehurst secur board member director ibm corp data secur program harri shapiro list administr extropi institut mail list privat communic extropian communiti harri shapiro list administr extropi institut mail list privat communic extropian communiti</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
       "0  From: (Gerald Belton) \\nSubject: Food Dehydrators\\nDistribution: world\\nOrganization: Ozone Online Operations, Inc. - New Orleans, LA\\nReply-To: (Gerald Belton) \\nLines: 39\\n\\n>Does anybody out there have one of those food dehydrators I've been seeing\\n>all over late-night TV recently? I was wondering if they use forced air, hea\\n>or both. If there's heat involved, anybody know what temperature they run at\\n>My wife would like one and I'm not inclined to pay >$100.00 for a box, a fan\\n>and a heater. Seems to me you should be able to throw a dehydrator together\\n>for just a few bucks. Heck, the technology is only what? 1,000 years old?\\n\\nYou can learn how to build a deyhdrator very easily from the book, \"The\\nHungry Hiker's Guide to Good Food,\" by Gretchen McHugh.  The heat source\\nis a 100 watt light bulb.  Basically, it's a vertical wooden box with\\nventilation holes in the top and bottom (lots of them, you want the air\\nto flow).  The light bulb goes in the bottom, and wire cake racks are\\nspaced every 6\" starting about 10\" above the bulb.\\n\\nOr, at a slightly higher cost in electricity, you can do what I do:  Use\\nyour oven.  (NOTE - I do this in an electric oven; some gas ovens may\\nnot have a low enough setting).  Put food to be dried on cookie tins or\\nracks in the oven.  Set oven to 140 degrees (the lowest setting on my\\noven - if yours goes down to 120 that's probably even better.)  Stick a\\nwooden spoon or something across the front corner of the oven and close\\nthe door on the spoon so that it stays open about an inch - this allows\\nfor airflow.  Leave the stuff in the oven for 6 to 8 hours; check it\\noften, since this dries it much faster than the dehydrator.  If you are\\nusing cookie sheets instead of racks, turn the stuff over halfway\\nthrough.\\n\\nIf you want more info, e-mail me since this isn't really the right sub\\nfor this stuff.\\n\\n\\n\\n * SLMR 2.1 * I still miss my boss, but my aim is improving.\\n                       \\n----\\nThe Ozone Hole BBS * A Private Bulletin Board Service * (504)891-3142\\n3 Full Service Nodes * USRobotics 16.8K bps * 10 Gigs * 100,000 Files\\nSKYDIVE New Orleans! * RIME Network Mail HUB * 500+ Usenet Newsgroups\\nPlease route all questions or inquiries to:                                                                                                                                                                                           \n",
       "1  From: Donald Mackie Subject: Re: REQUEST: Gyro (souvlaki) sauce\\nOrganization: UM Anesthesiology\\nLines: 11\\nDistribution: world\\nNNTP-Posting-Host: 141.214.86.38\\nX-UserAgent: Nuntius v1.1.1d9\\nX-XXDate: Fri, 23 Apr 93 14:56:04 GMT\\n\\nIn article Michael Trofimoff,\\nwrites:\\n>Would anyone out there in 'net-land' happen to have an\\n>authentic, sure-fire way of making this great sauce that\\n>is used to adorn Gyro's and Souvlaki?\\n\\nI'm not sure of the exact recipe, but I'm sure acidophilus is one of\\nthe major ingredients.   :-)\\n\\nDon Mackie - his opinions\\nUM Anesthesiology will disavow\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "2  From: (Brendan Dunn)\\nSubject: Re: Amusing atheists and agnostics\\nOrganization: California Institute of Technology, Pasadena\\nLines: 8\\nNNTP-Posting-Host: punisher.caltech.edu\\n\\nThanks to whoever posted this wonderful parody of people who post without \\nreading the FAQ!  I was laughing for a good 5 minutes.  Were there any \\nparts of the FAQ that weren't mentioned?  I think there might have been one\\nor two...\\n\\nPlease don't tell me this wasn't a joke.  I'm not ready to hear that yet...\\n\\nBrendan\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "3  From: (Selim Guncer )\\nSubject: Re: Islam & Dress Code for women\\nOrganization: Arizona State University, Tempe, AZ\\nLines: 53\\n\\nIn article (Benedikt Rosenau) writes:\\n>In article (Fred Rice) writes:\\n> \\n>>(2) Do women have souls in Islam?\\n>>\\n>>People have said here that some Muslims say that women do not have\\n>>souls.  I must admit I have never heard of such a view being held by\\n>>Muslims of any era.  I have heard of some Christians of some eras\\n>>holding this viewpoint, but not Muslims.  Are you sure you might not be\\n>>confusing Christian history with Islamic history?\\n>>\\n> \\n>Yes, it is supposed to have been a predominant view in the Turkish\\n>Caliphate.\\n> \\n\\nI am not aware of any \"Turkish Caliphate\" viewpoint on this. Can you\\nreference?\\n\\nHowever, I found a quote due to Imam Ali, whom the Shias follow:\\n\\n\"Men, never obey your women in any way whatsoever. Never let them give their\\nadvice on any matter whatsoever, even those of everyday life. Indeed, allow\\nthem freely to give advice on anything and they will fritter away one's\\nwealth and disobey the wishes of the owner of this wealth.\\n  We see them without religion, when, alone, they are left to their own\\ndevices; they are lacking in both pity and virtue when their carnal\\ndesires are at stake. It is easy to enjoy them, but they cause great\\nanxiety. The most virtious among them are libertines. But the most\\ncorrupt are whores. Only those of them whom age has deprived of any\\ncharm are untainted by vice. They have three qualities particular to\\nmiscreants; they complain of being oppressed, whereas it is they\\nwho oppress; they make oaths, whereas they are lying; they pretend\\nto refuse men's solicitations, whereas they desire them most ardently.\\nLet us beg the help of God to emerge victorious from their evil deeds.\\nAnd preserve us in any case from their good ones.\"\\n\\n(Quote from Mas'ud al-Qanawi, ref. A. Bouhdiba, Sexuality in Islam, \\np. 118).\\n\\nI wouldn't consider this quote as being exemplary of the Islamic (TM)\\nviewpoint though.  For all we know, the prophet's cousin and\\nthe Fourth Khalif Hazret-i Ali may have said this after a frustrating \\nnight with a woman.\\n\\nSelim Guncer\\n\\n--\\nSelim E. Guncer               | Jaca negra, luna grande,\\nCSSER-ASU                     | y aceitunas en mi alforja.\\n(602)-965-4096                | Aunque sepa los caminos\\n    | yo nunca llegare a Cordoba.. (FGL)\\n   \n",
       "4  From: (Harry Shapiro)\\nSubject: Re: The [secret] source of that announcement\\nOrganization: PANIX Public Access Unix, NYC\\nLines: 65\\n\\nIn (Jurgen Botz)\\nwrites:\\n\\n>Even more interesting: the SMTP server at csrc.ncsl.nist.gov no longer\\n>recognizes the 'expn' and 'vrfy' commands...\\n\\n>   telnet csrc.ncsl.nist.gov smtp\\n>   Trying 129.6.54.11...\\n>   Connected to csrc.ncsl.nist.gov.\\n>   Escape character is '^]'.\\n>   220 first.org sendmail 4.1/NIST ready at Tue, 20 Apr 93 17:01:34 EDT\\n>   expn clipper\\n>   500 Command unrecognized\\n\\n>Seems like sombody didn't like your snooping around, Marc.\\n\\nThen it is a good thing we already have this:\\n\\nThe csspub mailing list: and address on\\nthe clipper mailing list, seems to contain basically the members of\\nthe NIST security board.\\n\\nIn addition to the names already posted, their true names are as\\nfollows:\\n\\n= James Burrows a director of NIST's National Computer\\nSystems Laboratory\\n\\n= F. Lynn McNulty an associate director for computer\\nsecurity at the National Institute of Standards and Technology's\\nComputer Systems Laboratory\\n\\n= Gaetano Gangemi is director of the\\nsecure systems program at Wang Laboratories Inc. He wrote: Computer\\nSecurity Basics by Deborah Russell and G. T.  Gangemi, Sr. -1991,\\nO'Reilly and Associates\\n\\n= Sandra Lambert is vice-president of\\ninformation security at Citibank, N.A.\\n\\n= Lipner is Mitre Corp.'s director of information\\nsystems.\\n\\n= Patrick Gallagher, director of the\\nNational Security Agency's National Computer Security Center and a\\nsecurity board member\\n\\n= Stephen Walker a computer security expert and\\npresident of Trusted Information Systems, Inc. in Glenwood, Md\\n\\n= Willis H.  Ware a the Rand Corp.  executive who\\nchairs the security board.\\n\\n= William Whitehurst is a security board\\nmember and director of IBM Corp.'s data security programs.\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      List Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n\\n-- \\nHarry Shapiro  \\t\\t\\t\\t      List Administrator of the Extropy Institute Mailing List\\nPrivate Communication for the Extropian Community since 1991\\n                                                                                                                                                                                                                                             \n",
       "\n",
       "   label  \\\n",
       "0  2       \n",
       "1  3       \n",
       "2  0       \n",
       "3  0       \n",
       "4  1       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   vect  \\\n",
       "0  [gerald, belton, subject, food, dehydr, distribut, world, organ, ozon, onlin, oper, new, orlean, repli, gerald, belton, line, anybodi, food, dehydr, see, late, night, recent, wonder, use, forc, air, hea, heat, involv, anybodi, know, temperatur, run, wife, like, inclin, pay, box, fan, heater, abl, throw, dehydr, buck, heck, technolog, year, old, learn, build, deyhdrat, easili, book, hungri, hiker, guid, good, food, gretchen, mchugh, heat, sourc, watt, light, bulb, basic, vertic, wooden, box, ventil, hole, lot, want, air, flow, light, bulb, go, wire, cake, rack, space, start, bulb, slight, higher, cost, electr, use, oven, note, electr, oven, gas, oven, low, set, food, ...]                                                                \n",
       "1  [donald, macki, subject, request, gyro, souvlaki, sauc, organ, anesthesiolog, line, distribut, world, nntp, post, host, userag, nuntius, xxdate, fri, apr, gmt, articl, michael, trofimoff, write, net, land, happen, authent, sure, way, make, great, sauc, adorn, gyro, souvlaki, sure, exact, recip, sure, acidophilus, major, ingredi, macki, opinion, anesthesiolog, disavow]                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "2  [brendan, dunn, subject, amus, atheist, agnost, organ, california, institut, technolog, pasadena, line, nntp, post, host, punish, caltech, edu, thank, post, wonder, parodi, peopl, post, read, faq, laugh, good, minut, part, faq, weren, mention, think, tell, wasn, joke, readi, hear, brendan]                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "3  [selim, guncer, subject, islam, dress, code, women, organ, arizona, state, univers, temp, line, articl, benedikt, rosenau, write, articl, fred, rice, write, women, soul, islam, peopl, say, muslim, women, soul, admit, hear, view, hold, muslim, era, hear, christian, era, hold, viewpoint, muslim, sure, confus, christian, histori, islam, histori, yes, suppos, predomin, view, turkish, caliph, awar, turkish, caliph, viewpoint, refer, quot, imam, ali, shia, follow, men, obey, women, way, whatsoev, let, advic, matter, whatsoev, everyday, life, allow, freeli, advic, fritter, away, wealth, disobey, wish, owner, wealth, religion, leav, devic, lack, piti, virtu, carnal, desir, stake, easi, enjoy, caus, great, anxieti, virtious, libertin, ...]   \n",
       "4  [harri, shapiro, subject, secret, sourc, announc, organ, panix, public, access, unix, nyc, line, jurgen, botz, write, interest, smtp, server, csrc, ncsl, nist, gov, longer, recogn, expn, vrfi, command, telnet, csrc, ncsl, nist, gov, smtp, tri, connect, csrc, ncsl, nist, gov, escap, charact, org, sendmail, nist, readi, tue, apr, edt, expn, clipper, command, unrecogn, like, sombodi, like, snoop, marc, good, thing, csspub, mail, list, address, clipper, mail, list, contain, basic, member, nist, secur, board, addit, name, post, true, name, follow, jam, burrow, director, nist, nation, system, laboratori, lynn, mcnulti, associ, director, secur, nation, institut, standard, technolog, system, laboratori, gaetano, gangemi, director, ...]      \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text_vect  \n",
       "0  gerald belton subject food dehydr distribut world organ ozon onlin oper new orlean repli gerald belton line anybodi food dehydr see late night recent wonder use forc air hea heat involv anybodi know temperatur run wife like inclin pay box fan heater abl throw dehydr buck heck technolog year old learn build deyhdrat easili book hungri hiker guid good food gretchen mchugh heat sourc watt light bulb basic vertic wooden box ventil hole lot want air flow light bulb go wire cake rack space start bulb slight higher cost electr use oven note electr oven gas oven low set food dri cooki tin rack oven set oven degre lowest set oven go probabl better stick wooden spoon corner oven close door spoon stay open inch allow airflow leav stuff oven hour check dri faster dehydr cooki sheet instead rack turn stuff halfway want info mail isn right sub stuff slmr miss boss aim improv ozon hole bbs privat bulletin board servic servic nod usrobot bps gig file skydiv new orlean rime network mail hub usenet newsgroup rout question inquiri                                                                                                                                                                                                                                               \n",
       "1  donald macki subject request gyro souvlaki sauc organ anesthesiolog line distribut world nntp post host userag nuntius xxdate fri apr gmt articl michael trofimoff write net land happen authent sure way make great sauc adorn gyro souvlaki sure exact recip sure acidophilus major ingredi macki opinion anesthesiolog disavow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2  brendan dunn subject amus atheist agnost organ california institut technolog pasadena line nntp post host punish caltech edu thank post wonder parodi peopl post read faq laugh good minut part faq weren mention think tell wasn joke readi hear brendan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "3  selim guncer subject islam dress code women organ arizona state univers temp line articl benedikt rosenau write articl fred rice write women soul islam peopl say muslim women soul admit hear view hold muslim era hear christian era hold viewpoint muslim sure confus christian histori islam histori yes suppos predomin view turkish caliph awar turkish caliph viewpoint refer quot imam ali shia follow men obey women way whatsoev let advic matter whatsoev everyday life allow freeli advic fritter away wealth disobey wish owner wealth religion leav devic lack piti virtu carnal desir stake easi enjoy caus great anxieti virtious libertin corrupt whore age depriv charm untaint vice qualiti particular miscreant complain oppress oppress oath lie pretend refus men solicit desir ardent let beg help god emerg victori evil deed preserv case good one quot mas qanawi ref bouhdiba sexual islam wouldn consid quot exemplari islam viewpoint know prophet cousin fourth khalif hazret ali say frustrat night woman selim guncer selim guncer jaca negra luna grand csser asu aceituna alforja aunqu sepa los camino nunca llegar cordoba fgl                                                                                                                                                \n",
       "4  harri shapiro subject secret sourc announc organ panix public access unix nyc line jurgen botz write interest smtp server csrc ncsl nist gov longer recogn expn vrfi command telnet csrc ncsl nist gov smtp tri connect csrc ncsl nist gov escap charact org sendmail nist readi tue apr edt expn clipper command unrecogn like sombodi like snoop marc good thing csspub mail list address clipper mail list contain basic member nist secur board addit name post true name follow jam burrow director nist nation system laboratori lynn mcnulti associ director secur nation institut standard technolog system laboratori gaetano gangemi director secur system program wang laboratori write secur basic deborah russel gangemi reilli associ sandra lambert vice presid inform secur citibank lipner mitr corp director inform system patrick gallagh director nation secur agenc nation secur center secur board member stephen walker secur expert presid trust inform system glenwood willi ware rand corp execut chair secur board william whitehurst secur board member director ibm corp data secur program harri shapiro list administr extropi institut mail list privat communic extropian communiti harri shapiro list administr extropi institut mail list privat communic extropian communiti  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words on the data set\n",
    "- create a dictionary from 'processed_docs' containing the number of times a word appears in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the dictionary 24695\n",
      "size of the dictionary 24692\n"
     ]
    }
   ],
   "source": [
    "dictionary_made_by = processed_docs.vect\n",
    "dictionary_made_by_str = 'unigram'\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(dictionary_made_by)\n",
    "print('size of the dictionary %d' %len(dictionary))\n",
    "dictionary.filter_extremes(no_below=0.1, no_above=0.8)#, keep_n=dict_size)\n",
    "dictionary.compactify()\n",
    "print('size of the dictionary %d' %len(dictionary))\n",
    "max_dic_size = len(dictionary)\n",
    "dict_size = max_dic_size\n",
    "# dict_size = [6000]\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in dictionary_made_by]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "- tf-idf model on bow_corpus\n",
    "\n",
    "- tf = count(word, document) / len(document)\n",
    "- idf = log( len(collection) / count(document_containing_term, collection)\n",
    "- tf-idf = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3452, 24611)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=dict_size, stop_words='english')\n",
    "vectors = vectorizer.fit_transform(processed_docs.text_vect)\n",
    "vectors_features = vectorizer.get_feature_names()\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal number of topics\n",
    "- build many LDA models with different values of number of topics and pick the one that gives the highest coherence value based on corpus\n",
    "- 토픽 모델링 결과로 나온 주제들에 대해 각각의 주제에서 상위 N개의 단어를 뽑습니다. 모델링이 잘 되었을수록 한 주제 안에는 의미론적으로 유사한 단어가 많이 모여있게 마련입니다. 따라서 상위 단어 간의 유사도를 계산하면 실제로 해당 주제가 의미론적으로 일치하는 단어들끼리 모여있는지 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "trinomi mov dirk sedat sturm oxaprozin daypro junghann van gilligan\n",
      "Topic 1:\n",
      "write god line subject organ articl know post key peopl\n",
      "Log Likelihood:  -238127.07703714422\n",
      "Perplexity:  15403.781113643492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "oliveira egalon claudio trinomi mov tahiti dirk keesler junghann gilligan\n",
      "Topic 1:\n",
      "write god line subject organ articl know post key peopl\n",
      "Topic 2:\n",
      "fujitsu choler smd daypro oxaprozin sturm hfe vce kiria dopler\n",
      "Log Likelihood:  -241419.74212465156\n",
      "Perplexity:  17600.74714455263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "trinomi oxaprozin daypro scand malzbend rainer naproxin gms irreduc zbl\n",
      "Topic 1:\n",
      "write god line subject organ articl know post key peopl\n",
      "Topic 2:\n",
      "fujitsu smd speculum mwra eyepiec idc lausann powel hdd corwin\n",
      "Topic 3:\n",
      "egalon oliveira claudio choler mov sedat tahiti dirk keesler hfe\n",
      "Log Likelihood:  -244164.48863034399\n",
      "Perplexity:  19669.772927471946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "hfe vce malzbend rainer blockley umc murdoch vcbo icbo vcb\n",
      "Topic 1:\n",
      "write god line subject organ articl know post key peopl\n",
      "Topic 2:\n",
      "astemizol antihistamin choler fujitsu smd mwra idc drowsi fujita neurotic\n",
      "Topic 3:\n",
      "souvlaki paxil trinomi hunchback adorn trofimoff ebcdic scand faber yoghurt\n",
      "Topic 4:\n",
      "gehrel egalon oliveira claudio jupit brader radii mov perijov sedat\n",
      "Log Likelihood:  -246600.9540336643\n",
      "Perplexity:  21709.321738780367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "radii perijov hunchback ryukoku seta lindgren msged apoapsi periapsi coe\n",
      "Topic 1:\n",
      "vasomotor rhiniti claar trinomi knbr scand hprtnyc melittin decongest ccr\n",
      "Topic 2:\n",
      "write god line subject organ articl know post key peopl\n",
      "Topic 3:\n",
      "choler putz rolla ebcdic gilligan fujita neurotic phlegmat howel lausann\n",
      "Topic 4:\n",
      "egalon oliveira claudio mov landi tahiti dirk keesler grino gia\n",
      "Topic 5:\n",
      "hfe vce icbo vcb vcbo vceo angeben ccb cbo cob\n",
      "Log Likelihood:  -248753.9341503511\n",
      "Perplexity:  23686.88117055205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "venido traer paxil reprehensit gop catechist tenei vulgata mateo pensar\n",
      "Topic 1:\n",
      "foreskin choler speculum eyepiec moloney geoffm gia dopler kiria ebcdic\n",
      "Topic 2:\n",
      "write god line subject organ articl know post key peopl\n",
      "Topic 3:\n",
      "trinomi oxaprozin daypro anello tia bloodcount lindgren scand mtv thermocoupl\n",
      "Topic 4:\n",
      "egalon oliveira claudio ovul bidder putz tahiti hunchback dirk keesler\n",
      "Topic 5:\n",
      "burzynski claar knbr landi hfe seta ryukoku vce antineoplaston faber\n",
      "Topic 6:\n",
      "arcad mov lige hap hensley nextwork tra omg ecc sma\n",
      "Log Likelihood:  -250791.05088513353\n",
      "Perplexity:  25723.613218327275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "putz rolla clator blockley umc murdoch umr cpk dickhead butler\n",
      "Topic 1:\n",
      "key space chip encrypt line clipper organ subject post use\n",
      "Topic 2:\n",
      "trinomi scand het een landau jess irreduc slit dorsal davenport\n",
      "Topic 3:\n",
      "god christian write peopl think know articl believ subject line\n",
      "Topic 4:\n",
      "claar knbr dirk grino andr junghann realidad brandt fishman hprtnyc\n",
      "Topic 5:\n",
      "fujitsu smd idc powel hdd rainer malzbend microdisk scsi fyzzick\n",
      "Topic 6:\n",
      "ovul ebcdic neurotic fujita fertilz mccrae melenchol costa extravers mbti\n",
      "Topic 7:\n",
      "fpu souvlaki vasomotor gyro rhiniti calgari speculum keesler daypro oxaprozin\n",
      "Log Likelihood:  -255343.6343743048\n",
      "Perplexity:  30930.878987895918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "tron squarewav malzbend rainer fyzzick fettron fetron newest zation hpfcmgw\n",
      "Topic 1:\n",
      "write god line subject organ articl know post key peopl\n",
      "Topic 2:\n",
      "phobo computrac sedat tranquil ebcdic sacramento pix het deimo larouch\n",
      "Topic 3:\n",
      "claar knbr keesler sturm mtv loftus stansberi kubriek faber hprtnyc\n",
      "Topic 4:\n",
      "fujitsu trinomi landi smd dirk idc junghann lindgren scand powel\n",
      "Topic 5:\n",
      "gehrel jupit brader radii perijov softquad hfe inconst peri vce\n",
      "Topic 6:\n",
      "souvlaki ovul choler paxil hunchback oxaprozin daypro adorn trofimoff sacrum\n",
      "Topic 7:\n",
      "seta ryukoku nigel debugg sarchoidosi deseas bournemouth hoi vacant glaucoma\n",
      "Topic 8:\n",
      "putz lige grino rolla oven realidad hensley nextwork sma ecc\n",
      "Log Likelihood:  -254280.50834594318\n",
      "Perplexity:  29627.59943352082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "num_topics = [i for i in range(2, 10)]\n",
    "for n in num_topics:\n",
    "    lda = LatentDirichletAllocation(n_topics=n, max_iter=10, learning_decay=0.7,\n",
    "                                    learning_method='online', learning_offset=10.,random_state=0).fit(vectors)\n",
    "    \n",
    "    display_topics(lda, vectors_features, no_top_words)\n",
    "    \n",
    "    # Log Likelyhood: Higher the better\n",
    "    print(\"Log Likelihood: \", lda.score(vectors))\n",
    "    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "    print(\"Perplexity: \", lda.perplexity(vectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=8,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_components': [4, 5, 6, 7, 8, 9], 'learning_decay': [0.5, 0.7, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearch\n",
    "\n",
    "search_params = {'n_components': [i for i in range(4, 10)], 'learning_decay': [.5, .7, .9]}\n",
    "lda = LatentDirichletAllocation(n_jobs=4)\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.9, 'n_components': 4}\n",
      "Best Log Likelihood Score:  -89217.57893561426\n",
      "Model Perplexity:  19655.11673157945\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext- out of vocabulary, spelling mistake처리에 용이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'interface', 'computer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['human', 'interface', 'computer']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "print(common_texts[0])\n",
    "['human', 'interface', 'computer']\n",
    "\n",
    "# model = FastText(size=len(processed_docs.text_vect.tolist()), window=3, min_count=1)  # instantiate\n",
    "# model.build_vocab(sentences=processed_docs.text_vect.tolist())\n",
    "# model.train(sentences=processed_docs.text_vect.tolist(), total_examples=len(processed_docs.text_vect.tolist()), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 45.2% 433.2/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================--------------------------] 48.2% 462.1/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================-------------------------] 51.4% 492.7/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================-----------------------] 54.8% 525.2/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================---------------------] 58.2% 558.0/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 61.4% 588.1/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================------------------] 64.8% 621.2/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 90.1% 863.7/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================================----] 93.3% 893.9/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================================--] 96.6% 925.7/958.4MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#https://github.com/RaRe-Technologies/gensim-data\n",
    "# word_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
    "word_vectors = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# model = Word2Vec(processed_docs.text_vect.tolist(), size=10000, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastText import load_model\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from numpy.linalg import eig\n",
    "\n",
    "word_appearance = Counter(chain.from_iterable(processed_docs.vect))\n",
    "word_df = pd.DataFrame(list(word_appearance.items()), columns=['word', 'occurence'])\n",
    "word_df['p_occurence']  = word_df.occurence / word_df.occurence.sum()\n",
    "p_dict = {r['word']: r['p_occurence'] for _, r in word_df.iterrows()}\n",
    "\n",
    "def get_sentence_vector(ls, a=10e-4):\n",
    "    \"\"\"Average word vector for given list of words using fastText\n",
    "\n",
    "    ref: A Simple but Tough-to-Beat Baseline for Sentence Embeddings, ICLR 2017\n",
    "    https://openreview.net/pdf?id=SyK00v5xx\n",
    "    \"\"\"\n",
    "    wvs = []\n",
    "    for w in ls:\n",
    "        try:\n",
    "            wv = word_vectors.wv[w]\n",
    "#             wv = model.wv[w]\n",
    "        except KeyError:\n",
    "            wv = 0\n",
    "        if w is not None:\n",
    "            wvs.append([wv, p_dict[w]])\n",
    "    wvs = np.array(wvs)\n",
    "    return np.mean(wvs[:, 0] * (a / (a + wvs[:, 1])), axis=0)\n",
    "\n",
    "X = np.vstack(processed_docs.vect.map(get_sentence_vector))\n",
    "eig_val, eig_vec = eig(X.T.dot(X))\n",
    "u = eig_vec[:, 0].reshape(-1, 1).T\n",
    "X_sent = np.vstack([x - u.T.dot(u).dot(x) for x in X])\n",
    "y = pd.get_dummies(processed_docs.label).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_neg = np.amin(X_sent)\n",
    "X_sent = (-1*min_neg) + X_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3452x300 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 1035599 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparseX = sparse.csr_matrix(X_sent)\n",
    "sparseX\n",
    "\n",
    "# from sklearn.preprocessing import normalize\n",
    "# x_normalized = normalize(sparseX, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n",
      "/home/grace/.local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=10, n_jobs=4,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_components': [4, 5, 6, 7, 8, 9], 'learning_decay': [0.5, 0.7, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearch\n",
    "\n",
    "search_params = {'n_components': [i for i in range(4, 10)], 'learning_decay': [.5, .7, .9]}\n",
    "lda = LatentDirichletAllocation(n_jobs=4)\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(sparseX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 4}\n",
      "Best Log Likelihood Score:  -134743.4106375915\n",
      "Model Perplexity:  364.45463437772906\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(sparseX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dominant topic\n",
    "- find the most contributed topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = processed_docs.merge(documents, on='id')\n",
    "# test = processed_docs[['id', 'title_x', 'bow_corpus', 'title_y']]\n",
    "# test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32049, 5)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title_x</th>\n",
       "      <th>bow_corpus</th>\n",
       "      <th>index</th>\n",
       "      <th>title_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "      <td>[(0, 1), (1, 1)]</td>\n",
       "      <td>0</td>\n",
       "      <td>Analysis of efficacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>[comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]</td>\n",
       "      <td>[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]</td>\n",
       "      <td>1</td>\n",
       "      <td>Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "      <td>[(8, 1), (9, 1), (10, 1)]</td>\n",
       "      <td>2</td>\n",
       "      <td>Pattern of disease relapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>[grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]</td>\n",
       "      <td>[(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]</td>\n",
       "      <td>3</td>\n",
       "      <td>Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>[treatment, zoledron, acid]</td>\n",
       "      <td>[(16, 1), (17, 1), (18, 1)]</td>\n",
       "      <td>4</td>\n",
       "      <td>Treatment with zoledronic acid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  4106   \n",
       "1  4107   \n",
       "2  4108   \n",
       "3  4109   \n",
       "4  4112   \n",
       "\n",
       "                                                                 title_x  \\\n",
       "0  [analysi, efficaci]                                                     \n",
       "1  [comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]   \n",
       "2  [pattern, diseas, relaps]                                               \n",
       "3  [grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]       \n",
       "4  [treatment, zoledron, acid]                                             \n",
       "\n",
       "                                         bow_corpus index  \\\n",
       "0  [(0, 1), (1, 1)]                                  0      \n",
       "1  [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]  1      \n",
       "2  [(8, 1), (9, 1), (10, 1)]                         2      \n",
       "3  [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]     3      \n",
       "4  [(16, 1), (17, 1), (18, 1)]                       4      \n",
       "\n",
       "                                                                                                    title_y  \n",
       "0  Analysis of efficacy                                                                                      \n",
       "1  Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials  \n",
       "2  Pattern of disease relapse                                                                                \n",
       "3  Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine                         \n",
       "4  Treatment with zoledronic acid                                                                            "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            4325                                     \n",
       "title_x       [current, patient, status, follow]       \n",
       "bow_corpus    [(101, 1), (178, 1), (262, 1), (456, 1)] \n",
       "index         151                                      \n",
       "title_y       Current Patient Status at Last Follow-up \n",
       "Name: 151, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs.iloc[151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def format_topics_sentences(model, \\\n",
    "                            model_type='mallet', \\\n",
    "                            corpus=processed_docs, \\\n",
    "                            texts=dictionary_made_by):\n",
    "    sent_topic_df = pd.DataFrame()\n",
    "    if model_type=='tfidf':\n",
    "        target_corpus = corpus.tfidf_corpus\n",
    "    else:\n",
    "        target_corpus = corpus.bow_corpus\n",
    "    \n",
    "    for i, row in enumerate(model[target_corpus]):\n",
    "        origin_info = processed_docs.iloc[i]\n",
    "        text_vec = texts[i]\n",
    "        #get main topic in each document\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j==0: #dominant topic\n",
    "                wp = model.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "#                 print(pd.Series([origin_info.id,\\\n",
    "#                                 int(topic_num), \\\n",
    "#                                 round(prop_topic, 4), \\\n",
    "#                                 topic_keywords, \\\n",
    "#                                origin_info.title_y, \\\n",
    "#                                text_vec]))\n",
    "                sent_topic_df = sent_topic_df.append(pd.Series([origin_info.id,\\\n",
    "                                                                int(topic_num), \\\n",
    "                                                                round(prop_topic, 4), \\\n",
    "                                                                topic_keywords, \\\n",
    "                                                               origin_info.title_y, \\\n",
    "                                                               text_vec]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topic_df.columns = ['id', 'Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Origin_Text', 'Text_Vec']\n",
    "    \n",
    "    return (sent_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>Text_Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Analysis of efficacy</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "      <td>Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials</td>\n",
       "      <td>[comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>patient, data, comparison, score, respons, time, accord, test, status, assess</td>\n",
       "      <td>Pattern of disease relapse</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine</td>\n",
       "      <td>[grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "      <td>Treatment with zoledronic acid</td>\n",
       "      <td>[treatment, zoledron, acid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4113</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Treatment with docetaxel</td>\n",
       "      <td>[treatment, docetaxel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Treatments ever used at relapse, at the discretion of the treating clinician</td>\n",
       "      <td>[treatment, relaps, discret, treat, clinician]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4115</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Worst adverse event  (grade)  reported over entire time on trial</td>\n",
       "      <td>[worst, advers, event, grade, report, entir, time, trial]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  Dominant_Topic  Perc_Contribution  \\\n",
       "0  4106  1.0             0.1394              \n",
       "1  4107  2.0             0.1473              \n",
       "2  4108  0.0             0.1368              \n",
       "3  4109  1.0             0.1898              \n",
       "4  4112  2.0             0.1515              \n",
       "5  4113  1.0             0.1394              \n",
       "6  4114  1.0             0.1500              \n",
       "7  4115  1.0             0.2112              \n",
       "\n",
       "                                                                    Topic_Keywords  \\\n",
       "0  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "1  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control   \n",
       "2  patient, data, comparison, score, respons, time, accord, test, status, assess     \n",
       "3  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "4  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control   \n",
       "5  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "6  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "7  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "\n",
       "                                                                                                Origin_Text  \\\n",
       "0  Analysis of efficacy                                                                                       \n",
       "1  Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials   \n",
       "2  Pattern of disease relapse                                                                                 \n",
       "3  Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine                          \n",
       "4  Treatment with zoledronic acid                                                                             \n",
       "5  Treatment with docetaxel                                                                                   \n",
       "6  Treatments ever used at relapse, at the discretion of the treating clinician                               \n",
       "7  Worst adverse event  (grade)  reported over entire time on trial                                           \n",
       "\n",
       "                                                                Text_Vec  \n",
       "0  [analysi, efficaci]                                                    \n",
       "1  [comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]  \n",
       "2  [pattern, diseas, relaps]                                              \n",
       "3  [grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]      \n",
       "4  [treatment, zoledron, acid]                                            \n",
       "5  [treatment, docetaxel]                                                 \n",
       "6  [treatment, relaps, discret, treat, clinician]                         \n",
       "7  [worst, advers, event, grade, report, entir, time, trial]              "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords_mallet = format_topics_sentences(model=mallet_model, \\\n",
    "                                                  corpus=processed_docs, \\\n",
    "                                                  texts=dictionary_made_by, \\\n",
    "                                                 model_type='mallet')\n",
    "df_topic_sents_keywords_mallet.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32049, 6)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords_mallet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topic_sents_keywords_tfidf = format_topics_sentences(model=tfidf_model, \\\n",
    "#                                                   corpus=processed_docs, \\\n",
    "#                                                   texts=dictionary_made_by, \\\n",
    "#                                                  model_type='tfidf')\n",
    "# df_topic_sents_keywords_tfidf.head(max_coherence_topic_num_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topic_sents_keywords_on = format_topics_sentences(model=on_model, \\\n",
    "#                                                   corpus=processed_docs, \\\n",
    "#                                                   texts=dictionary_made_by, \\\n",
    "#                                                  model_type='online')\n",
    "# df_topic_sents_keywords_on.head(max_coherence_topic_num_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                   False\n",
      "Dominant_Topic       False\n",
      "Perc_Contribution    False\n",
      "Topic_Keywords       False\n",
      "Origin_Text          False\n",
      "Text_Vec             False\n",
      "dtype: bool\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "print(df_topic_sents_keywords_mallet.isna().any())\n",
    "print('=================')\n",
    "# print(df_topic_sents_keywords_tfidf.isna().any())\n",
    "# print('=================')\n",
    "# print(df_topic_sents_keywords_on.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 토픽별로 가장 대표적인 문서 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_sents(keywords, model_type):\n",
    "    sent_topics_sorted_df_mallet = pd.DataFrame()\n",
    "    \n",
    "    sent_topics_groupby = keywords.groupby('Dominant_Topic')\n",
    "\n",
    "    keywords_top = pd.DataFrame()\n",
    "    for i, grp in sent_topics_groupby:\n",
    "        keywords_top = pd.concat([keywords_top, \\\n",
    "                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(50)], \\\n",
    "                            axis=0)\n",
    "        keywords = pd.concat([keywords, \\\n",
    "                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(50)], \\\n",
    "                            axis=0)\n",
    "\n",
    "    keywords_top.reset_index(drop=True, inplace=True)\n",
    "    keywords_top.columns = ['id', 'Topic_Num', 'Topic_Perc_Contribu', 'Topic_Keywords', 'Origin_Text', 'Text']\n",
    "    keywords_top.to_csv(directory+'/'+model_type+'_top_sen.tsv', sep='\\t')\n",
    "    \n",
    "    keywords.reset_index(drop=True, inplace=True)\n",
    "    keywords.columns = ['id', 'Topic_Num', 'Topic_Perc_Contribu', 'Topic_Keywords', 'Origin_Text', 'Text']\n",
    "    \n",
    "    return keywords\n",
    "    \n",
    "df_topic_sents_mallet = get_rep_sents(df_topic_sents_keywords_mallet, 'mallet')\n",
    "# df_topic_sents_tfidf = get_rep_sents(df_topic_sents_keywords_tfidf, 'tfidf')\n",
    "# df_topic_sents_on = get_rep_sents(df_topic_sents_keywords_on, 'online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32449, 6)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_mallet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_mallet.drop_duplicates(subset=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32049, 6)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_mallet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_conn import get_connection\n",
    "\n",
    "# conn = get_connection()\n",
    "# conn.set_client_encoding('UTF8')\n",
    "\n",
    "\n",
    "def update_topic_num(keyword_df):\n",
    "    _cur = conn.cursor()\n",
    "    update_sql = \"\"\"UPDATE article_tables\n",
    "                    SET topic_num_title = %s,\n",
    "                    topic_num_title_perc = %s\n",
    "                    WHERE\n",
    "                     id=%s\"\"\"\n",
    "    \n",
    "    for row in keyword_df.iterrows():\n",
    "        _id = row[1]['id']\n",
    "        topic_num = row[1]['Topic_Num']\n",
    "#         print(topic_num)\n",
    "        perc = row[1]['Topic_Perc_Contribu']\n",
    "#         print(perc)\n",
    "        _cur.execute(update_sql, (topic_num, perc, _id, ))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contribu</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Analysis of efficacy</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "      <td>Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials</td>\n",
       "      <td>[comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>patient, data, comparison, score, respons, time, accord, test, status, assess</td>\n",
       "      <td>Pattern of disease relapse</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine</td>\n",
       "      <td>[grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "      <td>Treatment with zoledronic acid</td>\n",
       "      <td>[treatment, zoledron, acid]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  Topic_Num  Topic_Perc_Contribu  \\\n",
       "0  4106  1.0        0.1394                \n",
       "1  4107  2.0        0.1473                \n",
       "2  4108  0.0        0.1368                \n",
       "3  4109  1.0        0.1898                \n",
       "4  4112  2.0        0.1515                \n",
       "\n",
       "                                                                    Topic_Keywords  \\\n",
       "0  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "1  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control   \n",
       "2  patient, data, comparison, score, respons, time, accord, test, status, assess     \n",
       "3  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "4  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control   \n",
       "\n",
       "                                                                                                Origin_Text  \\\n",
       "0  Analysis of efficacy                                                                                       \n",
       "1  Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials   \n",
       "2  Pattern of disease relapse                                                                                 \n",
       "3  Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine                          \n",
       "4  Treatment with zoledronic acid                                                                             \n",
       "\n",
       "                                                                    Text  \n",
       "0  [analysi, efficaci]                                                    \n",
       "1  [comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]  \n",
       "2  [pattern, diseas, relaps]                                              \n",
       "3  [grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]      \n",
       "4  [treatment, zoledron, acid]                                            "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 문서 찾기\n",
    "\n",
    "sent_topics_sorted_df_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_groupby = df_topic_sents_keywords_mallet.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_groupby:\n",
    "    keywords = pd.concat([df_topic_sents_keywords_mallet, \\\n",
    "                         grp.sort_values(['Perc_Contribution'], ascending=[0])], \\\n",
    "                        axis=0)\n",
    "\n",
    "keywords.reset_index(drop=True, inplace=True)\n",
    "keywords.columns = ['id', 'Topic_Num', 'Topic_Perc_Contribu', 'Topic_Keywords', 'Origin_Text', 'Text']\n",
    "\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update 할때만 주석 해제\n",
    "# update_topic_num(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>Text_Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Analysis of efficacy</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "      <td>Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials</td>\n",
       "      <td>[comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>patient, data, comparison, score, respons, time, accord, test, status, assess</td>\n",
       "      <td>Pattern of disease relapse</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine</td>\n",
       "      <td>[grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "      <td>Treatment with zoledronic acid</td>\n",
       "      <td>[treatment, zoledron, acid]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  Dominant_Topic  Perc_Contribution  \\\n",
       "0  4106  1.0             0.1394              \n",
       "1  4107  2.0             0.1473              \n",
       "2  4108  0.0             0.1368              \n",
       "3  4109  1.0             0.1898              \n",
       "4  4112  2.0             0.1515              \n",
       "\n",
       "                                                                    Topic_Keywords  \\\n",
       "0  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "1  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control   \n",
       "2  patient, data, comparison, score, respons, time, accord, test, status, assess     \n",
       "3  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "4  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control   \n",
       "\n",
       "                                                                                                Origin_Text  \\\n",
       "0  Analysis of efficacy                                                                                       \n",
       "1  Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials   \n",
       "2  Pattern of disease relapse                                                                                 \n",
       "3  Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine                          \n",
       "4  Treatment with zoledronic acid                                                                             \n",
       "\n",
       "                                                                Text_Vec  \n",
       "0  [analysi, efficaci]                                                    \n",
       "1  [comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]  \n",
       "2  [pattern, diseas, relaps]                                              \n",
       "3  [grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]      \n",
       "4  [treatment, zoledron, acid]                                            "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords_mallet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contribu</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Analysis of efficacy</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "      <td>Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials</td>\n",
       "      <td>[comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>patient, data, comparison, score, respons, time, accord, test, status, assess</td>\n",
       "      <td>Pattern of disease relapse</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "      <td>Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine</td>\n",
       "      <td>[grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "      <td>Treatment with zoledronic acid</td>\n",
       "      <td>[treatment, zoledron, acid]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  Topic_Num  Topic_Perc_Contribu  \\\n",
       "0  4106  1.0        0.1394                \n",
       "1  4107  2.0        0.1473                \n",
       "2  4108  0.0        0.1368                \n",
       "3  4109  1.0        0.1898                \n",
       "4  4112  2.0        0.1515                \n",
       "\n",
       "                                                                    Topic_Keywords  \\\n",
       "0  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "1  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control   \n",
       "2  patient, data, comparison, score, respons, time, accord, test, status, assess     \n",
       "3  event, treatment, group, advers, studi, popul, treat, summari, week, report       \n",
       "4  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control   \n",
       "\n",
       "                                                                                                Origin_Text  \\\n",
       "0  Analysis of efficacy                                                                                       \n",
       "1  Comparisons of postoperative CA19-9 levels on survival of ESPAC-4 with the CONOKO-01 and JASPAC-1 trials   \n",
       "2  Pattern of disease relapse                                                                                 \n",
       "3  Grade 1–5 adverse events with gemcitabine alone and gemcitabine plus capecitabine                          \n",
       "4  Treatment with zoledronic acid                                                                             \n",
       "\n",
       "                                                                    Text  \n",
       "0  [analysi, efficaci]                                                    \n",
       "1  [comparison, postop, ca, level, surviv, espac, conoko, jaspac, trial]  \n",
       "2  [pattern, diseas, relaps]                                              \n",
       "3  [grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]      \n",
       "4  [treatment, zoledron, acid]                                            "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_mallet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_mallet.to_csv('./mallet_results.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32049, 6)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords_mallet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서 전체적인 토픽 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df_topic_sents_mallet['Topic_Num'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "topic_num_keywords = df_topic_sents_mallet[['Topic_Num', 'Topic_Keywords']]\n",
    "\n",
    "df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>patient, data, comparison, score, respons, time, accord, test, status, assess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.0</td>\n",
       "      <td>effect, number, trial, estim, type, intervent, case, compar, health, control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>year, age, mortal, relat, ratio, rat, incid, adjust, death, caus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.0</td>\n",
       "      <td>outcom, clinic, patient, follow, month, day, hospit, primari, procedur, secondari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.0</td>\n",
       "      <td>diseas, result, coronari, myocardi, valu, arteri, angiograph, infarct, frequenc, find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3.0</td>\n",
       "      <td>analysi, risk, associ, multivari, factor, regress, model, predictor, variabl, cardiac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Num  \\\n",
       "0   1.0         \n",
       "1   2.0         \n",
       "2   0.0         \n",
       "10  6.0         \n",
       "13  5.0         \n",
       "21  7.0         \n",
       "38  4.0         \n",
       "50  3.0         \n",
       "\n",
       "                                                                           Topic_Keywords  \n",
       "0   event, treatment, group, advers, studi, popul, treat, summari, week, report            \n",
       "1   chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control        \n",
       "2   patient, data, comparison, score, respons, time, accord, test, status, assess          \n",
       "10  effect, number, trial, estim, type, intervent, case, compar, health, control           \n",
       "13  year, age, mortal, relat, ratio, rat, incid, adjust, death, caus                       \n",
       "21  outcom, clinic, patient, follow, month, day, hospit, primari, procedur, secondari      \n",
       "38  diseas, result, coronari, myocardi, valu, arteri, angiograph, infarct, frequenc, find  \n",
       "50  analysi, risk, associ, multivari, factor, regress, model, predictor, variabl, cardiac  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_num_keywords.drop_duplicates(inplace=True)\n",
    "topic_num_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4207</td>\n",
       "      <td>0.1313</td>\n",
       "      <td>patient, data, comparison, score, respons, time, accord, test, status, assess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4892</td>\n",
       "      <td>0.1526</td>\n",
       "      <td>event, treatment, group, advers, studi, popul, treat, summari, week, report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4350</td>\n",
       "      <td>0.1357</td>\n",
       "      <td>chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4117</td>\n",
       "      <td>0.1285</td>\n",
       "      <td>analysi, risk, associ, multivari, factor, regress, model, predictor, variabl, cardiac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3769</td>\n",
       "      <td>0.1176</td>\n",
       "      <td>diseas, result, coronari, myocardi, valu, arteri, angiograph, infarct, frequenc, find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3416</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>year, age, mortal, relat, ratio, rat, incid, adjust, death, caus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3175</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>effect, number, trial, estim, type, intervent, case, compar, health, control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4123</td>\n",
       "      <td>0.1286</td>\n",
       "      <td>outcom, clinic, patient, follow, month, day, hospit, primari, procedur, secondari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Num_Documents  Perc_Documents  \\\n",
       "0  0.0        4207           0.1313           \n",
       "1  1.0        4892           0.1526           \n",
       "2  2.0        4350           0.1357           \n",
       "3  3.0        4117           0.1285           \n",
       "4  4.0        3769           0.1176           \n",
       "5  5.0        3416           0.1066           \n",
       "6  6.0        3175           0.0991           \n",
       "7  7.0        4123           0.1286           \n",
       "\n",
       "                                                                          Topic_Keywords  \n",
       "0  patient, data, comparison, score, respons, time, accord, test, status, assess          \n",
       "1  event, treatment, group, advers, studi, popul, treat, summari, week, report            \n",
       "2  chang, heart, paramet, function, hemodynam, measur, level, blood, rate, control        \n",
       "3  analysi, risk, associ, multivari, factor, regress, model, predictor, variabl, cardiac  \n",
       "4  diseas, result, coronari, myocardi, valu, arteri, angiograph, infarct, frequenc, find  \n",
       "5  year, age, mortal, relat, ratio, rat, incid, adjust, death, caus                       \n",
       "6  effect, number, trial, estim, type, intervent, case, compar, health, control           \n",
       "7  outcom, clinic, patient, follow, month, day, hospit, primari, procedur, secondari      "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "df_dominant_topics.columns = ['Topic_Num', 'Num_Documents', 'Perc_Documents']\n",
    "df_dominant_topics = df_dominant_topics.merge(topic_num_keywords, on='Topic_Num')\n",
    "df_dominant_topics = df_dominant_topics.sort_values('Topic_Num')\n",
    "\n",
    "df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "df_dominant_topics.to_html(directory+'/mallet_distribution.html', index=False)\n",
    "df_dominant_topics.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_counts = df_topic_sents_tfidf['Topic_Num'].value_counts()\n",
    "# topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# topic_num_keywords = df_topic_sents_tfidf[['Topic_Num', 'Topic_Keywords']]\n",
    "\n",
    "# df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "# df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "# df_dominant_topics.columns = ['Topic_Num', 'Num_Documents', 'Perc_Documents']\n",
    "# df_dominant_topics = df_dominant_topics.merge(topic_num_keywords.drop_duplicates(), on='Topic_Num')\n",
    "# df_dominant_topics = df_dominant_topics.sort_values('Topic_Num')\n",
    "\n",
    "# df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "# df_dominant_topics.to_html(directory+'/tfidf_distribution.html', index=False)\n",
    "# df_dominant_topics.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_counts = df_topic_sents_on['Topic_Num'].value_counts()\n",
    "# topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# topic_num_keywords = df_topic_sents_on[['Topic_Num', 'Topic_Keywords']]\n",
    "\n",
    "# df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "# df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "# df_dominant_topics.columns = ['Topic_Num', 'Num_Documents', 'Perc_Documents']\n",
    "# df_dominant_topics = df_dominant_topics.merge(topic_num_keywords.drop_duplicates(), on='Topic_Num')\n",
    "# df_dominant_topics = df_dominant_topics.sort_values('Topic_Num')\n",
    "\n",
    "# df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "# df_dominant_topics.to_html(directory+'/on_distribution.html', index=False)\n",
    "# df_dominant_topics.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32042, 8)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim    \n",
    "\n",
    "# model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(mallet_model)\n",
    "output = open('../data/output/lda.pkl', 'wb')\n",
    "# pickle.dump({'topic_model': model, \n",
    "#               'corpus': bow_corpus, \n",
    "#               'dictionary': dictionary\n",
    "#             }, output)\n",
    "pickle.dump({\n",
    "            'topic_term_dists':phi,\n",
    "            'doc_topic_dists':theta,\n",
    "            'doc_lengths':list(docs['doc_length']),\n",
    "            'vocab':list(vocab['type']),\n",
    "            'term_frequency':list(vocab['term_freq'])\n",
    "        }, output)\n",
    "output.close()\n",
    "\n",
    "# topic_term_dists = lda_results['topic_term_dists']\n",
    "# doc_topic_dists = lda_results['doc_topic_dists']\n",
    "# doc_lengths = lda_results['doc_lengths']\n",
    "# vocab = lda_results['vocab']\n",
    "# term_frequency = lda_results['term_frequency']\n",
    "\n",
    "# saved_model['lda_result'] = {\n",
    "#         'document_topic_counts':topic_term_dists,\n",
    "#         'topic_word_counts':doc_topic_dists,\n",
    "#         'topic_counts':doc_lengths,\n",
    "#         'document_lengths':vocab,\n",
    "#         'distinct_words':term_frequency\n",
    "#         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://ai.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html\n",
    "- http://projector.tensorflow.org/\n",
    "- http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb#loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.title_y.to_csv('../data/output/origin_text.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.title_x.to_csv('../data/output/processed.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
