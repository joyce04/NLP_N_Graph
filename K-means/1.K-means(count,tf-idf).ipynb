{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster a set of documents using Python\n",
    "\n",
    "1. tokenizing\n",
    "2. stemming (*based on stemming lib, results change)- reduce a word to its stem or root form\n",
    "3. calculate cosine distance between each document = measure of similarity\n",
    "4. cluster documents using the k-means algorithm\n",
    "5. using multidimensional scaling to reduce dimensionality within the corpus\n",
    "6. conduct a hierarchical clustering on the corpus using Ward clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References :\n",
    "- https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n",
    "- https://medium.com/@vegi/visualizing-higher-dimensional-data-using-t-sne-on-tensorboard-7dbf22682cf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import psycopg2\n",
    "# import db_conn\n",
    "from IPython.display import display\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = get_connection()\n",
    "\n",
    "def get_article_tables(is_file):\n",
    "    if is_file:\n",
    "        return pd.read_csv('../data/titles_condition_by_t.tsv', sep='\\t', header=None)\n",
    "#         return pd.read_csv('../topic_modeling/best_files/dic_unigram_size_6000/mallet_top_sen.tsv', sep='\\t')\n",
    "    else:\n",
    "        curs = conn.cursor()\n",
    "\n",
    "        select_sql = \"\"\"SELECT id, table_title, strip_tags(CONTENT) as content FROM article_tables order by id\"\"\" # limit 10000\n",
    "        curs.execute(select_sql)\n",
    "        return curs.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_article_tables(True)\n",
    "# train_data = train_data[['id', 'Origin_Text']]\n",
    "train_data.columns=['id', 'title']\n",
    "# clean_content = [x['content'].lower() for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>Analysis of efficacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>Comparisons of postoperative CA19-9 levels on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>Pattern of disease relapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>Grade 1–5 adverse events with gemcitabine alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>Treatment with zoledronic acid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              title\n",
       "0  4106                               Analysis of efficacy\n",
       "1  4107  Comparisons of postoperative CA19-9 levels on ...\n",
       "2  4108                         Pattern of disease relapse\n",
       "3  4109  Grade 1–5 adverse events with gemcitabine alon...\n",
       "4  4112                     Treatment with zoledronic acid"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       False\n",
      "title    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "train_data.title = train_data.title.str.strip()\n",
    "train_data['title'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       False\n",
      "title    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "train_data.loc[train_data.title.isna()]\n",
    "train_data.dropna(subset=['title'], inplace=True)\n",
    "print(train_data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep = {'nbsp':'', 'table':'', 'legend':'', 'mg/dl':'', 'g/l':'', 'yrs':'year', '\\n':' ', ';':'', 'kg/m2':'', 'n=':''}\n",
    "rep = {'nbsp':'', 'table':'', 'legend':'', 'yrs':'year', '\\n':' '}\n",
    "# clean_content = [pattern.sub(lambda m: rep[re.escape(m.group(0))], x['content']) for x in train_data]\n",
    "rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "train_data.title = [pattern.sub(lambda m: rep[re.escape(m.group(0))], str(x)) for x in train_data.title]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 Analysis of efficacy\n",
       "1    Comparisons of postoperative CA19-9 levels on ...\n",
       "2                           Pattern of disease relapse\n",
       "3    Grade 1–5 adverse events with gemcitabine alon...\n",
       "4                       Treatment with zoledronic acid\n",
       "5                             Treatment with docetaxel\n",
       "6    Treatments ever used at relapse, at the discre...\n",
       "7    Worst adverse event  (grade)  reported over en...\n",
       "8    Chemotherapy delivery and trial drug discontin...\n",
       "9                                       Adverse events\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.title[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/grace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "stemmer = SnowballStemmer('english')\n",
    "STOP_WORDS = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "STOP_WORDS.extend(['table', 'legend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "# -porter stemmer\n",
    "# -lancaster stemmer\n",
    "# -snowball stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "#     deacc=True removes punctuations\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True):\n",
    "        if token not in STOP_WORDS and len(token)>1:\n",
    "#             result.append(lemmatize_stemming(strip_numeric(token)))\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def preprocess_token_only(text):\n",
    "    result = []\n",
    "#     deacc=True removes punctuations\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True):\n",
    "        if token not in STOP_WORDS and len(token)>1:\n",
    "#             result.append(lemmatize_stemming(strip_numeric(token)))\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "#tokenizing\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #filter tokens not containing letters\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token)>2:\n",
    "            filtered.append(token)\n",
    "#     stems = [stemmer.stem(t, pos='v') for t in filtered]\n",
    "    stems = [stemmer.stem(t) for t in filtered]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token)>2:\n",
    "            filtered.append(token)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_stemmed = []\n",
    "total_vocab_tokenized = []\n",
    "\n",
    "for i in train_data.title.tolist():\n",
    "    all_stemmed = preprocess(i)\n",
    "    total_vocab_stemmed.extend(all_stemmed)\n",
    "    \n",
    "    all_tokenized = preprocess_token_only(i)\n",
    "    total_vocab_tokenized.extend(all_tokenized)\n",
    "\n",
    "processed_docs = pd.DataFrame()\n",
    "processed_docs = pd.concat([train_data.id, train_data.title.map(preprocess)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>[analys, eff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>[comparison, postop, ca, level, surv, espac, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>[grad, advers, ev, gemcitabin, gemcitabin, plu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>[tre, zoledron, acid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4113</td>\n",
       "      <td>[tre, docetaxel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4114</td>\n",
       "      <td>[tre, relaps, discret, tre, clin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4115</td>\n",
       "      <td>[worst, advers, ev, grad, report, entir, tim, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4117</td>\n",
       "      <td>[chemotherapy, delivery, tri, drug, discontinu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4118</td>\n",
       "      <td>[advers, ev]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              title\n",
       "0  4106                                      [analys, eff]\n",
       "1  4107  [comparison, postop, ca, level, surv, espac, c...\n",
       "2  4108                          [pattern, diseas, relaps]\n",
       "3  4109  [grad, advers, ev, gemcitabin, gemcitabin, plu...\n",
       "4  4112                              [tre, zoledron, acid]\n",
       "5  4113                                   [tre, docetaxel]\n",
       "6  4114                  [tre, relaps, discret, tre, clin]\n",
       "7  4115  [worst, advers, ev, grad, report, entir, tim, ...\n",
       "8  4117    [chemotherapy, delivery, tri, drug, discontinu]\n",
       "9  4118                                       [advers, ev]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>analys</th>\n",
       "      <td>analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff</th>\n",
       "      <td>efficacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparison</th>\n",
       "      <td>comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postop</th>\n",
       "      <td>postoperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    words\n",
       "analys           analysis\n",
       "eff              efficacy\n",
       "comparison    comparisons\n",
       "postop      postoperative\n",
       "ca                     ca"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe with stemmed vocab and tokenized words (link)\n",
    "vocab_frame = pd.DataFrame({'words':total_vocab_tokenized}, index=total_vocab_stemmed)\n",
    "vocab_frame.drop_duplicates(inplace=True)\n",
    "vocab_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf and document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 85.3 ms, total: 10.1 s\n",
      "Wall time: 10.2 s\n",
      "(27960, 147)\n"
     ]
    }
   ],
   "source": [
    "#frequency-inverse document frequencey(tf-idf) vectorize parameters and convert the document list into tf-idf matrix\n",
    "# 1. count word occurrences by document\n",
    "# 2. transform into a document-term matrix = term frequency matrix\n",
    "\n",
    "#max_df = max frequency within the documents\n",
    "#min_idf = if 5, the term would have to be in at least 5 of the documents to be considered, 0.2 = 20% of documents\n",
    "#ngram_ranges\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                                   max_features=10000, \n",
    "                                   min_df=0.01, \n",
    "                                   stop_words='english', \n",
    "                                   use_idf=True, \n",
    "                                   lowercase=True, \n",
    "                                   tokenizer=preprocess)\n",
    "#                                    tokenizer=preprocess, ngram_range=(1,2))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(train_data.title)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accord',\n",
       " 'act',\n",
       " 'acut',\n",
       " 'adjust',\n",
       " 'advers',\n",
       " 'ag',\n",
       " 'analys',\n",
       " 'angiograph',\n",
       " 'artery',\n",
       " 'assess']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref : http://www.pbarrett.net/techpapers/euclid.pdf\n",
    "Normalize :\n",
    "\n",
    "The problem of raw distance coefficient is that it has no obvious bound value for the maximum distance.\n",
    "Basically you don't know from its size whether a coefficient indicates a small or large distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 57s, sys: 23.8 s, total: 9min 21s\n",
      "Wall time: 13min 22s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_tsne_result = TSNE(learning_rate=300, init='pca', \\\n",
    "                               n_iter=250, random_state=0)\\\n",
    "                    .fit_transform(np.array(tfidf_matrix.toarray()))\n",
    "tfidf_vect = normalize(tfidf_tsne_result, norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grace/workspace/keras/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/grace/workspace/keras/venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = tf.Variable(tfidf_tsne_result)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver([tf_data])\n",
    "    sess.run(tf_data.initializer)\n",
    "#     saver.save(sess, os.path.join(LOG_DIR, 'tf_data.ckpt'))\n",
    "    config = projector.ProjectorConfig()\n",
    "    \n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = tf_data.name\n",
    "    \n",
    "#     embedding.metadata_path = metadata\n",
    "#     projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist = cosine similarity of each document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "         8.53883514e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.00000000e+00, -2.22044605e-16,  1.00000000e+00, ...,\n",
       "         5.88511646e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
       "       ...,\n",
       "       [ 8.53883514e-01,  5.88511646e-01,  1.00000000e+00, ...,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "         1.00000000e+00, -2.22044605e-16,  1.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "         1.00000000e+00,  1.00000000e+00, -2.22044605e-16]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer, CountVectorizer\n",
    "\n",
    "- 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩한 벡터를 만든다.\n",
    "- HashingVectorizer를 사용하면 해시 함수를 사용하여 단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accord': 0,\n",
       " 'act': 1,\n",
       " 'acut': 2,\n",
       " 'adjust': 3,\n",
       " 'advers': 4,\n",
       " 'ag': 5,\n",
       " 'analys': 6,\n",
       " 'angiograph': 7,\n",
       " 'artery': 8,\n",
       " 'assess': 9,\n",
       " 'assocy': 10,\n",
       " 'bas': 11,\n",
       " 'blood': 12,\n",
       " 'cardiac': 13,\n",
       " 'cardiovascul': 14,\n",
       " 'cas': 15,\n",
       " 'categ': 16,\n",
       " 'caus': 17,\n",
       " 'chang': 18,\n",
       " 'childr': 19,\n",
       " 'class': 20,\n",
       " 'clin': 21,\n",
       " 'cohort': 22,\n",
       " 'combin': 23,\n",
       " 'comp': 24,\n",
       " 'comparison': 25,\n",
       " 'comply': 26,\n",
       " 'cont': 27,\n",
       " 'control': 28,\n",
       " 'coron': 29,\n",
       " 'correl': 30,\n",
       " 'country': 31,\n",
       " 'cox': 32,\n",
       " 'dat': 33,\n",
       " 'day': 34,\n",
       " 'dea': 35,\n",
       " 'death': 36,\n",
       " 'diagnos': 37,\n",
       " 'diff': 38,\n",
       " 'diseas': 39,\n",
       " 'distribut': 40,\n",
       " 'dos': 41,\n",
       " 'drug': 42,\n",
       " 'eff': 43,\n",
       " 'effect': 44,\n",
       " 'end': 45,\n",
       " 'endpoint': 46,\n",
       " 'estim': 47,\n",
       " 'ev': 48,\n",
       " 'exerc': 49,\n",
       " 'fact': 50,\n",
       " 'fail': 51,\n",
       " 'flow': 52,\n",
       " 'follow': 53,\n",
       " 'frequ': 54,\n",
       " 'funct': 55,\n",
       " 'gen': 56,\n",
       " 'group': 57,\n",
       " 'hazard': 58,\n",
       " 'heal': 59,\n",
       " 'heart': 60,\n",
       " 'hemodynam': 61,\n",
       " 'high': 62,\n",
       " 'hospit': 63,\n",
       " 'incid': 64,\n",
       " 'ind': 65,\n",
       " 'independ': 66,\n",
       " 'index': 67,\n",
       " 'individ': 68,\n",
       " 'infarct': 69,\n",
       " 'infect': 70,\n",
       " 'interv': 71,\n",
       " 'leav': 72,\n",
       " 'level': 73,\n",
       " 'log': 74,\n",
       " 'maj': 75,\n",
       " 'mean': 76,\n",
       " 'meas': 77,\n",
       " 'med': 78,\n",
       " 'model': 79,\n",
       " 'mon': 80,\n",
       " 'month': 81,\n",
       " 'mort': 82,\n",
       " 'mult': 83,\n",
       " 'multivary': 84,\n",
       " 'myocard': 85,\n",
       " 'non': 86,\n",
       " 'numb': 87,\n",
       " 'od': 88,\n",
       " 'op': 89,\n",
       " 'outcom': 90,\n",
       " 'overal': 91,\n",
       " 'paramet': 92,\n",
       " 'particip': 93,\n",
       " 'paty': 94,\n",
       " 'period': 95,\n",
       " 'point': 96,\n",
       " 'pop': 97,\n",
       " 'post': 98,\n",
       " 'predict': 99,\n",
       " 'pres': 100,\n",
       " 'press': 101,\n",
       " 'prev': 102,\n",
       " 'prim': 103,\n",
       " 'proc': 104,\n",
       " 'proport': 105,\n",
       " 'random': 106,\n",
       " 'rat': 107,\n",
       " 'ratio': 108,\n",
       " 'reg': 109,\n",
       " 'regress': 110,\n",
       " 'rel': 111,\n",
       " 'report': 112,\n",
       " 'respons': 113,\n",
       " 'result': 114,\n",
       " 'risk': 115,\n",
       " 'saf': 116,\n",
       " 'scor': 117,\n",
       " 'second': 118,\n",
       " 'select': 119,\n",
       " 'sensit': 120,\n",
       " 'sev': 121,\n",
       " 'sex': 122,\n",
       " 'sign': 123,\n",
       " 'spec': 124,\n",
       " 'stat': 125,\n",
       " 'stratify': 126,\n",
       " 'stroke': 127,\n",
       " 'study': 128,\n",
       " 'subgroup': 129,\n",
       " 'subject': 130,\n",
       " 'sum': 131,\n",
       " 'surv': 132,\n",
       " 'test': 133,\n",
       " 'therapy': 134,\n",
       " 'tim': 135,\n",
       " 'tot': 136,\n",
       " 'tre': 137,\n",
       " 'tri': 138,\n",
       " 'typ': 139,\n",
       " 'valu': 140,\n",
       " 'vary': 141,\n",
       " 'ventricul': 142,\n",
       " 'vers': 143,\n",
       " 'week': 144,\n",
       " 'wom': 145,\n",
       " 'year': 146}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_df=0.8, \\\n",
    "                       min_df=0.01, \\\n",
    "                       max_features=10000,\\\n",
    "                       lowercase=True,\\\n",
    "                       tokenizer=preprocess,\\\n",
    "                       stop_words='english')\n",
    "# vect = HashingVectorizer(n_features=10000,\\\n",
    "#                        lowercase=True,\\\n",
    "#                        tokenizer=preprocess,\\\n",
    "#                        stop_words='english')\n",
    "\n",
    "# vect = CountVectorizer(max_df=0.8, min_df=0.02, ngram_range=(2,2))\n",
    "count_matrix = vect.fit_transform(train_data.title)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 18s, sys: 1min 44s, total: 22min 2s\n",
      "Wall time: 2h 39min 15s\n"
     ]
    }
   ],
   "source": [
    "%time count_tsne_result = TSNE(learning_rate=300, init='pca')\\\n",
    "                    .fit_transform(np.array(count_matrix.toarray()))\n",
    "count_vect = normalize(count_tsne_result, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means- predetermined number of clusters\n",
    "nums =[8]\n",
    "from sklearn.cluster import KMeans\n",
    "from __future__ import print_function\n",
    "\n",
    "def run_kmeans(vect):\n",
    "#     for num in nums:\n",
    "#         print('cluster : %s' % str(num))\n",
    "    num_clusters = 8\n",
    "    km = KMeans(n_clusters=num_clusters,\\\n",
    "                random_state=0,\\\n",
    "                init='random',\\\n",
    "                algorithm='auto',\\\n",
    "                max_iter=30000)\n",
    "    %time km.fit(vect)\n",
    "    clusters = km.labels_.tolist()\n",
    "\n",
    "    documents = {'id':[x for x in processed_docs.id],\n",
    "                'content': train_data.title.tolist(),\n",
    "                 'title': processed_docs.title.tolist(),\n",
    "                'cluster':clusters}\n",
    "\n",
    "    clu_docu = pd.DataFrame(documents, index=[clusters], columns=['id','content','title','cluster'])\n",
    "\n",
    "    print(clu_docu['cluster'].value_counts())\n",
    "\n",
    "    #top words nearest to the cluster centroid\n",
    "    print('Top terms per clusters')\n",
    "    print()\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        print('Cluster %d words:' % i, end='')\n",
    "\n",
    "        for ind in order_centroids[i, :]:\n",
    "            print('%s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "        print()\n",
    "\n",
    "        print(\"Cluster %d titles:\" % i, end='')\n",
    "        adver_contents = [x for x in clu_docu.loc[i]['content'].tolist() if x.lower().find('advers')>=0]\n",
    "        print('count of adverse included in content %s' % str(len(adver_contents)))\n",
    "\n",
    "    return km, clu_docu, km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster : 8\n",
      "CPU times: user 517 ms, sys: 21.3 ms, total: 538 ms\n",
      "Wall time: 553 ms\n",
      "2    6515\n",
      "7    4416\n",
      "1    3628\n",
      "0    3268\n",
      "4    3248\n",
      "5    2871\n",
      "3    2696\n",
      "6    1318\n",
      "Name: cluster, dtype: int64\n",
      "Top terms per clusters\n",
      "\n",
      "Cluster 0 words:activation,according,\n",
      "Cluster 0 titles:count of adverse included in content 21\n",
      "Cluster 1 words:according,activation,\n",
      "Cluster 1 titles:count of adverse included in content 2\n",
      "Cluster 2 words:activation,according,\n",
      "Cluster 2 titles:count of adverse included in content 33\n",
      "Cluster 3 words:according,activation,\n",
      "Cluster 3 titles:count of adverse included in content 7\n",
      "Cluster 4 words:activation,according,\n",
      "Cluster 4 titles:count of adverse included in content 3\n",
      "Cluster 5 words:according,activation,\n",
      "Cluster 5 titles:count of adverse included in content 3\n",
      "Cluster 6 words:activation,according,\n",
      "Cluster 6 titles:count of adverse included in content 34\n",
      "Cluster 7 words:according,activation,\n",
      "Cluster 7 titles:count of adverse included in content 1094\n"
     ]
    }
   ],
   "source": [
    "km, tfidf_clu, tfidf_centers = run_kmeans(tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 s, sys: 53.2 ms, total: 18.7 s\n",
      "Wall time: 18.7 s\n",
      "1    15185\n",
      "3     3020\n",
      "2     2096\n",
      "7     2059\n",
      "4     1730\n",
      "0     1696\n",
      "6     1602\n",
      "5      572\n",
      "Name: cluster, dtype: int64\n",
      "Top terms per clusters\n",
      "\n",
      "Cluster 0 words:treatment,events,patients,adverse,effectiveness,population,analysis,groups,study,according,weeks,related,days,response,rate,months,safety,clinician,period,year,follow,change,risk,time,trials,incidence,number,drug,randomised,outcomes,reported,failure,hospitalization,states,death,medication,different,primary,mortality,summary,endpoints,efficacy,comparisons,data,ratio,versus,association,disease,infarction,comparative,myocardial,severely,cause,results,control,end,mean,overall,dose,parameters,type,heart,non,survival,measures,acute,estimated,therapy,coronial,subgroups,assessed,factors,frequencies,stratified,prevalences,total,adjusted,blood,concentrations,complications,post,score,second,variable,levels,month,hemodynamic,infection,angiographic,function,cohorts,specific,activation,class,hazards,major,pressure,cardiac,participants,diagnosed,proportional,points,cardiovascular,children,value,individual,procedures,age,selective,combined,deaths,test,model,basic,regression,subjects,stroke,women,index,presenting,high,case,intervals,flow,artery,left,regions,exercise,predicted,genes,distribution,significantly,ventricular,operation,odds,sensitivity,categories,multivariate,logistic,country,health,independent,correlates,india,cox,multivariable,sex,\n",
      "Cluster 0 titles:count of adverse included in content 306\n",
      "Cluster 1 words:events,risk,study,clinician,related,predicted,association,mortality,data,effectiveness,variable,results,adverse,rate,ratio,model,factors,coronial,according,disease,hemodynamic,death,follow,change,parameters,measures,cause,score,days,cardiac,comparisons,function,time,heart,incidence,test,states,population,hospitalization,angiographic,mean,different,procedures,adjusted,value,number,response,summary,myocardial,cardiovascular,prevalences,blood,levels,type,medication,concentrations,months,ventricular,case,regions,correlates,estimated,control,hazards,trials,frequencies,therapy,left,women,specific,health,complications,reported,primary,multivariate,dose,versus,endpoints,activation,distribution,regression,odds,infarction,end,cohorts,artery,participants,class,severely,exercise,comparative,flow,drug,country,non,weeks,basic,diagnosed,presenting,index,genes,infection,major,total,assessed,individual,pressure,operation,categories,efficacy,failure,india,survival,proportional,multivariable,subjects,second,deaths,points,month,stroke,significantly,period,selective,children,sex,randomised,cox,sensitivity,independent,combined,post,acute,high,stratified,overall,safety,logistic,subgroups,treatment,intervals,patients,analysis,age,outcomes,groups,year,\n",
      "Cluster 1 titles:count of adverse included in content 630\n",
      "Cluster 2 words:analysis,regression,multivariate,predicted,cox,logistic,variable,risk,factors,results,association,mortality,model,multivariable,death,events,coronial,patients,survival,hazards,proportional,related,clinician,cardiac,study,angiographic,year,independent,effectiveness,subgroups,sensitivity,disease,cardiovascular,adjusted,follow,significantly,heart,states,time,cause,outcomes,days,rate,artery,change,hospitalization,presenting,population,according,primary,score,control,stroke,parameters,trials,value,myocardial,measures,overall,efficacy,age,failure,ratio,points,infarction,major,function,data,endpoints,case,versus,correlates,cohorts,end,estimated,basic,post,concentrations,test,second,levels,months,left,index,randomised,blood,summary,incidence,ventricular,subjects,stratified,selective,different,adverse,acute,assessed,pressure,specific,treatment,total,combined,number,regions,response,non,comparisons,procedures,operation,participants,severely,high,flow,deaths,activation,weeks,prevalences,comparative,health,therapy,month,groups,individual,india,odds,mean,exercise,children,diagnosed,intervals,period,sex,women,dose,reported,infection,type,class,medication,complications,genes,safety,hemodynamic,country,categories,frequencies,distribution,drug,\n",
      "Cluster 2 titles:count of adverse included in content 25\n",
      "Cluster 3 words:patients,clinician,events,data,control,coronial,study,comparisons,follow,risk,cardiac,heart,disease,angiographic,hospitalization,mortality,results,predicted,variable,related,failure,analysis,according,rate,year,adverse,myocardial,versus,number,subjects,effectiveness,parameters,therapy,time,days,factors,death,hemodynamic,randomised,acute,infarction,function,ventricular,procedures,artery,exercise,change,model,levels,type,measures,medication,test,score,survival,diagnosed,frequencies,severely,states,left,population,incidence,index,response,weeks,association,complications,stroke,trials,presenting,age,proportional,different,cause,comparative,distribution,class,value,major,mean,cardiovascular,blood,primary,months,total,non,subgroups,basic,flow,summary,ratio,stratified,high,end,month,multivariate,significantly,infection,assessed,cohorts,activation,prevalences,operation,combined,individual,points,regression,adjusted,dose,hazards,specific,regions,concentrations,genes,selective,drug,reported,endpoints,pressure,period,sensitivity,india,post,outcomes,logistic,cox,second,multivariable,correlates,case,safety,overall,independent,categories,deaths,estimated,treatment,efficacy,groups,intervals,sex,country,health,odds,participants,children,women,\n",
      "Cluster 3 titles:count of adverse included in content 109\n",
      "Cluster 4 words:year,age,risk,mortality,rate,sex,adjusted,events,related,cause,follow,death,ratio,according,number,estimated,prevalences,incidence,disease,specific,children,predicted,states,women,factors,association,change,population,study,effectiveness,diagnosed,case,survival,months,heart,score,model,deaths,clinician,regions,country,mean,days,groups,coronial,cohorts,health,hospitalization,total,overall,cardiovascular,control,regression,variable,odds,distribution,time,cardiac,individual,levels,stratified,proportional,data,comparisons,measures,type,hazards,analysis,combined,different,severely,period,categories,function,test,participants,multivariate,infection,patients,stroke,india,major,class,results,medication,selective,versus,randomised,reported,adverse,trials,blood,myocardial,basic,index,failure,independent,weeks,value,subjects,high,correlates,comparative,infarction,primary,activation,non,outcomes,genes,treatment,intervals,pressure,frequencies,cox,presenting,multivariable,parameters,subgroups,dose,efficacy,end,endpoints,logistic,acute,sensitivity,operation,procedures,summary,artery,points,concentrations,ventricular,therapy,left,month,post,assessed,response,angiographic,significantly,second,exercise,complications,safety,drug,flow,hemodynamic,\n",
      "Cluster 4 titles:count of adverse included in content 30\n",
      "Cluster 5 words:intervals,effectiveness,ratio,groups,coronial,mortality,risk,patients,control,rate,odds,estimated,outcomes,prevalences,time,year,adjusted,hazards,related,change,predicted,period,variable,disease,model,comparisons,different,study,months,measures,events,clinician,incidence,follow,according,country,trials,mean,results,analysis,association,days,ventricular,medication,activation,basic,myocardial,left,levels,versus,treatment,women,randomised,age,multivariate,deaths,factors,india,infarction,procedures,value,total,summary,heart,function,selective,score,number,regions,artery,high,cardiovascular,death,infection,response,health,regression,data,survival,cohorts,hemodynamic,subgroups,children,concentrations,reported,hospitalization,individual,categories,specific,type,states,logistic,pressure,weeks,test,primary,acute,post,cardiac,index,end,blood,population,multivariable,sex,case,stratified,major,frequencies,non,cause,complications,correlates,comparative,significantly,parameters,month,dose,efficacy,subjects,endpoints,failure,independent,adverse,stroke,proportional,angiographic,assessed,overall,distribution,exercise,combined,second,drug,presenting,operation,diagnosed,participants,sensitivity,points,therapy,genes,flow,severely,class,safety,cox,\n",
      "Cluster 5 titles:count of adverse included in content 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 6 words:groups,treatment,patients,study,age,control,comparisons,risk,according,different,mortality,rate,events,outcomes,clinician,data,follow,mean,change,related,results,number,coronial,year,cause,effectiveness,variable,incidence,time,randomised,death,measures,score,analysis,ratio,parameters,states,predicted,hospitalization,hemodynamic,adverse,medication,days,value,comparative,months,sex,disease,exercise,factors,adjusted,distribution,dose,population,procedures,heart,index,function,period,myocardial,prevalences,trials,deaths,type,angiographic,cardiac,primary,test,model,high,blood,concentrations,ventricular,non,health,specific,women,association,weeks,total,survival,left,month,subjects,frequencies,diagnosed,flow,end,summary,odds,complications,regions,artery,levels,infarction,drug,country,points,intervals,versus,second,response,categories,proportional,therapy,pressure,endpoints,estimated,participants,activation,overall,presenting,post,severely,combined,case,reported,hazards,stratified,children,india,major,assessed,failure,class,cardiovascular,efficacy,sensitivity,individual,basic,operation,multivariate,cohorts,regression,acute,genes,infection,subgroups,safety,stroke,independent,correlates,selective,cox,logistic,significantly,multivariable,\n",
      "Cluster 6 titles:count of adverse included in content 45\n",
      "Cluster 7 words:outcomes,clinician,primary,second,patients,treatment,year,hospitalization,days,months,according,follow,study,procedures,measures,association,analysis,risk,related,predicted,month,effectiveness,safety,randomised,efficacy,population,stratified,events,adjusted,groups,states,angiographic,adverse,score,women,variable,comparisons,model,cohorts,comparative,data,rate,trials,ratio,versus,function,time,weeks,results,age,health,post,cardiovascular,hazards,type,cardiac,therapy,different,factors,change,mortality,operation,acute,summary,assessed,complications,basic,subgroups,major,children,incidence,death,disease,survival,control,estimated,levels,presenting,participants,medication,individual,period,mean,categories,regression,non,number,high,multivariate,cause,failure,overall,specific,independent,multivariable,drug,case,regions,reported,coronial,combined,selective,infarction,india,heart,index,points,cox,myocardial,end,infection,distribution,dose,severely,proportional,left,stroke,value,response,correlates,concentrations,total,ventricular,logistic,parameters,artery,class,activation,frequencies,test,subjects,intervals,diagnosed,sex,endpoints,sensitivity,significantly,genes,prevalences,odds,flow,hemodynamic,country,deaths,blood,exercise,pressure,\n",
      "Cluster 7 titles:count of adverse included in content 48\n"
     ]
    }
   ],
   "source": [
    "km, tfidf_clu, tfidf_centers = run_kmeans(tfidf_matrix)\n",
    "import pickle\n",
    "\n",
    "output = open('../data/output/kmeans.pkl', 'wb')\n",
    "pickle.dump({'topic_model': km}, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 147)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
