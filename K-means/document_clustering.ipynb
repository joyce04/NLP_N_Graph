{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster a set of documents using Python\n",
    "#identify the latent structures within the synopses of the top 100 films\n",
    "\n",
    "#1. tokenizing\n",
    "#2. stemming (*based on stemming lib, results change)- reduce a word to its stem or root form\n",
    "#3. calculate cosine distance between each document = measure of similarity\n",
    "#4. cluster documents using the k-means algorithm\n",
    "#5. using multidimensional scaling to reduce dimensionality within the corpus\n",
    "#6. conduct a hierarchical clustering on the corpus using Ward clustering\n",
    "#7. Latent Dirichlet Allocation(LDA)????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grace/workspace/keras/venv/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import psycopg2\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "def get_connection():\n",
    "    conn = psycopg2.connect(host='smith.snu.ac.kr', database='daeb', user='daeb', password='daeb123123', )\n",
    "    conn.set_client_encoding('UTF8')\n",
    "    return conn\n",
    "\n",
    "conn = get_connection()\n",
    "\n",
    "def get_article_tables():\n",
    "    curs = conn.cursor()\n",
    "    \n",
    "    select_sql = \"\"\"SELECT id, table_title, strip_tags(CONTENT) as content FROM article_tables order by id\"\"\" # limit 10000\n",
    "    curs.execute(select_sql)\n",
    "    return curs.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "# twenty_train.target_names\n",
    "# from nltk.corpus import brown\n",
    "# from keras.datasets import imdb\n",
    "# (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=1000)\n",
    "\n",
    "\n",
    "train_data = get_article_tables()\n",
    "\n",
    "# clean_content = [x['content'].lower() for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39826"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.DataFrame.from_dict(train_data)\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = {'nbsp':'', 'table':'', 'legend':'', 'mg/dl':'', 'g/l':'', 'yrs':'year', '\\n':' ', ';':'', 'kg/m2':'', 'n=':''}#, 'e.g', '(', ')'}\n",
    "\n",
    "# clean_content = [pattern.sub(lambda m: rep[re.escape(m.group(0))], x['content']) for x in train_data]\n",
    "\n",
    "rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "clean_content = [(pattern.sub(lambda m: rep[re.escape(m.group(0))], x[2])) for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Table 2  Patients’ Characteristics and Hospitalization or Clinic Visits During Follow-Up          Group 1 (15 patients) Group 2 (24 patients) p Value     Age 29 ± 16 25 ± 11 NS   Gender (F/M) 23/2 22/2 NS   Psychiatric diagnosis 100% (15/15) 100% (24/24) NS   Mean duration of symptoms (months) 20 ± 5 22 ± 5 NS   Median monthly number of ER or clinic visits preablation (range) 3 (2–6) 3 (1–5) NS   Median number of ER or clinic admissions 1-month postablation (range) 0 (0–2) 3 (2–5) &lt 0.05   Median number of phone contacts 1-month postablation (range) 4 (2–6) 1 (1–2) &lt 0.05   Median number of phone contacts during the third month postablation (range) 0 (0–1) None &lt 0.05     ER = emergency room.  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_content[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTable 1\\n\\nPatient Characteristics Pre-LVAD Implantation\\n\\n\\n\\n\\n\\n\\nAge, yrs\\n60(51,68)\\n\\n\\nMale\\n64(80)\\n\\n\\nBody mass index, kg/m\\n2\\n\\n\\n27(24,32)\\n\\n\\nDiabetes mellitus\\n22(28)\\n\\n\\nHypertension\\n33(41)\\n\\n\\nChronic HF etiology\\n\\n\\n\\n\\nIdiopathic CMP\\n37(46)\\n\\n\\n\\nIschemic CMP\\n34(43)\\n\\n\\n\\nValvular CMP\\n5(6)\\n\\n\\n\\nChemotherapy CMP\\n3(4)\\n\\n\\n\\nPeripartum CMP\\n1(1)\\n\\n\\nDuration of HF symptoms, yrs\\n5.5(1,10)\\n\\n\\nNYHA functional class\\n\\n\\n\\n\\nIII\\n29(36)\\n\\n\\n\\nIV\\n51(64)\\n\\n\\nLVEF, %\\n16(13,23)\\n\\n\\nLVEDD, cm\\n6.6(5.8,7.3)\\n\\n\\nCardiac index, l/(min· m\\n2\\n)\\n1.7(1.4,2.0)\\n\\n\\nIABP/Centrimag/ECMO\\n8(10)\\n\\n\\nInotrope dependence\\n56(70)\\n\\n\\nINTERMACS profile\\n\\n\\n\\n\\n1\\n9(11)\\n\\n\\n\\n2\\n12(15)\\n\\n\\n\\n3\\n39(49)\\n\\n\\n\\n4\\n16(20)\\n\\n\\n\\n5\\n3(4)\\n\\n\\n\\n6\\n1(1)\\n\\n\\nLVAD indication\\n\\n\\n\\n\\nBTT\\n52(65)\\n\\n\\n\\nBTD\\n3(4)\\n\\n\\n\\nDT\\n25(31)\\n\\n\\n\\n\\nValues are median (25th, 75th percentiles) or n (%). CentriMag (Ventricular Assist System, Thoratec Corp., Pleasanton, California).\\nBTD = bridge to decision; BTT = bridge to transplant; CMP = cardiomyopathy; DT = destination therapy; ECMO = extracorporeal membrane oxygenator; HF = heart failure; IABP = intra-aortic balloon pump; LVAD = left ventricular assist device; LVEDD = left ventricular end-diastolic diameter; LVEF = left ventricular ejection fraction; NYHA = New York Heart Association; INTERMACS = Interagency Registry for Mechanically Assisted Circulatory Support.\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1000][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub('[^A-Za-z ]+', '', train_data[5])\n",
    "# re.sub('[^A-Za-z0-9 ]+', '', train_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/Users/grace/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../share/nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/workspace/keras/venv/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/keras/venv/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/Users/grace/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../share/nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b248fdc86178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/keras/venv/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/keras/venv/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/keras/venv/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/keras/venv/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/Users/grace/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../share/nltk_data'\n    - '/Users/grace/workspace/keras/venv/bin/../lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "# -porter stemmer\n",
    "# -lancaster stemmer\n",
    "# -snowball stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# stemmer = SnowballStemmer('english')\n",
    "stemmer = LancasterStemmer()\n",
    "#tokenizing\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #filter tokens not containing letters\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token)>2:\n",
    "            filtered.append(token)\n",
    "#     stems = [stemmer.stem(t, pos='v') for t in filtered]\n",
    "    stems = [stemmer.stem(t) for t in filtered]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token)>2:\n",
    "            filtered.append(token)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_stemmed = []\n",
    "total_vocab_tokenized = []\n",
    "\n",
    "for i in clean_content:\n",
    "    all_stemmed = tokenize_and_stem(i)\n",
    "    total_vocab_stemmed.extend(all_stemmed)\n",
    "    \n",
    "    all_tokenized = tokenize_only(i)\n",
    "    total_vocab_tokenized.extend(all_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe with stemmed vocab and tokenized words (link)\n",
    "vocab_frame = pd.DataFrame({'words':total_vocab_tokenized}, index=total_vocab_stemmed)\n",
    "vocab_frame.drop_duplicates(inplace=True)\n",
    "vocab_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([brown.raw(__id) for __id in [_id for _id in brown.fileids()]])\n",
    "# data_list = [brown.raw(__id) for __id in [_id for _id in brown.fileids()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-idf and document similarity\n",
    "#frequency-inverse document frequencey(tf-idf) vectorize parameters and convert the document list into tf-idf matrix\n",
    "# 1. count word occurrences by document\n",
    "# 2. transform into a document-term matrix = term frequency matrix\n",
    "\n",
    "#max_df = max frequency within the documents\n",
    "#min_idf = if 5, the term would have to be in at least 5 of the documents to be considered, 0.2 = 20% of documents\n",
    "#ngram_ranges\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                                   max_features=200000, \n",
    "                                   min_df=0.01, \n",
    "                                   stop_words='english', \n",
    "                                   use_idf=True, \n",
    "                                   lowercase=True, \n",
    "                                   tokenizer=tokenize_and_stem, ngram_range=(1,2))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(clean_content)\n",
    "print()\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist = cosine similarity of each document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means- predetermined number of clusters\n",
    "# nums = [2,3,4,5,6,7,8,9,10]\n",
    "nums=[3, 4, 5]\n",
    "from sklearn.cluster import KMeans\n",
    "from __future__ import print_function\n",
    "\n",
    "for num in nums:\n",
    "    print('cluster : %s' % str(num))\n",
    "    num_clusters = num\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    %time km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "\n",
    "    documents = {'id':[x[0] for x in train_data],\n",
    "                'content':clean_content,\n",
    "                 'title': [x[1] for x in train_data],\n",
    "                'cluster':clusters}\n",
    "\n",
    "    clu_docu = pd.DataFrame(documents, index=[clusters], columns=['id','content','title','cluster'])\n",
    "\n",
    "    print(clu_docu['cluster'].value_counts())\n",
    "\n",
    "    #top words nearest to the cluster centroid\n",
    "    print('Top terms per clusters')\n",
    "    print()\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        print('Cluster %d words:' % i, end='')\n",
    "\n",
    "        for ind in order_centroids[i, :]:\n",
    "            print('%s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "        print()\n",
    "\n",
    "        print(\"Cluster %d titles:\" % i, end='')\n",
    "        title_list = clu_docu.loc[i]['title'].values.tolist()\n",
    "        adver_titles = [x for x in title_list if x.lower().find('adverse')>=0]\n",
    "        adver_contents = [x for x in clu_docu.loc[i]['content'].values.tolist() if x.lower().find('adverse')>=0]\n",
    "        print('count of adverse included in title %s' % str(len(adver_titles)))\n",
    "        print('count of adverse included in content %s' % str(len(adver_contents)))\n",
    "        print(adver_titles)\n",
    "#         print(' %s, \\n' % title_list[:30])\n",
    "    #         for title in title_list[:10]:\n",
    "    #             print(' %s, ' % title, end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "\n",
    "# joblib.dump(km, 'doc_cluster.pkl')\n",
    "# joblib.load('doc_cluster.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multidimensional scaling\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "pos = mds.fit_transform(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize document clusters\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
