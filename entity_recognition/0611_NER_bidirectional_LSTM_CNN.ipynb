{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM CNN for NER\n",
    "\n",
    "Code adapted from: https://github.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL\n",
    "https://github.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL/blob/master/nn_CoNLL.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, Flatten, concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import SGD, Nadam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE_NUMERIC = 'numeric'\n",
    "CASE_LOWER = 'all_lower'\n",
    "CASE_UPPER = 'all_upper'\n",
    "CASE_INITIAL_UPPER = 'initial_upper'\n",
    "CASE_OTHER = 'other'\n",
    "CASE_MAIN_NUMERIC = 'mainly_numeric'\n",
    "CASE_CONTAIN_DIGIT = 'contains_digit'\n",
    "CASE_PADDING = 'padding_token'\n",
    "\n",
    "CASE_IDX = {CASE_NUMERIC:0, CASE_LOWER:1, CASE_UPPER:2, CASE_INITIAL_UPPER:3, CASE_OTHER:4, CASE_MAIN_NUMERIC:5, CASE_CONTAIN_DIGIT:6, CASE_PADDING:7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    \"\"\" return format \n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'] ...]\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    \n",
    "    f = open(file_name, 'r')\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        if len(line)== 0 or line.startswith('-DOCSTART') or line=='\\n':\n",
    "            if len(sent) > 0:\n",
    "#                 print(sent)\n",
    "                sentences.append(sent)\n",
    "                sent = []\n",
    "            continue\n",
    "            \n",
    "        words = line.split(' ')\n",
    "        sent.append([words[0], words[-1]])\n",
    "\n",
    "    if len(sent)>0:\n",
    "        sentences.append(sent)\n",
    "        sent = []\n",
    "    return sentences\n",
    "\n",
    "sample_sents = read_file('/Users/grace/workspace/bio_dataset/CONLL2003/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: len(x)==0, sample_sents))\n",
    "# sample_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['EU', ['E', 'U'], 'B-ORG'],\n",
       "  ['rejects', ['r', 'e', 'j', 'e', 'c', 't', 's'], 'O'],\n",
       "  ['German', ['G', 'e', 'r', 'm', 'a', 'n'], 'B-MISC'],\n",
       "  ['call', ['c', 'a', 'l', 'l'], 'O'],\n",
       "  ['to', ['t', 'o'], 'O'],\n",
       "  ['boycott', ['b', 'o', 'y', 'c', 'o', 't', 't'], 'O'],\n",
       "  ['British', ['B', 'r', 'i', 't', 'i', 's', 'h'], 'B-MISC'],\n",
       "  ['lamb', ['l', 'a', 'm', 'b'], 'O'],\n",
       "  ['.', ['.'], 'O']],\n",
       " [['Peter', ['P', 'e', 't', 'e', 'r'], 'B-PER'],\n",
       "  ['Blackburn', ['B', 'l', 'a', 'c', 'k', 'b', 'u', 'r', 'n'], 'I-PER']]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_char_info(sentences):\n",
    "    \"\"\" return in format\n",
    "    [ ['EU', ['E', 'U'], 'B-ORG'], ['rejects', ['r', 'e', 'j', 'e', 'c', 't', 's'], 'O'], ...]\n",
    "    \"\"\"\n",
    "    for i, sent in enumerate(sentences):\n",
    "        for j, data in enumerate(sent):\n",
    "            chars = [c for c in data[0]]\n",
    "            sentences[i][j] = [data[0], chars, data[1]]\n",
    "    return sentences\n",
    "\n",
    "sample_sents = add_char_info(sample_sents)\n",
    "sample_sents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_dict():\n",
    "    char2idx = {'PADDING':0, 'UNKNOWN':1}\n",
    "    # all standard ascii chars\n",
    "#     for i in range(128):\n",
    "#         char2idx[chr(i)] = len(char2idx)\n",
    "    for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
    "        char2idx[c] = len(char2idx)\n",
    "\n",
    "    return char2idx\n",
    "\n",
    "# generate_char_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'numeric': 0,\n",
       " 'all_lower': 1,\n",
       " 'all_upper': 2,\n",
       " 'initial_upper': 3,\n",
       " 'other': 4,\n",
       " 'mainly_numeric': 5,\n",
       " 'contains_digit': 6,\n",
       " 'padding_token': 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case info such as NN can be used to learn patterns\n",
    "def get_case_info(word, case_lookup):\n",
    "    casing = CASE_OTHER\n",
    "    \n",
    "    num_digits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            num_digits += 1\n",
    "    \n",
    "    digit_fraction = num_digits / float(len(word))\n",
    "    \n",
    "    if word.isdigit():\n",
    "        casing = CASE_NUMERIC\n",
    "    elif digit_fraction > 0.5:\n",
    "        casing = CASE_MAIN_NUMERIC\n",
    "    elif num_digits > 0:\n",
    "        casing = CASE_CONTAIN_DIGIT\n",
    "    elif word.islower():\n",
    "        casing = CASE_LOWER\n",
    "    elif word.isupper():\n",
    "        casing = CASE_UPPER\n",
    "    elif word[0].isupper():\n",
    "        casing = CASE_INITIAL_UPPER\n",
    "\n",
    "    return case_lookup[casing]\n",
    "\n",
    "get_case_info('am i 123 d989f8ds', CASE_IDX)\n",
    "get_case_info('am i lower case sentences', CASE_IDX)\n",
    "CASE_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrices(sentences, word2idx, label2idx, case2idx, char2idx):\n",
    "    unknown_idx = word2idx['UNKNOWN_TOKEN']\n",
    "    padding_idx = word2idx['PADDING_TOKEN']\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    word_count = 0\n",
    "    unknown_word_count = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        word_indices = []\n",
    "        case_indices = []\n",
    "        char_indices = []\n",
    "        label_indices = []\n",
    "        \n",
    "        for word, char, label in sent:\n",
    "            word_count += 1\n",
    "            if word in word2idx:\n",
    "                word_idx = word2idx[word]\n",
    "            elif word.lower() in word2idx:\n",
    "                word_idx = word2idx[word.lower()]\n",
    "            else:\n",
    "                word_idx = unknown_idx\n",
    "                unknown_word_count += 1\n",
    "                \n",
    "            char_idx = []\n",
    "            for x in char:\n",
    "                char_idx.append(char2idx[x])\n",
    "            \n",
    "            #map label to int\n",
    "            word_indices.append(word_idx)\n",
    "            case_indices.append(get_case_info(word, case2idx))\n",
    "            char_indices.append(char_idx)\n",
    "            label_indices.append(label2idx[label])\n",
    "        \n",
    "        dataset.append([word_indices, case_indices, char_indices, label_indices])\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# generate_matrices(sample_sents[:5], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def padding(sentences, max_len):\n",
    "#     for sent in sentences:\n",
    "#         char = sent[2]\n",
    "        \n",
    "#         for x in char:\n",
    "#             max_len = max(max_len, len(x))\n",
    "            \n",
    "    for i, sent in enumerate(sentences):\n",
    "        sentences[i][2] = pad_sequences(sentences[i][2], max_len, padding='post', value=0, dtype='int32')\n",
    "    return sentences\n",
    "\n",
    "# padding(sample_sents[0], MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "#         print(i[0])\n",
    "        l.append(len(i[0]))\n",
    "    \n",
    "    l = set(l)\n",
    "    \n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    \n",
    "    z = 0\n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if len(batch[0])==i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "#         print(z)\n",
    "        batch_len.append(z)\n",
    "        \n",
    "#     print(batches[0])\n",
    "    return batches, batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_mini_batches(dataset, batch_len):\n",
    "#     print(dataset[0:2])\n",
    "#     print(batch_len)\n",
    "    start = 0\n",
    "    for i in batch_len:\n",
    "        tokens = []\n",
    "        casing = []\n",
    "        char = []\n",
    "        labels = []\n",
    "#         print(start)\n",
    "        data = dataset[start:i]\n",
    "        start = i\n",
    "\n",
    "#         print(i)\n",
    "#         print(data)\n",
    "        \n",
    "        for dt in data:\n",
    "            t, c, ch, l = dt\n",
    "#             print(l)\n",
    "            l = np.expand_dims(l, -1)\n",
    "            tokens.append(t)\n",
    "            casing.append(c)\n",
    "            char.append(ch)\n",
    "            labels.append(l)\n",
    "        \n",
    "#         print(tokens)\n",
    "        yield np.asarray(labels), np.asarray(tokens), np.asarray(casing), np.asarray(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object iterate_mini_batches at 0x1352a51b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate_mini_batches(sample_sents[:5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(pred_sentences, correct_sentences):\n",
    "    assert(len(pred_sentences)==len(correct_sentences))\n",
    "    correct_count = 0\n",
    "    count = 0\n",
    "    \n",
    "    for sent_idx in range(len(pred_sentences)):\n",
    "        pred = pred_sentences[sent_idx]\n",
    "        correct = correct_sentences[sent_idx]\n",
    "        assert(len(pred)==len(correct))\n",
    "        \n",
    "        idx = 0\n",
    "        while idx < len(pred):\n",
    "            if pred[idx][0]=='B': # start of a new chunk\n",
    "                count += 1\n",
    "                \n",
    "                if pred[idx]==correct[idx]: #first prediction correct\n",
    "                    idx += 1\n",
    "                    correctly_predicted = True\n",
    "                    \n",
    "                    while idx < len(pred) and pred[idx][0]=='I': #check entire chunk\n",
    "                        if pred[idx] != correct[idx]:\n",
    "                            correctly_predicted = False\n",
    "                        idx +=1 \n",
    "                        \n",
    "                    if idx < len(pred):\n",
    "                        if correct[idx][0]=='I': #correct chunk is longer\n",
    "                            correctly_predicted = False\n",
    "                    \n",
    "                    if correctly_predicted:\n",
    "                        correct_count += 1\n",
    "                        \n",
    "                else:\n",
    "                    idx +=1\n",
    "            else:\n",
    "                idx += 1\n",
    "                \n",
    "                \n",
    "    precision = 0\n",
    "    if count > 0:\n",
    "        precision = float(correct_count) / count\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(predictions, correct, idx2label):\n",
    "    \"\"\"compute accuracy\"\"\"\n",
    "    pred_labels = []\n",
    "    for sent in predictions:\n",
    "        pred_labels.append([idx2label[ele] for ele in sent])\n",
    "        \n",
    "    correct_labels = []\n",
    "    for sent in correct:\n",
    "        correct_labels.append([idx2label[ele] for ele in sent])\n",
    "    \n",
    "    prec = compute_precision(pred_labels, correct_labels)\n",
    "    rec = compute_precision(correct_labels, pred_labels)\n",
    "    \n",
    "    f1 = 0\n",
    "    if (rec + prec) > 0:\n",
    "        f1 = 2.0 * prec * rec / (prec + rec)\n",
    "    \n",
    "    return prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_gloVe(glove_loc, words):\n",
    "    word2idx = {}\n",
    "    word_embeddings = []\n",
    "\n",
    "    f_embeddings = open(glove_loc, encoding='utf-8')\n",
    "\n",
    "    for line in f_embeddings:\n",
    "        sp_l = line.strip().split(' ')\n",
    "        word = sp_l[0] #embedding word entry\n",
    "\n",
    "        if len(word2idx)==0: #add padding + unknown\n",
    "            word2idx['PADDING_TOKEN'] = len(word2idx)\n",
    "            vector = np.zeros(len(sp_l)-1) #zero vector for padding\n",
    "            word_embeddings.append(vector)\n",
    "\n",
    "            word2idx['UNKNOWN_TOKEN'] = len(word2idx)\n",
    "            vector = np.random.uniform(-0.25, 0.25, len(sp_l)-1)\n",
    "            word_embeddings.append(vector)\n",
    "\n",
    "        if sp_l[0].lower() in words:\n",
    "            vector = np.array([float(num) for num in sp_l[1:]])\n",
    "            word_embeddings.append(vector) #word embedding vector\n",
    "            word2idx[sp_l[0]] = len(word2idx) #word dict\n",
    "#     print(list(word2idx.keys())[:5])\n",
    "    return np.array(word_embeddings), word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class initalized.\n"
     ]
    }
   ],
   "source": [
    "# object mentioned to compatibility with 2.x\n",
    "class CNN_BLSTM(object):\n",
    "    \n",
    "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER):\n",
    "        self.epochs = EPOCHS\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
    "        self.lstm_state_size = LSTM_STATE_SIZE\n",
    "        self.conv_size = CONV_SIZE\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.optimizer = OPTIMIZER\n",
    "        \n",
    "    def load_data(self, file_locs):\n",
    "        \"\"\"Load data\"\"\"\n",
    "        train_loc = file_locs['train']\n",
    "        dev_loc = file_locs['dev']\n",
    "        test_loc = file_locs['test']\n",
    "        \n",
    "        self.trans_sentences = read_file(train_loc)\n",
    "        self.dev_sentences = read_file(dev_loc)\n",
    "        self.test_sentences = read_file(test_loc)\n",
    "        \n",
    "    def add_char(self):\n",
    "        \"\"\"Add char information\"\"\"\n",
    "        self.trans_sentences = add_char_info(self.trans_sentences)\n",
    "        self.dev_sentences = add_char_info(self.dev_sentences)\n",
    "        self.test_sentences = add_char_info(self.test_sentences)\n",
    "            \n",
    "    def embed(self, glove_loc):\n",
    "        \"\"\"word-\n",
    "        and char- level embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        #distinct labels\n",
    "        label_set = set()\n",
    "        #distinct tokens\n",
    "        words = {}\n",
    "        \n",
    "        for dataset in [self.trans_sentences, self.dev_sentences, self.test_sentences]:\n",
    "            for sent in dataset:\n",
    "                # ['EU', ['E', 'U'], 'B-ORG']\n",
    "                for token, char, label in sent:\n",
    "                    label_set.add(label)\n",
    "                    words[token.lower()] = True\n",
    "\n",
    "        # mapping for label\n",
    "        self.label2idx = {}\n",
    "        for idx, label in enumerate(label_set):\n",
    "            self.label2idx[label] = idx\n",
    "        \n",
    "        # mapping for char\n",
    "        case2idx = CASE_IDX\n",
    "        self.case_embeddings = np.identity(len(case2idx), dtype='float32')\n",
    "        \n",
    "        #apply GloVe embedding\n",
    "        self.word_embeddings, word2idx = embed_gloVe(glove_loc, words)\n",
    "        \n",
    "        #build dictionary for all chars\n",
    "        self.char2idx = generate_char_dict()\n",
    "        \n",
    "        #format : [[word indices], [case indices], [padded word indices], [label indices]]\n",
    "        self.train_set = padding(generate_matrices(self.trans_sentences, word2idx, self.label2idx, case2idx, self.char2idx), MAX_LEN)\n",
    "        self.dev_set = padding(generate_matrices(self.dev_sentences, word2idx, self.label2idx, case2idx, self.char2idx), MAX_LEN)\n",
    "        self.test_set = padding(generate_matrices(self.test_sentences, word2idx, self.label2idx, case2idx, self.char2idx), MAX_LEN)\n",
    "        \n",
    "        self.idx2label = {v:k for k, v in self.label2idx.items()}\n",
    "    \n",
    "    def create_batches(self):\n",
    "        self.train_batch, self.train_batch_len = generate_batches(self.train_set)\n",
    "        self.dev_batch, self.dev_batch_len = generate_batches(self.dev_set)\n",
    "        self.test_batch, self.test_batch_len = generate_batches(self.test_set)  \n",
    "        \n",
    "    def tag_dataset(self, dataset):\n",
    "        \"\"\"tag with numeric values\"\"\"\n",
    "        correct_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        for i, data in enumerate(dataset):\n",
    "            tokens, casing, char, labels = data\n",
    "            tokens = np.asarray([tokens])\n",
    "            casing = np.asarray([casing])\n",
    "            char = np.asarray([char])\n",
    "\n",
    "            pred = self.model.predict([tokens, casing, char], verbose=False)[0]\n",
    "            pred = pred.argmax(axis=1) #class\n",
    "\n",
    "            correct_labels.append(labels)\n",
    "            pred_labels.append(pred)\n",
    "\n",
    "        return pred_labels, correct_labels\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        # char input\n",
    "        char_input = Input(shape=(None, MAX_LEN,), name='char_input')\n",
    "        embed_char_out = TimeDistributed(\n",
    "            Embedding(len(self.char2idx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(char_input)\n",
    "        drop_out = Dropout(self.dropout)(embed_char_out)\n",
    "        \n",
    "        # CNN\n",
    "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name='Convolution')(drop_out)\n",
    "        maxpool_out = TimeDistributed(MaxPooling1D(MAX_LEN), name='Maxpool')(conv1d_out)\n",
    "        char = TimeDistributed(Flatten(), name='Flatten')(maxpool_out)\n",
    "        char = Dropout(self.dropout)(char)\n",
    "        \n",
    "        # word-level input\n",
    "        words_input = Input(shape=(None,), dtype='int32', name='word_input')\n",
    "        words = Embedding(input_dim=self.word_embeddings.shape[0], output_dim=self.word_embeddings.shape[1], weights=[self.word_embeddings], trainable=False)(words_input)\n",
    "        \n",
    "        # case-info input\n",
    "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "        casing = Embedding(input_dim=self.case_embeddings.shape[0], output_dim=self.case_embeddings.shape[1], weights=[self.case_embeddings], trainable=False)(casing_input)\n",
    "        \n",
    "        # concat & Bidirectional LSTM\n",
    "        output = concatenate([words, casing, char])\n",
    "        #dropout - on input to each LSTM block\n",
    "        #recurrent dropout - on recurrent input signal\n",
    "        output = Bidirectional(LSTM(self.lstm_state_size, return_sequences=True, dropout=self.dropout, recurrent_dropout=self.dropout_recurrent), name='BLSTM')(output)\n",
    "        output = TimeDistributed(Dense(len(self.label2idx), activation='softmax'), name='Softmax')(output)\n",
    "        \n",
    "        #model set up\n",
    "        self.model = Model(inputs=[words_input, casing_input, char_input], outputs=[output])\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
    "        self.init_wegiths = self.model.get_weights()\n",
    "        plot_model(self.model, to_file='./model.png')\n",
    "        print('Model is built. Saved model.png\\n')\n",
    "        \n",
    "    def train(self):\n",
    "        self.f1_test_history = []\n",
    "        self.f1_dev_history = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print('Epoch {}/{}'.format(epoch+1, self.epochs))\n",
    "            \n",
    "#             print(self.train_batch[0])\n",
    "            for batch in iterate_mini_batches(self.train_batch, self.train_batch_len):\n",
    "                labels, tokens, casing, char = batch\n",
    "#                 print(tokens)\n",
    "                self.model.train_on_batch([tokens, casing, char], labels)\n",
    "            \n",
    "            #compute f1\n",
    "            pred_labels, correct_labels = self.tag_dataset(self.test_batch)\n",
    "            pre_test, rec_test, f1_test = compute_f1(pred_labels, correct_labels, self.idx2label)\n",
    "            self.f1_test_history.append(round(f1_test, 4))\n",
    "            print('F1 Test::: ', round(f1_test, 4))\n",
    "            \n",
    "            pred_labels, correct_labels = self.tag_dataset(self.dev_batch)\n",
    "            pre_dev, rec_dev, f1_dev = compute_f1(pred_labels, correct_labels, self.idx2label)\n",
    "            self.f1_dev_history.append(round(f1_dev, 4))\n",
    "            print('F1 DEV::: ', round(f1_dev, 4), '\\n')\n",
    "        \n",
    "        print('Final F1 test score: ', f1_test)\n",
    "        print('Training Finished.')\n",
    "        \n",
    "        #save model\n",
    "        self.model_name = \"{}_{}_{}_{}_{}_{}_{}\".format(self.epochs,\n",
    "                                                       self.dropout,\n",
    "                                                       self.dropout_recurrent,\n",
    "                                                       self.lstm_state_size,\n",
    "                                                       self.conv_size,\n",
    "                                                       self.learning_rate,\n",
    "                                                       self.optimizer.__class__.__name__)\n",
    "        self.model.save(self.model_name+'.h5')\n",
    "        print('Model Weights Saved.')\n",
    "        \n",
    "        self.model.set_weights(self.init_wegiths) #clear model\n",
    "        print('Model Weights Cleared')\n",
    "        \n",
    "    def write_to_file(self):\n",
    "        # .txt file format\n",
    "        # [epoch]\n",
    "        # [f1_test]\n",
    "        # [f1_dev]\n",
    "        \n",
    "        ouput = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
    "        with open(self.model_name+'.txt', 'wb') as f:\n",
    "            for line in ouput:\n",
    "                print(line[0])\n",
    "#                 np.savetxt(f, str(line[0]), fmt='%.5f')\n",
    "                \n",
    "        print('Model performance written to file.')\n",
    "    \n",
    "    def plot_progress(self):\n",
    "        fig, acc_ax = plt.subplots()\n",
    "        \n",
    "#         acc_ax = loss_ax.twinx()\n",
    "        acc_ax.plot(self.f1_test_history, 'y', label='Test F1 Score')\n",
    "        acc_ax.plot(self.f1_dev_history, 'r', label='Dev F1 Score')\n",
    "        \n",
    "        acc_ax.set_xlabel('epoch')\n",
    "        acc_ax.set_ylabel('F1 Score')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    print('Class initalized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/grace/workspace/keras/annon-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "EPOCHS = 50 #paper 80\n",
    "DROPOUT = 0.5 #paper 0.68\n",
    "DROPOUT_RECURRENT = 0.25 \n",
    "LSTM_STATE_SIZE = 200 #paper 275\n",
    "CONV_SIZE = 3 #paper 3\n",
    "LEARNING_RATE = 0.0105 #paper 0.015\n",
    "OPTIMIZER = Nadam() #paper SGD\n",
    "MAX_LEN = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/grace/workspace/keras/annon-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model is built. Saved model.png\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /Users/grace/workspace/keras/annon-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "F1 Test:::  0.3896\n",
      "F1 DEV:::  0.4147 \n",
      "\n",
      "Epoch 2/50\n",
      "F1 Test:::  0.5956\n",
      "F1 DEV:::  0.6441 \n",
      "\n",
      "Epoch 3/50\n",
      "F1 Test:::  0.6638\n",
      "F1 DEV:::  0.7051 \n",
      "\n",
      "Epoch 4/50\n",
      "F1 Test:::  0.7442\n",
      "F1 DEV:::  0.7714 \n",
      "\n",
      "Epoch 5/50\n",
      "F1 Test:::  0.7451\n",
      "F1 DEV:::  0.7549 \n",
      "\n",
      "Epoch 6/50\n",
      "F1 Test:::  0.7606\n",
      "F1 DEV:::  0.7892 \n",
      "\n",
      "Epoch 7/50\n",
      "F1 Test:::  0.7985\n",
      "F1 DEV:::  0.8185 \n",
      "\n",
      "Epoch 8/50\n",
      "F1 Test:::  0.7949\n",
      "F1 DEV:::  0.8101 \n",
      "\n",
      "Epoch 9/50\n",
      "F1 Test:::  0.8035\n",
      "F1 DEV:::  0.8196 \n",
      "\n",
      "Epoch 10/50\n",
      "F1 Test:::  0.7997\n",
      "F1 DEV:::  0.8134 \n",
      "\n",
      "Epoch 11/50\n",
      "F1 Test:::  0.817\n",
      "F1 DEV:::  0.8312 \n",
      "\n",
      "Epoch 12/50\n",
      "F1 Test:::  0.8133\n",
      "F1 DEV:::  0.8318 \n",
      "\n",
      "Epoch 13/50\n",
      "F1 Test:::  0.8316\n",
      "F1 DEV:::  0.8489 \n",
      "\n",
      "Epoch 14/50\n",
      "F1 Test:::  0.8214\n",
      "F1 DEV:::  0.8343 \n",
      "\n",
      "Epoch 15/50\n",
      "F1 Test:::  0.8284\n",
      "F1 DEV:::  0.8429 \n",
      "\n",
      "Epoch 16/50\n",
      "F1 Test:::  0.8291\n",
      "F1 DEV:::  0.8398 \n",
      "\n",
      "Epoch 17/50\n",
      "F1 Test:::  0.8398\n",
      "F1 DEV:::  0.8595 \n",
      "\n",
      "Epoch 18/50\n",
      "F1 Test:::  0.8435\n",
      "F1 DEV:::  0.8616 \n",
      "\n",
      "Epoch 19/50\n",
      "F1 Test:::  0.8316\n",
      "F1 DEV:::  0.8525 \n",
      "\n",
      "Epoch 20/50\n",
      "F1 Test:::  0.8476\n",
      "F1 DEV:::  0.8787 \n",
      "\n",
      "Epoch 21/50\n",
      "F1 Test:::  0.8578\n",
      "F1 DEV:::  0.8806 \n",
      "\n",
      "Epoch 22/50\n",
      "F1 Test:::  0.8559\n",
      "F1 DEV:::  0.8814 \n",
      "\n",
      "Epoch 23/50\n",
      "F1 Test:::  0.8599\n",
      "F1 DEV:::  0.8872 \n",
      "\n",
      "Epoch 24/50\n",
      "F1 Test:::  0.8646\n",
      "F1 DEV:::  0.8894 \n",
      "\n",
      "Epoch 25/50\n",
      "F1 Test:::  0.8443\n",
      "F1 DEV:::  0.8704 \n",
      "\n",
      "Epoch 26/50\n",
      "F1 Test:::  0.8634\n",
      "F1 DEV:::  0.8912 \n",
      "\n",
      "Epoch 27/50\n",
      "F1 Test:::  0.8721\n",
      "F1 DEV:::  0.8979 \n",
      "\n",
      "Epoch 28/50\n",
      "F1 Test:::  0.8717\n",
      "F1 DEV:::  0.8963 \n",
      "\n",
      "Epoch 29/50\n",
      "F1 Test:::  0.8607\n",
      "F1 DEV:::  0.8891 \n",
      "\n",
      "Epoch 30/50\n",
      "F1 Test:::  0.8698\n",
      "F1 DEV:::  0.8992 \n",
      "\n",
      "Epoch 31/50\n",
      "F1 Test:::  0.8729\n",
      "F1 DEV:::  0.904 \n",
      "\n",
      "Epoch 32/50\n",
      "F1 Test:::  0.8787\n",
      "F1 DEV:::  0.9102 \n",
      "\n",
      "Epoch 33/50\n",
      "F1 Test:::  0.8752\n",
      "F1 DEV:::  0.9047 \n",
      "\n",
      "Epoch 34/50\n",
      "F1 Test:::  0.8713\n",
      "F1 DEV:::  0.9003 \n",
      "\n",
      "Epoch 35/50\n",
      "F1 Test:::  0.8762\n",
      "F1 DEV:::  0.9063 \n",
      "\n",
      "Epoch 36/50\n",
      "F1 Test:::  0.8767\n",
      "F1 DEV:::  0.9078 \n",
      "\n",
      "Epoch 37/50\n",
      "F1 Test:::  0.8691\n",
      "F1 DEV:::  0.9038 \n",
      "\n",
      "Epoch 38/50\n",
      "F1 Test:::  0.8805\n",
      "F1 DEV:::  0.9094 \n",
      "\n",
      "Epoch 39/50\n",
      "F1 Test:::  0.8764\n",
      "F1 DEV:::  0.9045 \n",
      "\n",
      "Epoch 40/50\n",
      "F1 Test:::  0.8746\n",
      "F1 DEV:::  0.908 \n",
      "\n",
      "Epoch 41/50\n",
      "F1 Test:::  0.8777\n",
      "F1 DEV:::  0.9089 \n",
      "\n",
      "Epoch 42/50\n",
      "F1 Test:::  0.8716\n",
      "F1 DEV:::  0.9079 \n",
      "\n",
      "Epoch 43/50\n",
      "F1 Test:::  0.8773\n",
      "F1 DEV:::  0.9103 \n",
      "\n",
      "Epoch 44/50\n",
      "F1 Test:::  0.8838\n",
      "F1 DEV:::  0.9164 \n",
      "\n",
      "Epoch 45/50\n",
      "F1 Test:::  0.8838\n",
      "F1 DEV:::  0.916 \n",
      "\n",
      "Epoch 46/50\n",
      "F1 Test:::  0.8827\n",
      "F1 DEV:::  0.9151 \n",
      "\n",
      "Epoch 47/50\n",
      "F1 Test:::  0.8815\n",
      "F1 DEV:::  0.9175 \n",
      "\n",
      "Epoch 48/50\n",
      "F1 Test:::  0.8834\n",
      "F1 DEV:::  0.9191 \n",
      "\n",
      "Epoch 49/50\n",
      "F1 Test:::  0.8799\n",
      "F1 DEV:::  0.9156 \n",
      "\n",
      "Epoch 50/50\n",
      "F1 Test:::  0.8851\n",
      "F1 DEV:::  0.9205 \n",
      "\n",
      "Final F1 test score:  0.8850898836799436\n",
      "Training Finished.\n",
      "Model Weights Saved.\n",
      "Model Weights Cleared\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      "  18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      "  36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]]\n",
      "[[0.3896 0.5956 0.6638 0.7442 0.7451 0.7606 0.7985 0.7949 0.8035 0.7997\n",
      "  0.817  0.8133 0.8316 0.8214 0.8284 0.8291 0.8398 0.8435 0.8316 0.8476\n",
      "  0.8578 0.8559 0.8599 0.8646 0.8443 0.8634 0.8721 0.8717 0.8607 0.8698\n",
      "  0.8729 0.8787 0.8752 0.8713 0.8762 0.8767 0.8691 0.8805 0.8764 0.8746\n",
      "  0.8777 0.8716 0.8773 0.8838 0.8838 0.8827 0.8815 0.8834 0.8799 0.8851]]\n",
      "[[0.4147 0.6441 0.7051 0.7714 0.7549 0.7892 0.8185 0.8101 0.8196 0.8134\n",
      "  0.8312 0.8318 0.8489 0.8343 0.8429 0.8398 0.8595 0.8616 0.8525 0.8787\n",
      "  0.8806 0.8814 0.8872 0.8894 0.8704 0.8912 0.8979 0.8963 0.8891 0.8992\n",
      "  0.904  0.9102 0.9047 0.9003 0.9063 0.9078 0.9038 0.9094 0.9045 0.908\n",
      "  0.9089 0.9079 0.9103 0.9164 0.916  0.9151 0.9175 0.9191 0.9156 0.9205]]\n",
      "Model performance written to file.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Construction\n",
    "\n",
    "FILE_LOCS = {'train':'/Users/grace/workspace/bio_dataset/CONLL2003/train.txt',\n",
    "             'dev':'/Users/grace/workspace/bio_dataset/CONLL2003/valid.txt',\n",
    "             'test':'/Users/grace/workspace/bio_dataset/CONLL2003/test.txt'}\n",
    "GLOVE_LOC = '/Users/grace/workspace/embeddings/glove.6B/glove.6B.50d.txt'\n",
    "\n",
    "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
    "cnn_blstm.load_data(FILE_LOCS)\n",
    "cnn_blstm.add_char()\n",
    "cnn_blstm.embed(GLOVE_LOC)\n",
    "cnn_blstm.create_batches()\n",
    "cnn_blstm.build_model()\n",
    "cnn_blstm.train()\n",
    "cnn_blstm.write_to_file()\n",
    "cnn_blstm.plot_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label distribution\n",
    "category_count = {\"B-ORG\": 0, \"I-ORG\":0, \"B-MISC\": 0, \"I-MISC\":0, \"B-LOC\": 0, \"I-LOC\": 0, \"B-PER\": 0, \"I-PER\": 0, \"O\": 0}\n",
    "total_count = 0\n",
    "\n",
    "for sentence in cnn_blstm.trans_sentences:\n",
    "    for word in sentence:\n",
    "        if word[2] in category_count.keys():\n",
    "            category_count[word[2]] += 1\n",
    "            total_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-ORG: 3.1%\n",
      "I-ORG: 1.82%\n",
      "B-MISC: 1.69%\n",
      "I-MISC: 0.57%\n",
      "B-LOC: 3.51%\n",
      "I-LOC: 0.57%\n",
      "B-PER: 3.24%\n",
      "I-PER: 2.22%\n",
      "O: 83.28%\n"
     ]
    }
   ],
   "source": [
    "for category, count in category_count.items():\n",
    "    print(\"{}: {}%\".format(category.replace(\"\\n\", \"\"), round((count/total_count)*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annon-env",
   "language": "python",
   "name": "annon-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
