{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    - https://github.com/tkipf/keras-gcn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# graph convolution\n",
    "from keras import activations, initializers, constraints\n",
    "from keras import regularizers\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Layer):\n",
    "    \"\"\" Graph convolution layer as in https://arxiv.org/abs/1609.02907\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 units, \n",
    "                 support=1,\n",
    "                activation=None,\n",
    "                use_bias=True,\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None,\n",
    "                **kwargs):\n",
    "        assert support >= 1\n",
    "        \n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'), )\n",
    "        \n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.supports_masking = True\n",
    "        \n",
    "        self.support = support\n",
    "    \n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        features_shape = input_shapes[0]\n",
    "        output_shape = (features_shape[0], self.units)\n",
    "        return output_shape\n",
    "    \n",
    "    def build(self, input_shapes):\n",
    "        features_shape = input_shapes[0]\n",
    "#         print(features_shape)\n",
    "        assert len(features_shape) == 2\n",
    "        input_dim = features_shape[1]\n",
    "        \n",
    "        self.kernel = self.add_weight(shape=(input_dim * self.support, self.units),\n",
    "                                     initializer=self.kernel_initializer,\n",
    "                                     name='kernel',\n",
    "                                     regularizer=self.kernel_regularizer,\n",
    "                                     constraint=self.kernel_constraint)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units, ),\n",
    "                                       initializer= self.bias_initializer,\n",
    "                                       name='bias',\n",
    "                                       regularizer=self.bias_regularizer,\n",
    "                                       constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.built = True\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        features = inputs[0]\n",
    "        basis = inputs[1:]\n",
    "        \n",
    "        supports = list()\n",
    "        for i in range(self.support):\n",
    "            supports.append(K.dot(basis[i], features))\n",
    "        supports = K.concatenate(supports, axis=1)\n",
    "        output = K.dot(supports, self.kernel)\n",
    "        \n",
    "        if self.bias:\n",
    "            output += self.bias\n",
    "        return self.activation(output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'units': self.units,\n",
    "               'support': self.support,\n",
    "               'activiation': activations.serialize(self.activation),\n",
    "               'use_bias': self.use_bias,\n",
    "               'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "               'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "               'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "                'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "                'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "                'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "                'bias_constraint':constraints.serialize(self.bias_constraint)\n",
    "               }\n",
    "        \n",
    "        \n",
    "        base_config = super(GraphConvolution, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cora'\n",
    "FILTER = 'localpool' #chebyshev\n",
    "MAX_DEGREE = 2\n",
    "SYM_NORM = True # symmetric\n",
    "NB_EPOCH = 200\n",
    "PATIENCE = 10 # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def load_data(path='./datasets/CORA_origin/', dataset='cora'):\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "    \n",
    "    #graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "    \n",
    "    adj = adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    print('Dataset has {} nodes, {} edges, {} features.'.format(adj.shape[0], edges.shape[0], features.shape[1]))\n",
    "    \n",
    "    return features.todense(), adj, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "X, A, y = load_data(dataset=DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def get_splits(y):\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "    y_train = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_val = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_test = np.zeros(y.shape, dtype=np.int32)\n",
    "    y_train[idx_train] = y[idx_train]\n",
    "    y_val[idx_val] = y[idx_val]\n",
    "    y_test[idx_test] = y[idx_test]\n",
    "    train_mask = sample_mask(idx_train, y.shape[0])\n",
    "    return y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1698670605613"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratio of labels\n",
    "140/2708 *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X /= X.sum(1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg.eigen.arpack import eigsh, ArpackNoConvergence\n",
    "\n",
    "def normalize_adj(adj, symmetric=True):\n",
    "    if symmetric:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -0.5).flatten(), 0)\n",
    "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
    "    else:\n",
    "        d = sp.diags(np.power(np.array(adj.sum(1)), -1).flatten(), 0)\n",
    "        a_norm = adj.dot(d).transpose().dot(d).tocsr()\n",
    "    return a_norm\n",
    "\n",
    "def preprocess_adj(adj, symmetric=True):\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    return normalize_adj(adj, symmetric)\n",
    "\n",
    "def normalized_laplacian(adj, symmetric=True):\n",
    "    adj_n = normalize_adj(adj, symmetric)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_n\n",
    "    return laplacian\n",
    "\n",
    "def rescale_laplacian(laplacian):\n",
    "    try:\n",
    "        print('Calculating largest eigenvalue of normalized graph laplacian')\n",
    "        largest_eigenval = eigsh(laplacian, 1, which='LM', return_eigenvectors=False)[0]\n",
    "    except ArpackNoConvergence:\n",
    "        print('Eigevnvalue calculation did not converge! Using largest_eigenvaaal=2 instead.')\n",
    "        largest_eigenval = 2\n",
    "        \n",
    "    scaled_laplacian = (2. / largest_eigenval) * laplacian - sp.eye(laplacian.shape[0])\n",
    "    return scaled_laplacian\n",
    "\n",
    "def chebyshev_polynomial(X, k):\n",
    "    \"\"\" calculate chebyshev polynomials up to order k\"\"\"\n",
    "    T_k = list()\n",
    "    T_k.append(sp.eye(X.shape[0]).tocsr())\n",
    "    T_k.append(X)\n",
    "    \n",
    "    def chebyshev_recurrence(T_k_minus_one, T_k_minus_two, X):\n",
    "        X_ = sp.csr_matrix(X, copy=True)\n",
    "        return 2 * X_.dot(T_k_minus_one) - T_k_minus_two\n",
    "    \n",
    "    for i in range(2, k+1):\n",
    "        T_k.append(chebyshev_recurrence(T_k[-1], T_k[-2], X))\n",
    "    \n",
    "    return T_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0715 11:45:43.509906 139835335409792 deprecation_wrapper.py:119] From /home/madigun/.pyenv/versions/3.7.3/envs/annon/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0715 11:45:43.518188 139835335409792 deprecation_wrapper.py:119] From /home/madigun/.pyenv/versions/3.7.3/envs/annon/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if FILTER == 'localpool':\n",
    "    \"\"\" renormalization trick in Kipf & Welling, arXiv 2016 \"\"\"\n",
    "    A_ = preprocess_adj(A, SYM_NORM)\n",
    "    support = 1\n",
    "    graph = [X, A_]\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None))]\n",
    "\n",
    "elif FILTER == 'chebyshev':\n",
    "    \"\"\" chebyshev Defferard et al. 2016\"\"\"\n",
    "    L = normalized_laplacian(A, SYM_NORM)\n",
    "    L_scaled = rescale_laplacian(L)\n",
    "    T_k = chebyshev_polynomial(L_scaled, MAX_DEGREE)\n",
    "    support = MAX_DEGREE + 1\n",
    "    graph = [X] + T_k\n",
    "    G = [Input(shape=(None, None), batch_shape=(None, None)) for i in range(support)]\n",
    "\n",
    "else:\n",
    "    raise Exception('Invalid filter type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = Input(shape=(X.shape[1], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 11:45:43.528494 139835335409792 deprecation_wrapper.py:119] From /home/madigun/.pyenv/versions/3.7.3/envs/annon/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0715 11:45:43.533531 139835335409792 deprecation.py:506] From /home/madigun/.pyenv/versions/3.7.3/envs/annon/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0715 11:45:43.582360 139835335409792 deprecation_wrapper.py:119] From /home/madigun/.pyenv/versions/3.7.3/envs/annon/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model architecture\n",
    "# pass arguments for graph convolutional layers as a list of tensors\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(20, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 11:45:43.628032 139835335409792 deprecation_wrapper.py:119] From /home/madigun/.pyenv/versions/3.7.3/envs/annon/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0715 11:45:43.633022 139835335409792 deprecation_wrapper.py:119] From /home/madigun/.pyenv/versions/3.7.3/envs/annon/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compile\n",
    "model = Model(inputs=[X_in] + G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_crossentropy(preds, labels):\n",
    "    return np.mean(-np.log(np.extract(labels, preds)))\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(np.equal(np.argmax(labels, 1), np.argmax(preds, 1)))\n",
    "\n",
    "def evaluate_preds (preds, labels, indices):\n",
    "    split_loss = list()\n",
    "    split_acc = list()\n",
    "    \n",
    "    for y_split, idx_split, in zip(labels, indices):\n",
    "        split_loss.append(categorical_crossentropy(preds[idx_split], y_split[idx_split]))\n",
    "        split_acc.append(accuracy(preds[idx_split], y_split[idx_split]))\n",
    "        \n",
    "    return split_loss, split_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 11:45:43.718542 139835335409792 deprecation.py:323] From /home/madigun/.pyenv/versions/3.7.3/envs/annon/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9278 train_acc= 0.4143 val_loss= 1.9365 val_acc= 0.3133 time= 0.6058\n",
      "Epoch: 0002 train_loss= 1.9101 train_acc= 0.4071 val_loss= 1.9279 val_acc= 0.3600 time= 0.1779\n",
      "Epoch: 0003 train_loss= 1.8911 train_acc= 0.4286 val_loss= 1.9195 val_acc= 0.3767 time= 0.1710\n",
      "Epoch: 0004 train_loss= 1.8708 train_acc= 0.4500 val_loss= 1.9112 val_acc= 0.3933 time= 0.1646\n",
      "Epoch: 0005 train_loss= 1.8491 train_acc= 0.4643 val_loss= 1.9023 val_acc= 0.4267 time= 0.1752\n",
      "Epoch: 0006 train_loss= 1.8270 train_acc= 0.4500 val_loss= 1.8935 val_acc= 0.4200 time= 0.1672\n",
      "Epoch: 0007 train_loss= 1.8049 train_acc= 0.4357 val_loss= 1.8851 val_acc= 0.3967 time= 0.1716\n",
      "Epoch: 0008 train_loss= 1.7843 train_acc= 0.4143 val_loss= 1.8786 val_acc= 0.3900 time= 0.1803\n",
      "Epoch: 0009 train_loss= 1.7648 train_acc= 0.4143 val_loss= 1.8732 val_acc= 0.3900 time= 0.1671\n",
      "Epoch: 0010 train_loss= 1.7456 train_acc= 0.4357 val_loss= 1.8687 val_acc= 0.3933 time= 0.1683\n",
      "Epoch: 0011 train_loss= 1.7276 train_acc= 0.4571 val_loss= 1.8647 val_acc= 0.4167 time= 0.1633\n",
      "Epoch: 0012 train_loss= 1.7104 train_acc= 0.4786 val_loss= 1.8616 val_acc= 0.4233 time= 0.1684\n",
      "Epoch: 0013 train_loss= 1.6942 train_acc= 0.4714 val_loss= 1.8589 val_acc= 0.4200 time= 0.1767\n",
      "Epoch: 0014 train_loss= 1.6789 train_acc= 0.4929 val_loss= 1.8566 val_acc= 0.4167 time= 0.1713\n",
      "Epoch: 0015 train_loss= 1.6641 train_acc= 0.4786 val_loss= 1.8540 val_acc= 0.4133 time= 0.1683\n",
      "Epoch: 0016 train_loss= 1.6496 train_acc= 0.4786 val_loss= 1.8503 val_acc= 0.4100 time= 0.1731\n",
      "Epoch: 0017 train_loss= 1.6350 train_acc= 0.4786 val_loss= 1.8452 val_acc= 0.3933 time= 0.1651\n",
      "Epoch: 0018 train_loss= 1.6206 train_acc= 0.4857 val_loss= 1.8388 val_acc= 0.4000 time= 0.1850\n",
      "Epoch: 0019 train_loss= 1.6063 train_acc= 0.4786 val_loss= 1.8312 val_acc= 0.4100 time= 0.1707\n",
      "Epoch: 0020 train_loss= 1.5928 train_acc= 0.4786 val_loss= 1.8227 val_acc= 0.4133 time= 0.1752\n",
      "Epoch: 0021 train_loss= 1.5796 train_acc= 0.4786 val_loss= 1.8146 val_acc= 0.4200 time= 0.1763\n",
      "Epoch: 0022 train_loss= 1.5666 train_acc= 0.4929 val_loss= 1.8063 val_acc= 0.4167 time= 0.1749\n",
      "Epoch: 0023 train_loss= 1.5537 train_acc= 0.5000 val_loss= 1.7973 val_acc= 0.4167 time= 0.1678\n",
      "Epoch: 0024 train_loss= 1.5411 train_acc= 0.5143 val_loss= 1.7887 val_acc= 0.4167 time= 0.1623\n",
      "Epoch: 0025 train_loss= 1.5285 train_acc= 0.5214 val_loss= 1.7794 val_acc= 0.4200 time= 0.1676\n",
      "Epoch: 0026 train_loss= 1.5163 train_acc= 0.5429 val_loss= 1.7700 val_acc= 0.4267 time= 0.1730\n",
      "Epoch: 0027 train_loss= 1.5045 train_acc= 0.5500 val_loss= 1.7610 val_acc= 0.4267 time= 0.1740\n",
      "Epoch: 0028 train_loss= 1.4930 train_acc= 0.5571 val_loss= 1.7524 val_acc= 0.4267 time= 0.1744\n",
      "Epoch: 0029 train_loss= 1.4814 train_acc= 0.5571 val_loss= 1.7439 val_acc= 0.4233 time= 0.1697\n",
      "Epoch: 0030 train_loss= 1.4700 train_acc= 0.5571 val_loss= 1.7360 val_acc= 0.4300 time= 0.1714\n",
      "Epoch: 0031 train_loss= 1.4589 train_acc= 0.5571 val_loss= 1.7282 val_acc= 0.4300 time= 0.1742\n",
      "Epoch: 0032 train_loss= 1.4480 train_acc= 0.5571 val_loss= 1.7207 val_acc= 0.4333 time= 0.1695\n",
      "Epoch: 0033 train_loss= 1.4374 train_acc= 0.5571 val_loss= 1.7131 val_acc= 0.4333 time= 0.1652\n",
      "Epoch: 0034 train_loss= 1.4272 train_acc= 0.5571 val_loss= 1.7061 val_acc= 0.4367 time= 0.1620\n",
      "Epoch: 0035 train_loss= 1.4171 train_acc= 0.5571 val_loss= 1.6990 val_acc= 0.4433 time= 0.1660\n",
      "Epoch: 0036 train_loss= 1.4073 train_acc= 0.5571 val_loss= 1.6922 val_acc= 0.4367 time= 0.1712\n",
      "Epoch: 0037 train_loss= 1.3982 train_acc= 0.5571 val_loss= 1.6865 val_acc= 0.4367 time= 0.1645\n",
      "Epoch: 0038 train_loss= 1.3894 train_acc= 0.5571 val_loss= 1.6813 val_acc= 0.4400 time= 0.1692\n",
      "Epoch: 0039 train_loss= 1.3805 train_acc= 0.5571 val_loss= 1.6751 val_acc= 0.4400 time= 0.1661\n",
      "Epoch: 0040 train_loss= 1.3716 train_acc= 0.5571 val_loss= 1.6679 val_acc= 0.4400 time= 0.1678\n",
      "Epoch: 0041 train_loss= 1.3631 train_acc= 0.5571 val_loss= 1.6610 val_acc= 0.4467 time= 0.1703\n",
      "Epoch: 0042 train_loss= 1.3546 train_acc= 0.5571 val_loss= 1.6545 val_acc= 0.4467 time= 0.1666\n",
      "Epoch: 0043 train_loss= 1.3466 train_acc= 0.5571 val_loss= 1.6485 val_acc= 0.4467 time= 0.1645\n",
      "Epoch: 0044 train_loss= 1.3390 train_acc= 0.5571 val_loss= 1.6428 val_acc= 0.4533 time= 0.1661\n",
      "Epoch: 0045 train_loss= 1.3318 train_acc= 0.5571 val_loss= 1.6375 val_acc= 0.4500 time= 0.1638\n",
      "Epoch: 0046 train_loss= 1.3248 train_acc= 0.5571 val_loss= 1.6323 val_acc= 0.4467 time= 0.1652\n",
      "Epoch: 0047 train_loss= 1.3177 train_acc= 0.5571 val_loss= 1.6270 val_acc= 0.4467 time= 0.1685\n",
      "Epoch: 0048 train_loss= 1.3107 train_acc= 0.5571 val_loss= 1.6221 val_acc= 0.4467 time= 0.1697\n",
      "Epoch: 0049 train_loss= 1.3038 train_acc= 0.5571 val_loss= 1.6173 val_acc= 0.4467 time= 0.1627\n",
      "Epoch: 0050 train_loss= 1.2972 train_acc= 0.5571 val_loss= 1.6130 val_acc= 0.4600 time= 0.1652\n",
      "Epoch: 0051 train_loss= 1.2909 train_acc= 0.5571 val_loss= 1.6094 val_acc= 0.4600 time= 0.1668\n",
      "Epoch: 0052 train_loss= 1.2848 train_acc= 0.5571 val_loss= 1.6059 val_acc= 0.4500 time= 0.1691\n",
      "Epoch: 0053 train_loss= 1.2789 train_acc= 0.5571 val_loss= 1.6017 val_acc= 0.4500 time= 0.1675\n",
      "Epoch: 0054 train_loss= 1.2731 train_acc= 0.5571 val_loss= 1.5968 val_acc= 0.4500 time= 0.1653\n",
      "Epoch: 0055 train_loss= 1.2674 train_acc= 0.5643 val_loss= 1.5920 val_acc= 0.4533 time= 0.1689\n",
      "Epoch: 0056 train_loss= 1.2620 train_acc= 0.5643 val_loss= 1.5865 val_acc= 0.4567 time= 0.1720\n",
      "Epoch: 0057 train_loss= 1.2568 train_acc= 0.5643 val_loss= 1.5810 val_acc= 0.4567 time= 0.1646\n",
      "Epoch: 0058 train_loss= 1.2518 train_acc= 0.5643 val_loss= 1.5758 val_acc= 0.4500 time= 0.1652\n",
      "Epoch: 0059 train_loss= 1.2471 train_acc= 0.5643 val_loss= 1.5720 val_acc= 0.4600 time= 0.1671\n",
      "Epoch: 0060 train_loss= 1.2425 train_acc= 0.5643 val_loss= 1.5687 val_acc= 0.4600 time= 0.1669\n",
      "Epoch: 0061 train_loss= 1.2379 train_acc= 0.5714 val_loss= 1.5654 val_acc= 0.4633 time= 0.1601\n",
      "Epoch: 0062 train_loss= 1.2335 train_acc= 0.5714 val_loss= 1.5625 val_acc= 0.4733 time= 0.1675\n",
      "Epoch: 0063 train_loss= 1.2290 train_acc= 0.5714 val_loss= 1.5593 val_acc= 0.4733 time= 0.1755\n",
      "Epoch: 0064 train_loss= 1.2244 train_acc= 0.5714 val_loss= 1.5560 val_acc= 0.4767 time= 0.1715\n",
      "Epoch: 0065 train_loss= 1.2200 train_acc= 0.5714 val_loss= 1.5536 val_acc= 0.4733 time= 0.1657\n",
      "Epoch: 0066 train_loss= 1.2156 train_acc= 0.5714 val_loss= 1.5509 val_acc= 0.4667 time= 0.1692\n",
      "Epoch: 0067 train_loss= 1.2115 train_acc= 0.5714 val_loss= 1.5484 val_acc= 0.4600 time= 0.1609\n",
      "Epoch: 0068 train_loss= 1.2076 train_acc= 0.5714 val_loss= 1.5463 val_acc= 0.4600 time= 0.1685\n",
      "Epoch: 0069 train_loss= 1.2043 train_acc= 0.5714 val_loss= 1.5446 val_acc= 0.4633 time= 0.1716\n",
      "Epoch: 0070 train_loss= 1.2011 train_acc= 0.5786 val_loss= 1.5424 val_acc= 0.4667 time= 0.1792\n",
      "Epoch: 0071 train_loss= 1.1982 train_acc= 0.5786 val_loss= 1.5419 val_acc= 0.4700 time= 0.1933\n",
      "Epoch: 0072 train_loss= 1.1952 train_acc= 0.5786 val_loss= 1.5411 val_acc= 0.4667 time= 0.1666\n",
      "Epoch: 0073 train_loss= 1.1918 train_acc= 0.5786 val_loss= 1.5381 val_acc= 0.4700 time= 0.1655\n",
      "Epoch: 0074 train_loss= 1.1884 train_acc= 0.5857 val_loss= 1.5348 val_acc= 0.4700 time= 0.1610\n",
      "Epoch: 0075 train_loss= 1.1849 train_acc= 0.5857 val_loss= 1.5312 val_acc= 0.4733 time= 0.1592\n",
      "Epoch: 0076 train_loss= 1.1814 train_acc= 0.5857 val_loss= 1.5268 val_acc= 0.4733 time= 0.1664\n",
      "Epoch: 0077 train_loss= 1.1778 train_acc= 0.5857 val_loss= 1.5213 val_acc= 0.4733 time= 0.1719\n",
      "Epoch: 0078 train_loss= 1.1746 train_acc= 0.5857 val_loss= 1.5166 val_acc= 0.4667 time= 0.1675\n",
      "Epoch: 0079 train_loss= 1.1720 train_acc= 0.5786 val_loss= 1.5130 val_acc= 0.4600 time= 0.1644\n",
      "Epoch: 0080 train_loss= 1.1695 train_acc= 0.5786 val_loss= 1.5108 val_acc= 0.4567 time= 0.1636\n",
      "Epoch: 0081 train_loss= 1.1674 train_acc= 0.5714 val_loss= 1.5095 val_acc= 0.4500 time= 0.1687\n",
      "Epoch: 0082 train_loss= 1.1651 train_acc= 0.5643 val_loss= 1.5084 val_acc= 0.4500 time= 0.1677\n",
      "Epoch: 0083 train_loss= 1.1620 train_acc= 0.5714 val_loss= 1.5067 val_acc= 0.4500 time= 0.1708\n",
      "Epoch: 0084 train_loss= 1.1583 train_acc= 0.5857 val_loss= 1.5046 val_acc= 0.4533 time= 0.1726\n",
      "Epoch: 0085 train_loss= 1.1552 train_acc= 0.5857 val_loss= 1.5029 val_acc= 0.4667 time= 0.1616\n",
      "Epoch: 0086 train_loss= 1.1527 train_acc= 0.5857 val_loss= 1.5014 val_acc= 0.4667 time= 0.1646\n",
      "Epoch: 0087 train_loss= 1.1501 train_acc= 0.5857 val_loss= 1.5004 val_acc= 0.4667 time= 0.1647\n",
      "Epoch: 0088 train_loss= 1.1473 train_acc= 0.5929 val_loss= 1.4995 val_acc= 0.4700 time= 0.1654\n",
      "Epoch: 0089 train_loss= 1.1447 train_acc= 0.5929 val_loss= 1.4980 val_acc= 0.4700 time= 0.1648\n",
      "Epoch: 0090 train_loss= 1.1420 train_acc= 0.5929 val_loss= 1.4967 val_acc= 0.4733 time= 0.1732\n",
      "Epoch: 0091 train_loss= 1.1394 train_acc= 0.5929 val_loss= 1.4948 val_acc= 0.4733 time= 0.1637\n",
      "Epoch: 0092 train_loss= 1.1368 train_acc= 0.5929 val_loss= 1.4927 val_acc= 0.4733 time= 0.1650\n",
      "Epoch: 0093 train_loss= 1.1344 train_acc= 0.6071 val_loss= 1.4919 val_acc= 0.4833 time= 0.1685\n",
      "Epoch: 0094 train_loss= 1.1320 train_acc= 0.6071 val_loss= 1.4898 val_acc= 0.4833 time= 0.1642\n",
      "Epoch: 0095 train_loss= 1.1294 train_acc= 0.6071 val_loss= 1.4861 val_acc= 0.4833 time= 0.1814\n",
      "Epoch: 0096 train_loss= 1.1271 train_acc= 0.6071 val_loss= 1.4833 val_acc= 0.4767 time= 0.1745\n",
      "Epoch: 0097 train_loss= 1.1251 train_acc= 0.6071 val_loss= 1.4804 val_acc= 0.4733 time= 0.1711\n",
      "Epoch: 0098 train_loss= 1.1231 train_acc= 0.6071 val_loss= 1.4780 val_acc= 0.4767 time= 0.1770\n",
      "Epoch: 0099 train_loss= 1.1209 train_acc= 0.6071 val_loss= 1.4771 val_acc= 0.4767 time= 0.1647\n",
      "Epoch: 0100 train_loss= 1.1190 train_acc= 0.6071 val_loss= 1.4762 val_acc= 0.4767 time= 0.1778\n",
      "Epoch: 0101 train_loss= 1.1169 train_acc= 0.6071 val_loss= 1.4750 val_acc= 0.4767 time= 0.1762\n",
      "Epoch: 0102 train_loss= 1.1148 train_acc= 0.6071 val_loss= 1.4740 val_acc= 0.4767 time= 0.1604\n",
      "Epoch: 0103 train_loss= 1.1125 train_acc= 0.6071 val_loss= 1.4742 val_acc= 0.4833 time= 0.1615\n",
      "Epoch: 0104 train_loss= 1.1104 train_acc= 0.6071 val_loss= 1.4727 val_acc= 0.4833 time= 0.1801\n",
      "Epoch: 0105 train_loss= 1.1085 train_acc= 0.6071 val_loss= 1.4717 val_acc= 0.4767 time= 0.1678\n",
      "Epoch: 0106 train_loss= 1.1069 train_acc= 0.6071 val_loss= 1.4699 val_acc= 0.4767 time= 0.1675\n",
      "Epoch: 0107 train_loss= 1.1058 train_acc= 0.6071 val_loss= 1.4689 val_acc= 0.4767 time= 0.1668\n",
      "Epoch: 0108 train_loss= 1.1047 train_acc= 0.6071 val_loss= 1.4687 val_acc= 0.4800 time= 0.1739\n",
      "Epoch: 0109 train_loss= 1.1036 train_acc= 0.6071 val_loss= 1.4699 val_acc= 0.4867 time= 0.1657\n",
      "Epoch: 0110 train_loss= 1.1027 train_acc= 0.6071 val_loss= 1.4729 val_acc= 0.4833 time= 0.1675\n",
      "Epoch: 0111 train_loss= 1.1022 train_acc= 0.6071 val_loss= 1.4750 val_acc= 0.4833 time= 0.1662\n",
      "Epoch: 0112 train_loss= 1.0992 train_acc= 0.6071 val_loss= 1.4685 val_acc= 0.4867 time= 0.1708\n",
      "Epoch: 0113 train_loss= 1.0966 train_acc= 0.6071 val_loss= 1.4624 val_acc= 0.4833 time= 0.1705\n",
      "Epoch: 0114 train_loss= 1.0948 train_acc= 0.6071 val_loss= 1.4588 val_acc= 0.4833 time= 0.1687\n",
      "Epoch: 0115 train_loss= 1.0940 train_acc= 0.6071 val_loss= 1.4581 val_acc= 0.4733 time= 0.1675\n",
      "Epoch: 0116 train_loss= 1.0925 train_acc= 0.6000 val_loss= 1.4582 val_acc= 0.4700 time= 0.1653\n",
      "Epoch: 0117 train_loss= 1.0899 train_acc= 0.6071 val_loss= 1.4572 val_acc= 0.4733 time= 0.1693\n",
      "Epoch: 0118 train_loss= 1.0871 train_acc= 0.6071 val_loss= 1.4557 val_acc= 0.4767 time= 0.1648\n",
      "Epoch: 0119 train_loss= 1.0836 train_acc= 0.6071 val_loss= 1.4513 val_acc= 0.4933 time= 0.1636\n",
      "Epoch: 0120 train_loss= 1.0818 train_acc= 0.6071 val_loss= 1.4514 val_acc= 0.4867 time= 0.1662\n",
      "Epoch: 0121 train_loss= 1.0811 train_acc= 0.6071 val_loss= 1.4546 val_acc= 0.4900 time= 0.1650\n",
      "Epoch: 0122 train_loss= 1.0803 train_acc= 0.6071 val_loss= 1.4571 val_acc= 0.4833 time= 0.1656\n",
      "Epoch: 0123 train_loss= 1.0790 train_acc= 0.6071 val_loss= 1.4569 val_acc= 0.4867 time= 0.1661\n",
      "Epoch: 0124 train_loss= 1.0767 train_acc= 0.6071 val_loss= 1.4531 val_acc= 0.4933 time= 0.1615\n",
      "Epoch: 0125 train_loss= 1.0741 train_acc= 0.6071 val_loss= 1.4487 val_acc= 0.4867 time= 0.1649\n",
      "Epoch: 0126 train_loss= 1.0713 train_acc= 0.6071 val_loss= 1.4456 val_acc= 0.4900 time= 0.1674\n",
      "Epoch: 0127 train_loss= 1.0685 train_acc= 0.6071 val_loss= 1.4432 val_acc= 0.4900 time= 0.1633\n",
      "Epoch: 0128 train_loss= 1.0660 train_acc= 0.6071 val_loss= 1.4413 val_acc= 0.4900 time= 0.1664\n",
      "Epoch: 0129 train_loss= 1.0639 train_acc= 0.6071 val_loss= 1.4419 val_acc= 0.4867 time= 0.1644\n",
      "Epoch: 0130 train_loss= 1.0621 train_acc= 0.6143 val_loss= 1.4443 val_acc= 0.4900 time= 0.1640\n",
      "Epoch: 0131 train_loss= 1.0603 train_acc= 0.6143 val_loss= 1.4447 val_acc= 0.4867 time= 0.1648\n",
      "Epoch: 0132 train_loss= 1.0584 train_acc= 0.6143 val_loss= 1.4434 val_acc= 0.4833 time= 0.1663\n",
      "Epoch: 0133 train_loss= 1.0564 train_acc= 0.6143 val_loss= 1.4417 val_acc= 0.4800 time= 0.1693\n",
      "Epoch: 0134 train_loss= 1.0545 train_acc= 0.6143 val_loss= 1.4399 val_acc= 0.4833 time= 0.1689\n",
      "Epoch: 0135 train_loss= 1.0527 train_acc= 0.6143 val_loss= 1.4391 val_acc= 0.4867 time= 0.1651\n",
      "Epoch: 0136 train_loss= 1.0511 train_acc= 0.6071 val_loss= 1.4374 val_acc= 0.4800 time= 0.1638\n",
      "Epoch: 0137 train_loss= 1.0495 train_acc= 0.6071 val_loss= 1.4362 val_acc= 0.4800 time= 0.1657\n",
      "Epoch: 0138 train_loss= 1.0479 train_acc= 0.6071 val_loss= 1.4338 val_acc= 0.4900 time= 0.1681\n",
      "Epoch: 0139 train_loss= 1.0461 train_acc= 0.6143 val_loss= 1.4313 val_acc= 0.4933 time= 0.1716\n",
      "Epoch: 0140 train_loss= 1.0447 train_acc= 0.6143 val_loss= 1.4303 val_acc= 0.5000 time= 0.1715\n",
      "Epoch: 0141 train_loss= 1.0434 train_acc= 0.6143 val_loss= 1.4297 val_acc= 0.5000 time= 0.1669\n",
      "Epoch: 0142 train_loss= 1.0421 train_acc= 0.6214 val_loss= 1.4296 val_acc= 0.5033 time= 0.1669\n",
      "Epoch: 0143 train_loss= 1.0405 train_acc= 0.6214 val_loss= 1.4282 val_acc= 0.5033 time= 0.1663\n",
      "Epoch: 0144 train_loss= 1.0385 train_acc= 0.6214 val_loss= 1.4254 val_acc= 0.4933 time= 0.1676\n",
      "Epoch: 0145 train_loss= 1.0372 train_acc= 0.6214 val_loss= 1.4247 val_acc= 0.4900 time= 0.1682\n",
      "Epoch: 0146 train_loss= 1.0358 train_acc= 0.6286 val_loss= 1.4245 val_acc= 0.4900 time= 0.1661\n",
      "Epoch: 0147 train_loss= 1.0340 train_acc= 0.6286 val_loss= 1.4232 val_acc= 0.4900 time= 0.1671\n",
      "Epoch: 0148 train_loss= 1.0321 train_acc= 0.6286 val_loss= 1.4230 val_acc= 0.4933 time= 0.1657\n",
      "Epoch: 0149 train_loss= 1.0305 train_acc= 0.6214 val_loss= 1.4215 val_acc= 0.4933 time= 0.1665\n",
      "Epoch: 0150 train_loss= 1.0294 train_acc= 0.6214 val_loss= 1.4226 val_acc= 0.4900 time= 0.1673\n",
      "Epoch: 0151 train_loss= 1.0285 train_acc= 0.6286 val_loss= 1.4260 val_acc= 0.5000 time= 0.1700\n",
      "Epoch: 0152 train_loss= 1.0284 train_acc= 0.6429 val_loss= 1.4303 val_acc= 0.5033 time= 0.1668\n",
      "Epoch: 0153 train_loss= 1.0276 train_acc= 0.6429 val_loss= 1.4292 val_acc= 0.5000 time= 0.1694\n",
      "Epoch: 0154 train_loss= 1.0264 train_acc= 0.6286 val_loss= 1.4256 val_acc= 0.4967 time= 0.1681\n",
      "Epoch: 0155 train_loss= 1.0253 train_acc= 0.6286 val_loss= 1.4232 val_acc= 0.4900 time= 0.1681\n",
      "Epoch: 0156 train_loss= 1.0235 train_acc= 0.6286 val_loss= 1.4189 val_acc= 0.4867 time= 0.1661\n",
      "Epoch: 0157 train_loss= 1.0218 train_acc= 0.6286 val_loss= 1.4149 val_acc= 0.4900 time= 0.1650\n",
      "Epoch: 0158 train_loss= 1.0202 train_acc= 0.6286 val_loss= 1.4120 val_acc= 0.4833 time= 0.1640\n",
      "Epoch: 0159 train_loss= 1.0192 train_acc= 0.6214 val_loss= 1.4107 val_acc= 0.4900 time= 0.1651\n",
      "Epoch: 0160 train_loss= 1.0190 train_acc= 0.6143 val_loss= 1.4119 val_acc= 0.4900 time= 0.1675\n",
      "Epoch: 0161 train_loss= 1.0175 train_acc= 0.6357 val_loss= 1.4152 val_acc= 0.5000 time= 0.1684\n",
      "Epoch: 0162 train_loss= 1.0179 train_acc= 0.6643 val_loss= 1.4236 val_acc= 0.5033 time= 0.1644\n",
      "Epoch: 0163 train_loss= 1.0184 train_acc= 0.6643 val_loss= 1.4306 val_acc= 0.5100 time= 0.1678\n",
      "Epoch: 0164 train_loss= 1.0182 train_acc= 0.6714 val_loss= 1.4343 val_acc= 0.5200 time= 0.1706\n",
      "Epoch: 0165 train_loss= 1.0160 train_acc= 0.6643 val_loss= 1.4306 val_acc= 0.5067 time= 0.1672\n",
      "Epoch: 0166 train_loss= 1.0131 train_acc= 0.6571 val_loss= 1.4234 val_acc= 0.5033 time= 0.1690\n",
      "Epoch: 0167 train_loss= 1.0100 train_acc= 0.6500 val_loss= 1.4163 val_acc= 0.4933 time= 0.1703\n",
      "Epoch: 0168 train_loss= 1.0081 train_acc= 0.6429 val_loss= 1.4107 val_acc= 0.4867 time= 0.1677\n",
      "Epoch: 0169 train_loss= 1.0069 train_acc= 0.6357 val_loss= 1.4072 val_acc= 0.4900 time= 0.1644\n",
      "Epoch: 0170 train_loss= 1.0062 train_acc= 0.6214 val_loss= 1.4070 val_acc= 0.4867 time= 0.1663\n",
      "Epoch: 0171 train_loss= 1.0065 train_acc= 0.6071 val_loss= 1.4103 val_acc= 0.4833 time= 0.1681\n",
      "Epoch: 0172 train_loss= 1.0054 train_acc= 0.6214 val_loss= 1.4099 val_acc= 0.4867 time= 0.1610\n",
      "Epoch: 0173 train_loss= 1.0041 train_acc= 0.6214 val_loss= 1.4092 val_acc= 0.4900 time= 0.1645\n",
      "Epoch: 0174 train_loss= 1.0023 train_acc= 0.6286 val_loss= 1.4083 val_acc= 0.5000 time= 0.1638\n",
      "Epoch: 0175 train_loss= 1.0012 train_acc= 0.6500 val_loss= 1.4090 val_acc= 0.5033 time= 0.1685\n",
      "Epoch: 0176 train_loss= 1.0006 train_acc= 0.6571 val_loss= 1.4114 val_acc= 0.5200 time= 0.1640\n",
      "Epoch: 0177 train_loss= 0.9999 train_acc= 0.6714 val_loss= 1.4132 val_acc= 0.5133 time= 0.1655\n",
      "Epoch: 0178 train_loss= 0.9991 train_acc= 0.6786 val_loss= 1.4141 val_acc= 0.5167 time= 0.1658\n",
      "Epoch: 0179 train_loss= 0.9975 train_acc= 0.6786 val_loss= 1.4125 val_acc= 0.5200 time= 0.1684\n",
      "Epoch: 0180 train_loss= 0.9955 train_acc= 0.6786 val_loss= 1.4107 val_acc= 0.5133 time= 0.1669\n",
      "Epoch: 0181 train_loss= 0.9942 train_acc= 0.6786 val_loss= 1.4104 val_acc= 0.5100 time= 0.1708\n",
      "Epoch 181: early stopping\n",
      "time for training 0.17099332809448242\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NB_EPOCH + 1):\n",
    "    t = time.time()\n",
    "    \n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "             batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "    \n",
    "    # predict full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "    \n",
    "    # train/ validation\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val], [idx_train, idx_val])\n",
    "    \n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "    \n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1\n",
    "        \n",
    "print('time for training {}'.format(str(time.time() - t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 1.4783 accuracy= 0.4720\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(\"Test set results:\",\n",
    "      \"loss= {:.4f}\".format(test_loss[0]),\n",
    "      \"accuracy= {:.4f}\".format(test_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Annotaion Kernel",
   "language": "python",
   "name": "annon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
