{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for Finding Table Titles Relevant to Drug-Adverse Events\n",
    "\n",
    "- The following codes is implemented for topic modeling through LDA in attempt to find document samples(table title data) relevant to drug and adverse events.\n",
    "- Topic modeling is one of statistical modeling for discovering the abstract 'topics' that occur in a collection of document.\n",
    " - An example of topic model is used to classify text in a document to a particular topic\n",
    " - it builds a topic per document model and words per topic model, modeled as Dirichlet distributions\n",
    "- Following urls and research articles were referenced.\n",
    " - https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    " - http://www.engear.net/wp/topic-modeling-gensimpython/\n",
    " - https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html\n",
    " - https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#14computemodelperplexityandcoherencescore\n",
    " - https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html\n",
    " - http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    " - http://jeriwieringa.com/2018/07/17/pyLDAviz-and-Mallet/#comment-4018495276\n",
    " - https://ldavis.cpsievert.me/reviews/reviews.html\n",
    " - https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if text is missing\n",
      "id       False\n",
      "title    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45352, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve table titles from csv\n",
    "data = pd.read_csv('titles.csv', delimiter='\\t', error_bad_lines=True, header=None)\n",
    "data.columns = ['id', 'title']\n",
    "data.title = data.title.str.strip()\n",
    "documents = data\n",
    "documents['title'].replace('', np.nan, inplace=True)\n",
    "documents = documents.astype(str)\n",
    "print('checking if text is missing')\n",
    "print(documents.isna().any())\n",
    "# documents.dropna(subset=['title'], inplace=True)\n",
    "documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing\n",
    "- tokenization : \n",
    "  - split the text into sentences and then sentences into words\n",
    "  - lower case words\n",
    "  - remove punctuation\n",
    "  - remove words with fewer than 2 characters\n",
    "  - remove all stopwords + added ('table', 'legend')\n",
    "- Lemmatization : \n",
    " - verbs in past and future tenses are changed into present\n",
    "- Stemmazation :\n",
    " - words are reduced to their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/grace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "STOP_WORDS = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "STOP_WORDS.extend(['table', 'legend'])\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "#     deacc=True removes punctuations\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True):\n",
    "        if token not in STOP_WORDS and len(token)>1:\n",
    "#             result.append(lemmatize_stemming(strip_numeric(token)))\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 52 ms, total: 11.7 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#process the text, save the results as processed_docs\n",
    "processed_docs = pd.DataFrame()\n",
    "processed_docs = pd.concat([documents.id, documents.title.map(preprocess)], axis=1)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trigram model : implement n-grams with Gensim Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING MIN_COUNT\n",
    "import itertools\n",
    "\n",
    "def check_trends_with_min_count(min_count):\n",
    "    bigram = gensim.models.Phrases(processed_docs.title, min_count=min_count, threshold=1)\n",
    "    trigram = gensim.models.Phrases(bigram[processed_docs.title], threshold=1)\n",
    "\n",
    "    #sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    #generated n-grams\n",
    "    n_grams = []\n",
    "    word_2d = processed_docs.map(lambda x: trigram_mod[bigram_mod[x]])\n",
    "    for words in word_2d:\n",
    "        n_grams.extend(list(filter(lambda x : x.find('_')>=0, words)))\n",
    "    return set(n_grams)\n",
    "\n",
    "# n_gram_file = open('n_gram_by_min_count.csv', 'w')\n",
    "# test_min_counts = list(itertools.chain(range(1, 2), range(50, 500, 50)))\n",
    "# n_gram_matrix = []\n",
    "# matrix_index = 0\n",
    "# for i in reversed(test_min_counts):\n",
    "#     print('min_count %d' %i)\n",
    "#     print('matrix_index %d' %matrix_index)\n",
    "#     if matrix_index > 0:\n",
    "#         n_gram_matrix.append(list(set(check_trends_with_min_count(i)) - set(n_gram_matrix[matrix_index-1])))\n",
    "#     else:\n",
    "#         n_gram_matrix.append(list(set(check_trends_with_min_count(i))))\n",
    "#     n_gram_file.write(str(i)+'\\t'+str(n_gram_matrix[matrix_index])+'\\n')\n",
    "#     matrix_index += 1\n",
    "\n",
    "# n_gram_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 954 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#min_count = ignore all words and bigrams with total collected count lower than this value\n",
    "#threshold = represent a score threshold for forming the phrases(higher means fewer phrases)\n",
    "bigram = gensim.models.Phrases(processed_docs, min_count=1, threshold=0.2)\n",
    "trigram = gensim.models.Phrases(bigram[processed_docs], threshold=0.2)\n",
    "\n",
    "#sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigram(text):\n",
    "    processed_text = [[word for word in simple_preprocess(str(doc)) if word not in STOP_WORDS] for doc in text]\n",
    "    return [bigram_mod[doc] for doc in processed_text]\n",
    "\n",
    "def make_trigram(text):\n",
    "    processed_text = [[word for word in simple_preprocess(str(doc)) if word not in STOP_WORDS] for doc in text]\n",
    "    return trigram_mod[[bigram_mod[doc] for doc in processed_text]]\n",
    "\n",
    "def explain_make_trigram(text):\n",
    "    conversion = {}\n",
    "    for doc in text:\n",
    "        pro_doc = simple_preprocess(str(doc))\n",
    "        if doc!= pro_doc:\n",
    "            print(doc)\n",
    "            print(pro_doc)\n",
    "            conversion[doc]=pro_doc\n",
    "    return conversion\n",
    "\n",
    "def n_gram_lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    text_out = []\n",
    "    for sent in text:\n",
    "        doc = nlp(' '.join(sent))\n",
    "        text_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return text_out\n",
    "\n",
    "def explain_n_gram_lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    conversions = {}\n",
    "    for sent in text:\n",
    "        doc = nlp(' '.join(sent))\n",
    "        for token in doc:\n",
    "            if len(token.lemma_) <4 and str(token)!=str(token.lemma_):\n",
    "                conversions[token] = token.lemma_\n",
    "#                 print('%s : %s' (token, token.lemma_))\n",
    "    return conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45352\n",
      "45352\n",
      "CPU times: user 12min 45s, sys: 23min 30s, total: 36min 15s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_bigram = n_gram_lemmatization(make_bigram(processed_docs.title))\n",
    "processed_trigram = n_gram_lemmatization(make_trigram(processed_docs.title))\n",
    "print(str(len(processed_bigram)))\n",
    "print(str(len(processed_trigram)))\n",
    "processed_docs['bigram'] = processed_bigram\n",
    "processed_docs['trigram'] = processed_trigram\n",
    "processed_docs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking bigram\n",
    "# count = 0\n",
    "# for ind, bigram in enumerate(processed_bigram):\n",
    "#     for bi in bigram:\n",
    "#         if bi.find('_')>=0:\n",
    "#             print(bi)\n",
    "#     if any(bi.find('_')>=0 for bi in bigram) and count<10:\n",
    "#         print(bigram)\n",
    "#         count +=1\n",
    "\n",
    "for ind, trigram in enumerate(processed_trigram):\n",
    "    for bi in trigram:\n",
    "        if bi.find('_')>=0:\n",
    "            print(bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words on the data set\n",
    "- create a dictionary from 'processed_docs' containing the number of times a word appears in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the dictionary 11628\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "dictionary_made_by = processed_trigram\n",
    "dictionary_made_by_str = 'trigram'\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(dictionary_made_by)\n",
    "print('size of the dictionary %d' %len(dictionary))\n",
    "max_dic_size = len(dictionary)\n",
    "test_dic_sizes = [max_dic_size, int(max_dic_size*2/3), int(max_dic_size/2), 10000]\n",
    "\n",
    "# count = 0\n",
    "# for k, v in dictionary.iteritems():\n",
    "#     if v.find('_')>=0:\n",
    "#         print(k,v)\n",
    "#     count += 1\n",
    "#     if count > 200:\n",
    "#         break\n",
    "#     if v.find('tion')>=0:\n",
    "#         print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out tokens that appear in less than 0.05 documents \n",
    "# or more than 0.5 documents\n",
    "# after above, keep only the first 40000 most frequent tokens.\n",
    "\n",
    "#############\n",
    "dict_size = test_dic_sizes[2]\n",
    "dictionary.filter_extremes(no_below=0.05, no_above=0.5, keep_n=dict_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 218 (\"clinic\") appears 1 times.\n",
      "Word 228 (\"arteri\") appears 1 times.\n",
      "Word 229 (\"coronari\") appears 1 times.\n",
      "Word 260 (\"calcif\") appears 1 times.\n",
      "Word 261 (\"correl\") appears 1 times.\n",
      "Word 262 (\"score\") appears 1 times.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>bigram</th>\n",
       "      <th>trigram</th>\n",
       "      <th>bow_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4105</td>\n",
       "      <td>[baselin, characterist]</td>\n",
       "      <td>[baselin, characterist]</td>\n",
       "      <td>[baselin, characterist]</td>\n",
       "      <td>[(0, 1), (1, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4106</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "      <td>[(2, 1), (3, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4107</td>\n",
       "      <td>[comparison, postop, carbohydr, antigen, level...</td>\n",
       "      <td>[comparison, postop, carbohydr, antigen, level...</td>\n",
       "      <td>[comparison, postop, carbohydr, antigen, level...</td>\n",
       "      <td>[(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4108</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "      <td>[pattern, disea, relap]</td>\n",
       "      <td>[pattern, disea, relap]</td>\n",
       "      <td>[(11, 1), (12, 1), (13, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4109</td>\n",
       "      <td>[grade, advers, event, gemcitabin, gemcitabin,...</td>\n",
       "      <td>[grade, adver, event, gemcitabin, gemcitabin, ...</td>\n",
       "      <td>[grade, adver, event, gemcitabin, gemcitabin, ...</td>\n",
       "      <td>[(14, 1), (15, 1), (16, 1), (17, 2), (18, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4111</td>\n",
       "      <td>[baselin, characterist]</td>\n",
       "      <td>[baselin, characterist]</td>\n",
       "      <td>[baselin, characterist]</td>\n",
       "      <td>[(0, 1), (1, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4112</td>\n",
       "      <td>[treatment, zoledron, acid]</td>\n",
       "      <td>[treatment, zoledron, acid]</td>\n",
       "      <td>[treatment, zoledron, acid]</td>\n",
       "      <td>[(19, 1), (20, 1), (21, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4113</td>\n",
       "      <td>[treatment, docetaxel]</td>\n",
       "      <td>[treatment, docetaxel]</td>\n",
       "      <td>[treatment, docetaxel]</td>\n",
       "      <td>[(20, 1), (22, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4114</td>\n",
       "      <td>[treatment, relaps, discret, treat, clinician]</td>\n",
       "      <td>[treatment, relap, discret, treat, clinician]</td>\n",
       "      <td>[treatment, relap, discret, treat, clinician]</td>\n",
       "      <td>[(13, 1), (20, 1), (23, 1), (24, 1), (25, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4115</td>\n",
       "      <td>[worst, advers, event, grade, report, entir, t...</td>\n",
       "      <td>[bad, adver, event, grade, report, entir, time...</td>\n",
       "      <td>[bad, adver, event, grade, report, entir, time...</td>\n",
       "      <td>[(10, 1), (14, 1), (16, 1), (18, 1), (26, 1), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              title  \\\n",
       "0  4105                            [baselin, characterist]   \n",
       "1  4106                                [analysi, efficaci]   \n",
       "2  4107  [comparison, postop, carbohydr, antigen, level...   \n",
       "3  4108                          [pattern, diseas, relaps]   \n",
       "4  4109  [grade, advers, event, gemcitabin, gemcitabin,...   \n",
       "5  4111                            [baselin, characterist]   \n",
       "6  4112                        [treatment, zoledron, acid]   \n",
       "7  4113                             [treatment, docetaxel]   \n",
       "8  4114     [treatment, relaps, discret, treat, clinician]   \n",
       "9  4115  [worst, advers, event, grade, report, entir, t...   \n",
       "\n",
       "                                              bigram  \\\n",
       "0                            [baselin, characterist]   \n",
       "1                                [analysi, efficaci]   \n",
       "2  [comparison, postop, carbohydr, antigen, level...   \n",
       "3                            [pattern, disea, relap]   \n",
       "4  [grade, adver, event, gemcitabin, gemcitabin, ...   \n",
       "5                            [baselin, characterist]   \n",
       "6                        [treatment, zoledron, acid]   \n",
       "7                             [treatment, docetaxel]   \n",
       "8      [treatment, relap, discret, treat, clinician]   \n",
       "9  [bad, adver, event, grade, report, entir, time...   \n",
       "\n",
       "                                             trigram  \\\n",
       "0                            [baselin, characterist]   \n",
       "1                                [analysi, efficaci]   \n",
       "2  [comparison, postop, carbohydr, antigen, level...   \n",
       "3                            [pattern, disea, relap]   \n",
       "4  [grade, adver, event, gemcitabin, gemcitabin, ...   \n",
       "5                            [baselin, characterist]   \n",
       "6                        [treatment, zoledron, acid]   \n",
       "7                             [treatment, docetaxel]   \n",
       "8      [treatment, relap, discret, treat, clinician]   \n",
       "9  [bad, adver, event, grade, report, entir, time...   \n",
       "\n",
       "                                          bow_corpus  \n",
       "0                                   [(0, 1), (1, 1)]  \n",
       "1                                   [(2, 1), (3, 1)]  \n",
       "2  [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1...  \n",
       "3                        [(11, 1), (12, 1), (13, 1)]  \n",
       "4      [(14, 1), (15, 1), (16, 1), (17, 2), (18, 1)]  \n",
       "5                                   [(0, 1), (1, 1)]  \n",
       "6                        [(19, 1), (20, 1), (21, 1)]  \n",
       "7                                 [(20, 1), (22, 1)]  \n",
       "8      [(13, 1), (20, 1), (23, 1), (24, 1), (25, 1)]  \n",
       "9  [(10, 1), (14, 1), (16, 1), (18, 1), (26, 1), ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize\n",
    "# Bag-of-words representation of the documents\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in dictionary_made_by]\n",
    "\n",
    "bow_doc_100 = bow_corpus[100]\n",
    "for i in range(len(bow_doc_100)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} times.\".format(bow_doc_100[i][0],\n",
    "                                                     dictionary[bow_doc_100[i][0]],\n",
    "                                                     bow_doc_100[i][1]))\n",
    "\n",
    "processed_docs['bow_corpus'] = bow_corpus\n",
    "processed_docs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "- tf-idf model on bow_corpus\n",
    "\n",
    "- tf = count(word, document) / len(document)\n",
    "- idf = log( len(collection) / count(document_containing_term, collection)\n",
    "- tf-idf = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 218 (\"clinic\") appears 0.22153788953758755 times.\n",
      "Word 228 (\"arteri\") appears 0.33702236441322053 times.\n",
      "Word 229 (\"coronari\") appears 0.3058129489306719 times.\n",
      "Word 260 (\"calcif\") appears 0.6406461142717561 times.\n",
      "Word 261 (\"correl\") appears 0.4491888596048284 times.\n",
      "Word 262 (\"score\") appears 0.36279092370991606 times.\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "\n",
    "tfidf_mod = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf_mod[bow_corpus]\n",
    "\n",
    "processed_docs['tfidf_corpus'] = tfidf_corpus\n",
    "tfidf_doc_100 = tfidf_corpus[100]\n",
    "for i in range(len(tfidf_doc_100)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} times.\".format(tfidf_doc_100[i][0],\n",
    "                                                     dictionary[tfidf_doc_100[i][0]],\n",
    "                                                     tfidf_doc_100[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5814\n",
      "Number of documents bow corpus: 45352\n",
      "Number of documents tfidf: 45352\n",
      "Number of documents : 45352\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents bow corpus: %d' % len(bow_corpus))\n",
    "print('Number of documents tfidf: %d' % len(tfidf_corpus))\n",
    "print('Number of documents : %d' % len(dictionary_made_by))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal number of topics\n",
    "- build many LDA models with different values of number of topics and pick the one that gives the highest coherence value based on corpus\n",
    "- 토픽 모델링 결과로 나온 주제들에 대해 각각의 주제에서 상위 N개의 단어를 뽑습니다. 모델링이 잘 되었을수록 한 주제 안에는 의미론적으로 유사한 단어가 많이 모여있게 마련입니다. 따라서 상위 단어 간의 유사도를 계산하면 실제로 해당 주제가 의미론적으로 일치하는 단어들끼리 모여있는지 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_coherence_values(model_type, dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    compute c_v coherence for various number of topics\n",
    "    \n",
    "    parameters:\n",
    "    dictionary : gensim dictionary\n",
    "    corpus : gensim corpus\n",
    "    texts : list of input texts\n",
    "    limit : max number of topics\n",
    "    \n",
    "    returns:\n",
    "    model_list : list of LDA topic models\n",
    "    coherence_values : coherence value\n",
    "    \"\"\"\n",
    "    \n",
    "    coherenece_values = []\n",
    "    u_mass_coherenece_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        if model_type == 'mallet':\n",
    "            model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, \\\n",
    "                                                     num_topics=num_topics, \\\n",
    "                                                     id2word=dictionary, workers=8, \\\n",
    "                                                     iterations=60)\n",
    "        elif model_type == 'online':\n",
    "            model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, \\\n",
    "                                                    num_topics=num_topics, update_every=0, \\\n",
    "                                                    passes=20)\n",
    "        else:\n",
    "            model = gensim.models.LdaMulticore(corpus, num_topics=num_topics, \\\n",
    "                                               id2word=dictionary, passes=2, \\\n",
    "                                               workers=8, iterations=60)\n",
    "        \n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        u_mass_coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        \n",
    "        coherenece_values.append(coherence_model.get_coherence())\n",
    "        u_mass_coherenece_values.append(u_mass_coherence_model.get_coherence())\n",
    "\n",
    "    return model_list, coherenece_values, u_mass_coherenece_values\n",
    "\n",
    "def find_optimal_topic_num(model_type, model_list, coherence_values, umass_co_val):   \n",
    "    #graph\n",
    "    print('======== '+model_type + ' ========')\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    x = range(start, limit, step)\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.plot(x, coherence_values)\n",
    "    ax1.set_xlabel(\"Number of Topics\")\n",
    "    ax1.set_ylabel(\"Coherence score\")\n",
    "    #     plt.legend((\"coherence_values\"), loc=\"best\")\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.plot(x, umass_co_val)\n",
    "    ax2.set_xlabel(\"Number of Topics\")\n",
    "    ax2.set_ylabel(\"u mass Coherence score\")\n",
    "    #     plt.legend((\"u mass coherence_values\"), loc=\"best\")\n",
    "\n",
    "    fig.savefig(directory+'/'+model_type+'_coherence.png')\n",
    "\n",
    "    # Print the coherence scores\n",
    "    # one that maximizes the topic coherence\n",
    "    print('Coherence c_v ================')\n",
    "    max_coherence_topic_num = 8 #default\n",
    "    prev_co = 0\n",
    "    first_max_cohe_found = False\n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        cur_coherence = round(cv, 4)\n",
    "        print(\"Num Topics =\", m, \" has Coherence cv Value of\", cur_coherence)\n",
    "#         if (cur_coherence > 0.32) and m > 5:\n",
    "        if m > 5:\n",
    "            if (prev_co <= cur_coherence) and not first_max_cohe_found:\n",
    "                max_coherence_topic_num = m\n",
    "            else:\n",
    "                first_max_cohe_found = True\n",
    "        prev_co = cur_coherence\n",
    "    print('Best number of topic is : %d' %max_coherence_topic_num)\n",
    "\n",
    "    print('Coherence umass ================')\n",
    "    #where score plateaus안정\n",
    "    for m, um in zip(x, umass_co_val):\n",
    "        print(\"Num Topics =\", m, \" has Coherence umass Value of\", round(um, 4))\n",
    "    \n",
    "    return max_coherence_topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config='dic_'+dictionary_made_by_str+'_size_'+str(len(dictionary))\n",
    "\n",
    "import os\n",
    "directory ='./stat_files/'+model_config\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "mallet_path = './mallet-2.0.8/bin/mallet'\n",
    "from gensim.models import CoherenceModel\n",
    "limit = 25; start=2; step=1;\n",
    "\n",
    "model_list_on, coherence_values_on, umass_co_val_on = compute_coherence_values(model_type='online', dictionary=dictionary, \\\n",
    "                                                                      corpus=bow_corpus, \\\n",
    "                                                                      texts=processed_trigram, \\\n",
    "                                                                      start=start, \\\n",
    "                                                                      limit=limit, \\\n",
    "                                                                      step=step)\n",
    "max_coherence_topic_num_on= find_optimal_topic_num('online', model_list_on, coherence_values_on, umass_co_val_on)\n",
    "\n",
    "model_list, coherence_values, umass_co_val = compute_coherence_values(model_type='mallet', dictionary=dictionary, \\\n",
    "                                                                      corpus=bow_corpus, \\\n",
    "                                                                      texts=processed_trigram, \\\n",
    "                                                                      start=start, \\\n",
    "                                                                      limit=limit, \\\n",
    "                                                                      step=step)\n",
    "max_coherence_topic_num = find_optimal_topic_num('mallet', model_list, coherence_values, umass_co_val)\n",
    "\n",
    "model_list_tfidf, coherence_values_tfidf, umass_co_val_tfidf = compute_coherence_values(model_type='tfidf', dictionary=dictionary, \\\n",
    "                                                                                              corpus=tfidf_corpus, \\\n",
    "                                                                                              texts=processed_trigram, \\\n",
    "                                                                                              start=start, \\\n",
    "                                                                                              limit=limit, \\\n",
    "                                                                                              step=step)\n",
    "max_coherence_topic_num_tfidf = find_optimal_topic_num('tfidf', model_list_tfidf, coherence_values_tfidf, umass_co_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using bag of words\n",
    "\n",
    "- train LDA using gensim.models.LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=8, id2word=dictionary, passes=2, workers=2)\n",
    "# for idx, topic in lda_model_bow.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n",
    "#is mallet better?\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "dir_to_check = Path(directory+'/data/')\n",
    "if not dir_to_check.is_dir():\n",
    "    os.makedirs(directory+'/data/')\n",
    "\n",
    "def get_optimal_model(model_type, optimal_topic_num):\n",
    "#     optimal_topic_num = max_coherence_topic_num\n",
    "    if model_type=='mallet':\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path=mallet_path, \\\n",
    "                                                      corpus=bow_corpus, \\\n",
    "                                                      num_topics=optimal_topic_num, \\\n",
    "                                                      id2word=dictionary,\\\n",
    "                                                      iterations=100,\\\n",
    "                                                      prefix=directory+'/data/')\n",
    "    elif model_type=='online':\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus, id2word=dictionary, \\\n",
    "                                                    num_topics=optimal_topic_num, update_every=0, \\\n",
    "                                                    passes=20)\n",
    "    else:\n",
    "        model = gensim.models.LdaMulticore(tfidf_corpus,\\\n",
    "                                             num_topics=max_coherence_topic_num_tfidf, \\\n",
    "                                             id2word=dictionary, \\\n",
    "                                             passes=2, \\\n",
    "                                             workers=8,\\\n",
    "                                            iterations=100)\n",
    "        \n",
    "    #show topics\n",
    "    pprint(model.show_topics(formatted=False))\n",
    "    # lda_mallet.save('./data/mallet_topics_num_'+str(optimal_topic_num)+'_'+model_config+'.state.gz')\n",
    "#     model.load_word_topics()\n",
    "  \n",
    "    fig = plt.figure(figsize=(15,20))\n",
    "    fig.suptitle(model_type+'_topics_num_'+str(optimal_topic_num)+'_'+model_config)\n",
    "\n",
    "    for i in range(optimal_topic_num):\n",
    "        df=pd.DataFrame(model.show_topic(i), columns=['term','prob']).set_index('term')\n",
    "\n",
    "        axi = fig.add_subplot(math.ceil(optimal_topic_num/2),2,i+1)\n",
    "        axi.set_title('topic '+str(i+1))\n",
    "        sns.barplot(x='prob', y=df.index, data=df, palette='Reds_d')\n",
    "        axi.set_xlabel('probability')\n",
    "\n",
    "    # plt.show()\n",
    "    fig.savefig(directory+'/'+model_type+'_topics_num_'+str(optimal_topic_num)+'.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_model = get_optimal_model('mallet', max_coherence_topic_num)\n",
    "on_model = get_optimal_model('online', max_coherence_topic_num_on)\n",
    "tfidf_model = get_optimal_model('tfidf', max_coherence_topic_num_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check topic distribution of a sample data\n",
    "# for ind, score in sorted(lda_mallet[bow_corpus[45]], key=lambda x: -1*x[1]):\n",
    "#     print('\\nScore: {}\\t \\nTopic: {}'.format(score, lda_mallet.print_topic(ind, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "dataDir = directory+'/data/'\n",
    "\n",
    "def extract_params(statefile):\n",
    "    \"\"\"extract alpha and beta values from MALLET statefile by path to statfile\n",
    "    \n",
    "    Args:\n",
    "        statefile (str) : Path to statefile produced by MALLET\n",
    "    Returns:\n",
    "        tuple: alpha (list), beta\n",
    "    \"\"\"\n",
    "    \n",
    "#     with codecs.open(statefile, \"r\",encoding='utf-8') as state:\n",
    "    with gzip.open(statefile, 'r') as state:\n",
    "        params = [x.decode('utf-8').strip() for x in state.readlines()[1:3]]\n",
    "#         params = [x.strip() for x in state.readlines()[1:3]]\n",
    "    return (list(params[0].split(':')[1].split(\" \")), float(params[1].split(':')[1]))\n",
    "\n",
    "def state_to_df(statefile):\n",
    "    \"\"\"transform state file into pandas dataframe\n",
    "    the MALLET statefile is tab-separated, and the first two rows contain the alpha and beta parameters\n",
    "    \n",
    "    Args:\n",
    "        statefile (str): Path to statefile produced by MALLET\n",
    "    Returns:\n",
    "        dataframe: topic assignment for each token in each documnet of the model\n",
    "    \"\"\"\n",
    "    return pd.read_csv(statefile,\\\n",
    "                      compression='gzip',\\\n",
    "                      sep=' ',\\\n",
    "                      skiprows=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = extract_params(os.path.join(dataDir, 'state.mallet.gz'))\n",
    "alpha = [float(x) for x in params[0][1:]]\n",
    "beta = params[1]\n",
    "print(\"{}, {}\".format(alpha, beta))\n",
    "\n",
    "df = state_to_df(os.path.join(dataDir, 'state.mallet.gz'))\n",
    "df['type'] = df.type.astype(str)\n",
    "df[:5]\n",
    "#doc id, word position index, word index, topic assignmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the length of the document, group by the document id and count the tokens\n",
    "docs = df.groupby('#doc')['type'].count().reset_index(name='doc_length')\n",
    "docs[:10]\n",
    "\n",
    "#get vocab and term frequencies\n",
    "vocab = df['type'].value_counts().reset_index()\n",
    "vocab.columns = ['type', 'term_freq']\n",
    "vocab = vocab.sort_values(by='type', ascending=True)\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix file\n",
    "#need to normalize data so that each row sums to 1\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def pivot_and_smooth(df, smooth_values, rows_variable, cols_variable, values_variable):\n",
    "    \"\"\"\n",
    "    modify dataframe into matrix\n",
    "    Args:\n",
    "        df (dataframe) : \n",
    "        smooth_values (float) : value to add to the matrix to account for the priors\n",
    "        rows_variable (str) : title of rows\n",
    "        cols_variable (str) : title of columns\n",
    "        values_variable (str) : values\n",
    "    Returns:\n",
    "        dataframe : that has been normalized on the rows.\n",
    "    \"\"\"\n",
    "    matrix = df.pivot(index=rows_variable, columns=cols_variable, values=values_variable).fillna(value=0)\n",
    "    matrix = matrix.values + smooth_values\n",
    "    \n",
    "    normed = sklearn.preprocessing.normalize(matrix, norm='l1', axis=1)\n",
    "    \n",
    "    return pd.DataFrame(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of topic assingments for words in documents\n",
    "#phi - topic-term matrix and counted the number of times each word was assigned to each topic \n",
    "#and sorted by alphabetically to match the order of the vocabulary frame\n",
    "\n",
    "#beta as the smoothign value\n",
    "phi_df = df.groupby(['topic', 'type'])['type'].count().reset_index(name='token_count')\n",
    "phi_df = phi_df.sort_values(by='type', ascending=True)\n",
    "phi_df[:5]\n",
    "\n",
    "phi = pivot_and_smooth(phi_df, beta, 'topic', 'type', 'token_count')\n",
    "phi[:5]\n",
    "\n",
    "#theta document-topic matrix and use alpha as the smoothign value\n",
    "theta_df = df.groupby(['#doc', 'topic'])['topic'].count().reset_index(name='topic_count')\n",
    "theta_df[:5]\n",
    "\n",
    "theta = pivot_and_smooth(theta_df, alpha, '#doc', 'topic', 'topic_count')\n",
    "theta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mallet_data = {\n",
    "    'topic_term_dists':phi,\n",
    "    'doc_topic_dists':theta,\n",
    "    'doc_lengths':list(docs['doc_length']),\n",
    "    'vocab':list(vocab['type']),\n",
    "    'term_frequency':list(vocab['term_freq'])\n",
    "}\n",
    "mallet_vis_data = pyLDAvis.prepare(**lda_mallet_data)\n",
    "pyLDAvis.display(mallet_vis_data)\n",
    "pyLDAvis.save_html(mallet_vis_data, directory+'/mallet_topics_num_'+str(max_coherence_topic_num)+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each bubble = topic, the larger the bubble, the more prevalent is that topic\n",
    "#good topic = fairly big, non-overlapping bubbles scattered throughout the chart\n",
    "#model with too many topics = typically have many overlaps, small sized bubbles in one region of the chart\n",
    "data_tfidf = pyLDAvis.gensim.prepare(tfidf_model, tfidf_corpus, dictionary)\n",
    "data_tfidf\n",
    "\n",
    "#bar = salient keywords that form the selected topic\n",
    "pyLDAvis.display(data_tfidf)\n",
    "pyLDAvis.save_html(data_tfidf, directory+'/tfidf_topics_num_'+str(max_coherence_topic_num_tfidf)+'.html')\n",
    "\n",
    "data_on = pyLDAvis.gensim.prepare(on_model, bow_corpus, dictionary)\n",
    "data_on\n",
    "\n",
    "pyLDAvis.display(data_on)\n",
    "pyLDAvis.save_html(data_on, directory+'/online_topics_num_'+str(max_coherence_topic_num_on)+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity and Coherence score\n",
    "- model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute perplexity : a measure of how good the model is\n",
    "# lower the better\n",
    "# print('\\nPerplexit: ', lda_mallet.log_perplexity(bow_corpus))\n",
    "# print('\\nPerplexit: ', lda_model_tfidf.log_perplexity(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#compute coherence score\n",
    "coherence_model_lda = CoherenceModel(model=mallet_model, texts=processed_trigram, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#compute coherence score\n",
    "coherence_model_lda_tfidf = CoherenceModel(model=tfidf_model, texts=processed_trigram, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda_tfidf = coherence_model_lda_tfidf.get_coherence()\n",
    "print('\\nTF-IDF Coherence Score: ', coherence_lda_tfidf)\n",
    "\n",
    "#compute coherence score\n",
    "coherence_model_lda_on = CoherenceModel(model=on_model, texts=processed_trigram, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda_on = coherence_model_lda_on.get_coherence()\n",
    "print('\\nONLINE Coherence Score: ', coherence_lda_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dominant topic\n",
    "- find the most contributed topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = processed_docs.merge(documents, on='id')\n",
    "# test = processed_docs[['id', 'title_x', 'bow_corpus', 'title_y']]\n",
    "# test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.iloc[151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def format_topics_sentences(model, \\\n",
    "                            model_type='mallet', \\\n",
    "                            corpus=processed_docs, \\\n",
    "                            texts=dictionary_made_by):\n",
    "    sent_topic_df = pd.DataFrame()\n",
    "    if model_type=='tfidf':\n",
    "        target_corpus = corpus.tfidf_corpus\n",
    "    else:\n",
    "        target_corpus = corpus.bow_corpus\n",
    "    \n",
    "    for i, row in enumerate(model[target_corpus]):\n",
    "        origin_info = processed_docs.iloc[i]\n",
    "        text_vec = texts[i]\n",
    "        #get main topic in each document\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j==0: #dominant topic\n",
    "                wp = model.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "#                 print(pd.Series([origin_info.id,\\\n",
    "#                                 int(topic_num), \\\n",
    "#                                 round(prop_topic, 4), \\\n",
    "#                                 topic_keywords, \\\n",
    "#                                origin_info.title_y, \\\n",
    "#                                text_vec]))\n",
    "                sent_topic_df = sent_topic_df.append(pd.Series([origin_info.id,\\\n",
    "                                                                int(topic_num), \\\n",
    "                                                                round(prop_topic, 4), \\\n",
    "                                                                topic_keywords, \\\n",
    "                                                               origin_info.title_y, \\\n",
    "                                                               text_vec]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topic_df.columns = ['id', 'Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Origin_Text', 'Text_Vec']\n",
    "    \n",
    "    return (sent_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_mallet = format_topics_sentences(model=mallet_model, \\\n",
    "                                                  corpus=processed_docs, \\\n",
    "                                                  texts=dictionary_made_by, \\\n",
    "                                                 model_type='mallet')\n",
    "df_topic_sents_keywords_mallet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_tfidf = format_topics_sentences(model=tfidf_model, \\\n",
    "                                                  corpus=processed_docs, \\\n",
    "                                                  texts=dictionary_made_by, \\\n",
    "                                                 model_type='tfidf')\n",
    "df_topic_sents_keywords_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_on = format_topics_sentences(model=on_model, \\\n",
    "                                                  corpus=processed_docs, \\\n",
    "                                                  texts=dictionary_made_by, \\\n",
    "                                                 model_type='online')\n",
    "df_topic_sents_keywords_on.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_topic_sents_keywords_mallet.isna().any())\n",
    "print('=================')\n",
    "print(df_topic_sents_keywords_tfidf.isna().any())\n",
    "print('=================')\n",
    "print(df_topic_sents_keywords_on.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 토픽별로 가장 대표적인 문서 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_sents(keywords, model_type):\n",
    "    sent_topics_sorted_df_mallet = pd.DataFrame()\n",
    "    \n",
    "    sent_topics_groupby = keywords.groupby('Dominant_Topic')\n",
    "\n",
    "    for i, grp in sent_topics_groupby:\n",
    "        keywords = pd.concat([keywords, \\\n",
    "                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(10)], \\\n",
    "                            axis=0)\n",
    "\n",
    "    keywords.reset_index(drop=True, inplace=True)\n",
    "    keywords.columns = ['id', 'Topic_Num', 'Topic_Perc_Contribu', 'Topic_Keywords', 'Origin_Text', 'Text']\n",
    "    keywords.to_csv(directory+'/'+model_type+'_top_sen.tsv', sep='\\t')\n",
    "    return keywords\n",
    "    \n",
    "df_topic_sents_mallet = get_rep_sents(df_topic_sents_keywords_mallet, 'mallet')\n",
    "df_topic_sents_tfidf = get_rep_sents(df_topic_sents_keywords_tfidf, 'tfidf')\n",
    "df_topic_sents_on = get_rep_sents(df_topic_sents_keywords_on, 'online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_mallet.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서 전체적인 토픽 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "topic_counts = df_topic_sents_mallet['Topic_Num'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "topic_num_keywords = df_topic_sents_mallet[['Topic_Num', 'Topic_Keywords']]\n",
    "\n",
    "df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "df_dominant_topics.columns = ['Topic_Num', 'Num_Documents', 'Perc_Documents']\n",
    "df_dominant_topics = df_dominant_topics.merge(topic_num_keywords.drop_duplicates(), on='Topic_Num')\n",
    "df_dominant_topics = df_dominant_topics.sort_values('Topic_Num')\n",
    "\n",
    "df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "df_dominant_topics.to_html(directory+'/mallet_distribution.html', index=False)\n",
    "df_dominant_topics.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df_topic_sents_keywords_tfidf['Dominant_Topic'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "topic_num_keywords = df_topic_sents_keywords_tfidf[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Num_Documents', 'Perc_Documents']\n",
    "df_dominant_topics = df_dominant_topics.merge(topic_num_keywords.drop_duplicates(), on='Dominant_Topic')\n",
    "df_dominant_topics = df_dominant_topics.sort_values('Dominant_Topic')\n",
    "\n",
    "df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "df_dominant_topics.to_html(directory+'/tfidf_distribution.html', index=False)\n",
    "df_dominant_topics.head(max_coherence_topic_num_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df_topic_sents_keywords_on['Dominant_Topic'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "topic_num_keywords = df_topic_sents_keywords_on[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Num_Documents', 'Perc_Documents']\n",
    "df_dominant_topics = df_dominant_topics.merge(topic_num_keywords.drop_duplicates(), on='Dominant_Topic')\n",
    "df_dominant_topics = df_dominant_topics.sort_values('Dominant_Topic')\n",
    "\n",
    "df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "df_dominant_topics.to_html(directory+'/on_distribution.html', index=False)\n",
    "df_dominant_topics.head(max_coherence_topic_num_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://ai.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html\n",
    "- http://projector.tensorflow.org/\n",
    "- http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb#loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate testing dataset by cosine similarity \n",
    "- between corresponding parts (higher the better)\n",
    "- between 10,000 random parts (lower the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
