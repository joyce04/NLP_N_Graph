{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.myyellowroad.com/unsupervised-sentence-representation-with-deep-learning-104b90079a93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if text is missing\n",
      "id       False\n",
      "title    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46641, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve table titles from csv\n",
    "data = pd.read_csv('./topic_modeling/titles.csv', delimiter='\\t', error_bad_lines=True, header=None)\n",
    "\n",
    "data.columns = ['id', 'title']\n",
    "data.title = data.title.str.strip()\n",
    "documents = data\n",
    "documents['title'].replace('', np.nan, inplace=True)\n",
    "documents = documents.astype(str)\n",
    "print('checking if text is missing')\n",
    "print(documents.isna().any())\n",
    "\n",
    "documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/grace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "STOP_WORDS = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "STOP_WORDS.extend(['table', 'legend'])\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "#     deacc=True removes punctuations\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True):\n",
    "        if token not in STOP_WORDS and len(token)>1:\n",
    "#             result.append(lemmatize_stemming(strip_numeric(token)))\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 56 ms, total: 12.1 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#process the text, save the results as processed_docs\n",
    "processed_docs = pd.DataFrame()\n",
    "processed_docs = pd.concat([documents.id, documents.title.map(preprocess)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4105</td>\n",
       "      <td>[baselin, characterist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4106</td>\n",
       "      <td>[analysi, efficaci]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4107</td>\n",
       "      <td>[comparison, postop, carbohydr, antigen, level, surviv, espac, conoko, jaspac, trial]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4108</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4109</td>\n",
       "      <td>[grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  4105   \n",
       "1  4106   \n",
       "2  4107   \n",
       "3  4108   \n",
       "4  4109   \n",
       "\n",
       "                                                                                   title  \n",
       "0  [baselin, characterist]                                                                \n",
       "1  [analysi, efficaci]                                                                    \n",
       "2  [comparison, postop, carbohydr, antigen, level, surviv, espac, conoko, jaspac, trial]  \n",
       "3  [pattern, diseas, relaps]                                                              \n",
       "4  [grade, advers, event, gemcitabin, gemcitabin, plus, capecitabin]                      "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the dictionary 12915\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs.title)\n",
    "print('size of the dictionary %d' %len(dictionary))\n",
    "max_dic_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building X and Y\n",
    "num_words=2000\n",
    "maxlen=20\n",
    "tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "tokenizer.fit_on_texts(df['body'].values)\n",
    "seqs = tokenizer.texts_to_sequences(df['body'].values)\n",
    "seq = []\n",
    "for i in seqs:\n",
    "    seq+=i\n",
    "    \n",
    "X = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(seq)-maxlen-1)):\n",
    "    X.append(seq[i:i+maxlen])\n",
    "    Y.append(seq[i+maxlen+1])\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "Y[0]=Y[0].astype('category')\n",
    "Y =pd.get_dummies(Y)\n",
    "#Buidling the network\n",
    "embed_dim = 150\n",
    "lstm_out = 128\n",
    "batch_size= 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim,input_length = maxlen))\n",
    "model.add(Bidirectional(LSTM(lstm_out)))\n",
    "model.add(Dense(Y.shape[1],activation='softmax'))\n",
    "adam = Adam(lr=0.001, beta_1=0.7, beta_2=0.99, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=adam)\n",
    "model.summary()\n",
    "print('fit')\n",
    "model.fit(X, Y, batch_size =batch_size,validation_split=0.1, epochs = 5,  verbose = 1)\n",
    "\n",
    "#Feature extraction\n",
    "headlines = tokenizer.texts_to_sequences(data['headline'].values)\n",
    "headlines = pad_sequences(headlines,maxlen=maxlen)\n",
    "inp = model.input                                          \n",
    "outputs = [model.layers[1].output]      \n",
    "functor = K.function([inp]+ [K.learning_phase()], outputs )\n",
    "x = functor([headlines, 1.])[0]\n",
    "\n",
    "#classifier\n",
    "X_train,y_train,X_test,y_test = x[msk],y[msk],x[~msk],y[~msk]\n",
    "lr = LogisticRegression().fit(X_train,y_train)\n",
    "lr.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
