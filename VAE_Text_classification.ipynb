{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://alexadam.ca/ml/2017/05/05/keras-vae.html\n",
    "\n",
    "similarity in expressiveness between human languages  \n",
    "latent space = meaning of document encoded as numerical vectors  \n",
    "### variational autoencoder\n",
    "- generative model : place an additional constraint on the loss function such that the latent space is spread out and doesn't countain dead zones where reconstruting an input from those locations results in garbage.\n",
    "- randomly sample a vector from the latent space in attempt to create a meaningful decoded ouput\n",
    "- variational comes from approximating the posterior distribution with a variational distribution (multivariate Gaussian distribution)\n",
    "- latent representation is obtained by sampling this distribution\n",
    "- decoder then takes the latent representation and tries to reconstruct the original input from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model\n",
    "- bi-directional RNN encoder\n",
    "- linear single-layer fully-connected classification network\n",
    "- RNN decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import objectives, backend as K\n",
    "from keras.layers import Bidirectional, Dense, Embedding\\\n",
    ",Input, Lambda, LSTM, RepeatVector, TimeDistributed\n",
    "from keras.models import Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(object):\n",
    "    def create(self, vocab_size=500, max_length=300, latent_rep_size=200):\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.sentiment_predictor = None\n",
    "        self.autoencoder = None\n",
    "        \n",
    "        #convert to learned word embeddings\n",
    "        #language models tend to have semantically similar words close together in embedding space in PCA\n",
    "        x = Input(shape=(max_length,))\n",
    "        x_embed = Embedding(vocab_size, 64, input_length=max_length)(x)\n",
    "        \n",
    "        #build encoder, vae loss function\n",
    "        vae_loss, encoded = self._build_encoder(x_embed, latent_rep_size=latent_rep_size, max_length=max_length)\n",
    "        self.encoder = Model(inputs=x, outputs=encoded)\n",
    "        \n",
    "        encoded_input = Input(shape=(latent_rep_size,))\n",
    "        #prediction model based on the encoded latent space representation\n",
    "        predicted_sentiment = self._build_sentiment_predictor(encoded_input)\n",
    "        self.sentiment_predictor = Model(encoded_input, predicted_sentiment)\n",
    "        \n",
    "        #decoder based on latent space representation\n",
    "        decoded = self._build_decoder(encoded_input, vocab_size, max_length)\n",
    "        self.decoder = Model(encoded_input, decoded)\n",
    "        \n",
    "        #two outputs for reconstructed input and predicted sentiment\n",
    "        self.autoencoder = Model(inputs=x, outputs=[self._build_decoder(encoded, vocab_size, max_length),\\\n",
    "                                                   self._build_sentiment_predictor(encoded)])\n",
    "        self.autoencoder.compile(optimizer='Adam',\n",
    "                                loss=[vae_loss, 'binary_crossentropy'],\n",
    "                                metrics=['accuracy'])\n",
    "    \n",
    "    #encoder = stacked bi-directional RNN\n",
    "    #sampling function based on multivariate Gaussian distribution\n",
    "    def _build_encoder(self, x, latent_rep_size=200, max_length=300, epsilon_std=0.01):\n",
    "        h = Bidirectional(LSTM(500, return_sequences=True, name='lstm_1'), merge_mode='concat')(x)\n",
    "        h = Bidirectional(LSTM(500, return_sequences=False, name='lstm_2'), merge_mode='concat')(h)\n",
    "        h = Dense(435, activation='relu', name='dense_1')(h)\n",
    "        \n",
    "        def sampling(args):\n",
    "            z_mean_, z_log_var_ = args\n",
    "            batch_size = K.shape(z_mean_)[0]\n",
    "            epsilon = K.random_normal(shape=(batch_size, latent_rep_size),\\\n",
    "                                     mean=0., stddev=epsilon_std)\n",
    "            return z_mean_ + K.exp(z_log_var_/2)*epsilon\n",
    "        \n",
    "        z_mean = Dense(latent_rep_size, name='z_mean', activation='linear')(h)\n",
    "        z_log_var = Dense(latent_rep_size, name='z_log_var', activation='linear')(h)\n",
    "        \n",
    "        def vae_loss(x, x_decoded_mean):\n",
    "            x = K.flatten(x)\n",
    "            x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "            xent_loss = max_length*objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "            kl_loss = -0.5 * K.mean(1+z_log_var-K.square(z_mean)-K.exp(z_log_var), axis=-1)\n",
    "            return xent_loss + kl_loss\n",
    "        \n",
    "        return(vae_loss, Lambda(sampling, output_shape=(latent_rep_size,), name='lambda')\\\n",
    "               ([z_mean, z_log_var]))\n",
    "    \n",
    "    def _build_decoder(self, encoded, vocab_size, max_length):\n",
    "        repeated_context = RepeatVector(max_length)(encoded)\n",
    "        \n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_1')(repeated_context)\n",
    "        h = LSTM(500, return_sequences=True, name='dec_lstm_2')(h)\n",
    "        \n",
    "        decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)\n",
    "        return decoded\n",
    "    \n",
    "    def _build_sentiment_predictor(self, encoded):\n",
    "        h = Dense(100, activation='linear')(encoded)\n",
    "        \n",
    "        return Dense(1, activation='sigmoid', name='pred')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 300\n",
    "NUM_WORDS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "(25000,)\n",
      "(25000,)\n",
      "Number of words:\n",
      "998\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "print('training data')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Number of words:')\n",
    "print(len(np.unique(np.hstack(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=MAX_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LENGTH)\n",
    "\n",
    "train_indices = np.random.choice(np.arange(X_train.shape[0]), 2000, replace=False)\n",
    "test_indices = np.random.choice(np.arange(X_test.shape[0]), 1000, replace=False)\n",
    "\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "X_test = X_test[test_indices]\n",
    "y_test = y_test[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros((X_train.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_train.shape[0]), axis=0).reshape(X_train.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_train.shape[0], axis=0), X_train] = 1\n",
    "\n",
    "X_train_one_hot = temp\n",
    "\n",
    "temp = np.zeros((X_test.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_test.shape[0]), axis=0).reshape(X_test.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_test.shape[0], axis=0), X_test] = 1\n",
    "\n",
    "x_test_one_hot = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + \\\n",
    "               model_name + \"-{epoch:02d}-{val_decoded_mean_acc:.2f}-{val_pred_loss:.2f}.h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath,\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False)\n",
    "\n",
    "    return checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = VAE()\n",
    "    model.create(vocab_size=NUM_WORDS, max_length=MAX_LENGTH)\n",
    "\n",
    "    checkpointer = create_model_checkpoint('models', 'rnn_ae')\n",
    "\n",
    "    model.autoencoder.fit(x=X_train, y={'decoded_mean': X_train_one_hot, 'pred': y_train},\n",
    "                          batch_size=10, epochs=10, callbacks=[checkpointer],\n",
    "                          validation_data=(X_test, {'decoded_mean': x_test_one_hot, 'pred':  y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/grace/workspace/keras/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/grace/workspace/keras/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1990/2000 [============================>.] - ETA: 4s - loss: 2.1214 - decoded_mean_loss: 1.4191 - pred_loss: 0.7023 - decoded_mean_acc: 0.3644 - pred_acc: 0.5221Epoch 00000: saving model to models/rnn_ae-00-0.36-0.68.h5\n",
      "2000/2000 [==============================] - 1153s - loss: 2.1208 - decoded_mean_loss: 1.4185 - pred_loss: 0.7022 - decoded_mean_acc: 0.3647 - pred_acc: 0.5220 - val_loss: 2.0995 - val_decoded_mean_loss: 1.4156 - val_pred_loss: 0.6839 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5440\n",
      "Epoch 2/10\n",
      "1990/2000 [============================>.] - ETA: 4s - loss: 2.0811 - decoded_mean_loss: 1.4224 - pred_loss: 0.6587 - decoded_mean_acc: 0.3634 - pred_acc: 0.6558Epoch 00001: saving model to models/rnn_ae-01-0.36-0.68.h5\n",
      "2000/2000 [==============================] - 1133s - loss: 2.0811 - decoded_mean_loss: 1.4224 - pred_loss: 0.6587 - decoded_mean_acc: 0.3633 - pred_acc: 0.6560 - val_loss: 2.0871 - val_decoded_mean_loss: 1.4090 - val_pred_loss: 0.6781 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.6120\n",
      "Epoch 3/10\n",
      "1990/2000 [============================>.] - ETA: 4s - loss: 1.9851 - decoded_mean_loss: 1.3975 - pred_loss: 0.5876 - decoded_mean_acc: 0.3657 - pred_acc: 0.6935Epoch 00002: saving model to models/rnn_ae-02-0.36-0.69.h5\n",
      "2000/2000 [==============================] - 1130s - loss: 1.9841 - decoded_mean_loss: 1.3960 - pred_loss: 0.5881 - decoded_mean_acc: 0.3667 - pred_acc: 0.6930 - val_loss: 2.0884 - val_decoded_mean_loss: 1.4027 - val_pred_loss: 0.6857 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5130\n",
      "Epoch 4/10\n",
      "1990/2000 [============================>.] - ETA: 101s - loss: 2.0434 - decoded_mean_loss: 1.3849 - pred_loss: 0.6585 - decoded_mean_acc: 0.3675 - pred_acc: 0.5819Epoch 00003: saving model to models/rnn_ae-03-0.36-0.69.h5\n",
      "2000/2000 [==============================] - 20272s - loss: 2.0446 - decoded_mean_loss: 1.3861 - pred_loss: 0.6585 - decoded_mean_acc: 0.3667 - pred_acc: 0.5805 - val_loss: 2.0888 - val_decoded_mean_loss: 1.4026 - val_pred_loss: 0.6861 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5530\n",
      "Epoch 5/10\n",
      "1990/2000 [============================>.] - ETA: 4s - loss: 2.0271 - decoded_mean_loss: 1.3847 - pred_loss: 0.6424 - decoded_mean_acc: 0.3663 - pred_acc: 0.6216Epoch 00004: saving model to models/rnn_ae-04-0.36-0.72.h5\n",
      "2000/2000 [==============================] - 1097s - loss: 2.0271 - decoded_mean_loss: 1.3841 - pred_loss: 0.6430 - decoded_mean_acc: 0.3667 - pred_acc: 0.6215 - val_loss: 2.1250 - val_decoded_mean_loss: 1.4091 - val_pred_loss: 0.7159 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5630\n",
      "Epoch 6/10\n",
      "1990/2000 [============================>.] - ETA: 4s - loss: 2.0212 - decoded_mean_loss: 1.3825 - pred_loss: 0.6387 - decoded_mean_acc: 0.3667 - pred_acc: 0.6286Epoch 00005: saving model to models/rnn_ae-05-0.36-0.69.h5\n",
      "2000/2000 [==============================] - 1100s - loss: 2.0212 - decoded_mean_loss: 1.3825 - pred_loss: 0.6388 - decoded_mean_acc: 0.3667 - pred_acc: 0.6285 - val_loss: 2.0855 - val_decoded_mean_loss: 1.4004 - val_pred_loss: 0.6851 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5630\n",
      "Epoch 7/10\n",
      "1990/2000 [============================>.] - ETA: 24s - loss: 2.0221 - decoded_mean_loss: 1.3844 - pred_loss: 0.6377 - decoded_mean_acc: 0.3637 - pred_acc: 0.6296Epoch 00006: saving model to models/rnn_ae-06-0.36-0.71.h5\n",
      "2000/2000 [==============================] - 4955s - loss: 2.0225 - decoded_mean_loss: 1.3837 - pred_loss: 0.6388 - decoded_mean_acc: 0.3643 - pred_acc: 0.6295 - val_loss: 2.1181 - val_decoded_mean_loss: 1.4076 - val_pred_loss: 0.7104 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5620\n",
      "Epoch 8/10\n",
      "1990/2000 [============================>.] - ETA: 4s - loss: 2.0188 - decoded_mean_loss: 1.3836 - pred_loss: 0.6352 - decoded_mean_acc: 0.3662 - pred_acc: 0.6307Epoch 00007: saving model to models/rnn_ae-07-0.36-0.70.h5\n",
      "2000/2000 [==============================] - 1132s - loss: 2.0186 - decoded_mean_loss: 1.3836 - pred_loss: 0.6350 - decoded_mean_acc: 0.3663 - pred_acc: 0.6315 - val_loss: 2.1008 - val_decoded_mean_loss: 1.4016 - val_pred_loss: 0.6992 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5610\n",
      "Epoch 9/10\n",
      "1990/2000 [============================>.] - ETA: 4s - loss: 2.0157 - decoded_mean_loss: 1.3798 - pred_loss: 0.6359 - decoded_mean_acc: 0.3674 - pred_acc: 0.6317Epoch 00008: saving model to models/rnn_ae-08-0.36-0.69.h5\n",
      "2000/2000 [==============================] - 1136s - loss: 2.0167 - decoded_mean_loss: 1.3809 - pred_loss: 0.6358 - decoded_mean_acc: 0.3667 - pred_acc: 0.6315 - val_loss: 2.0836 - val_decoded_mean_loss: 1.3976 - val_pred_loss: 0.6860 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5620\n",
      "Epoch 10/10\n",
      "1990/2000 [============================>.] - ETA: 4s - loss: 2.0169 - decoded_mean_loss: 1.3844 - pred_loss: 0.6325 - decoded_mean_acc: 0.3665 - pred_acc: 0.6322Epoch 00009: saving model to models/rnn_ae-09-0.36-0.68.h5\n",
      "2000/2000 [==============================] - 1143s - loss: 2.0171 - decoded_mean_loss: 1.3846 - pred_loss: 0.6325 - decoded_mean_acc: 0.3663 - pred_acc: 0.6320 - val_loss: 2.0743 - val_decoded_mean_loss: 1.3927 - val_pred_loss: 0.6816 - val_decoded_mean_acc: 0.3576 - val_pred_acc: 0.5630\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
