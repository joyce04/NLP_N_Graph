{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for Finding Table Titles Relevant to Drug-Adverse Events\n",
    "\n",
    "- The following codes is implemented for topic modeling through LDA in attempt to find document samples(table title data) relevant to drug and adverse events.\n",
    "- Topic modeling is one of statistical modeling for discovering the abstract 'topics' that occur in a collection of document.\n",
    " - An example of topic model is used to classify text in a document to a particular topic\n",
    " - it builds a topic per document model and words per topic model, modeled as Dirichlet distributions\n",
    "- Following urls and research articles were referenced.\n",
    " - https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    " - http://www.engear.net/wp/topic-modeling-gensimpython/\n",
    " - https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html\n",
    " - https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#14computemodelperplexityandcoherencescore\n",
    " - https://markroxor.github.io/gensim/static/notebooks/lda_training_tips.html\n",
    " - http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb\n",
    " - http://jeriwieringa.com/2018/07/17/pyLDAviz-and-Mallet/#comment-4018495276\n",
    " - https://ldavis.cpsievert.me/reviews/reviews.html\n",
    " - https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve table titles from csv\n",
    "data = pd.read_csv('titles.csv', delimiter='\\t', error_bad_lines=True, header=None)\n",
    "data.columns = ['id', 'title']\n",
    "data.title = data.title.str.strip()\n",
    "documents = data\n",
    "documents['title'].replace('', np.nan, inplace=True)\n",
    "documents = documents.astype(str)\n",
    "print('checking if text is missing')\n",
    "print(documents.isna().any())\n",
    "# documents.dropna(subset=['title'], inplace=True)\n",
    "documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-processing\n",
    "- tokenization : \n",
    "  - split the text into sentences and then sentences into words\n",
    "  - lower case words\n",
    "  - remove punctuation\n",
    "  - remove words with fewer than 2 characters\n",
    "  - remove all stopwords + added ('table', 'legend')\n",
    "- Lemmatization : \n",
    " - verbs in past and future tenses are changed into present\n",
    "- Stemmazation :\n",
    " - words are reduced to their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "STOP_WORDS = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "STOP_WORDS.extend(['table', 'legend'])\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "#     deacc=True removes punctuations\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True):\n",
    "        if token not in STOP_WORDS and len(token)>1:\n",
    "#             result.append(lemmatize_stemming(strip_numeric(token)))\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#process the text, save the results as processed_docs\n",
    "processed_docs = pd.DataFrame()\n",
    "processed_docs = pd.concat([documents.id, documents.title.map(preprocess)], axis=1)\n",
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trigram model : implement n-grams with Gensim Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING MIN_COUNT\n",
    "import itertools\n",
    "\n",
    "def check_trends_with_min_count(min_count):\n",
    "    bigram = gensim.models.Phrases(processed_docs.title, min_count=min_count, threshold=1)\n",
    "    trigram = gensim.models.Phrases(bigram[processed_docs.title], threshold=1)\n",
    "\n",
    "    #sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    #generated n-grams\n",
    "    n_grams = []\n",
    "    word_2d = processed_docs.map(lambda x: trigram_mod[bigram_mod[x]])\n",
    "    for words in word_2d:\n",
    "        n_grams.extend(list(filter(lambda x : x.find('_')>=0, words)))\n",
    "    return set(n_grams)\n",
    "\n",
    "# n_gram_file = open('n_gram_by_min_count.csv', 'w')\n",
    "# test_min_counts = list(itertools.chain(range(1, 2), range(50, 500, 50)))\n",
    "# n_gram_matrix = []\n",
    "# matrix_index = 0\n",
    "# for i in reversed(test_min_counts):\n",
    "#     print('min_count %d' %i)\n",
    "#     print('matrix_index %d' %matrix_index)\n",
    "#     if matrix_index > 0:\n",
    "#         n_gram_matrix.append(list(set(check_trends_with_min_count(i)) - set(n_gram_matrix[matrix_index-1])))\n",
    "#     else:\n",
    "#         n_gram_matrix.append(list(set(check_trends_with_min_count(i))))\n",
    "#     n_gram_file.write(str(i)+'\\t'+str(n_gram_matrix[matrix_index])+'\\n')\n",
    "#     matrix_index += 1\n",
    "\n",
    "# n_gram_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#min_count = ignore all words and bigrams with total collected count lower than this value\n",
    "#threshold = represent a score threshold for forming the phrases(higher means fewer phrases)\n",
    "bigram = gensim.models.Phrases(processed_docs.title, min_count=1, threshold=0.3)\n",
    "trigram = gensim.models.Phrases(bigram[processed_docs.title], threshold=0.3)\n",
    "\n",
    "#sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigram(text):\n",
    "    processed_text = [[word for word in simple_preprocess(str(doc)) if word not in STOP_WORDS] for doc in text]\n",
    "    return [bigram_mod[doc] for doc in processed_text]\n",
    "\n",
    "def make_trigram(text):\n",
    "    processed_text = [[word for word in simple_preprocess(str(doc)) if word not in STOP_WORDS] for doc in text]\n",
    "    return trigram_mod[[bigram_mod[doc] for doc in processed_text]]\n",
    "\n",
    "def explain_make_trigram(text):\n",
    "    conversion = {}\n",
    "    for doc in text:\n",
    "        pro_doc = simple_preprocess(str(doc))\n",
    "        if doc!= pro_doc:\n",
    "            print(doc)\n",
    "            print(pro_doc)\n",
    "            conversion[doc]=pro_doc\n",
    "    return conversion\n",
    "\n",
    "def n_gram_lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    text_out = []\n",
    "    for sent in text:\n",
    "        doc = nlp(' '.join(sent))\n",
    "        text_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return text_out\n",
    "\n",
    "def explain_n_gram_lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    conversions = {}\n",
    "    for sent in text:\n",
    "        doc = nlp(' '.join(sent))\n",
    "        for token in doc:\n",
    "            if len(token.lemma_) <4 and str(token)!=str(token.lemma_):\n",
    "                conversions[token] = token.lemma_\n",
    "#                 print('%s : %s' (token, token.lemma_))\n",
    "    return conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processed_bigram = n_gram_lemmatization(make_bigram(processed_docs.title))\n",
    "processed_trigram = n_gram_lemmatization(make_trigram(processed_docs.title))\n",
    "print(str(len(processed_bigram)))\n",
    "print(str(len(processed_trigram)))\n",
    "processed_docs['bigram'] = processed_bigram\n",
    "processed_docs['trigram'] = processed_trigram\n",
    "processed_docs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking bigram\n",
    "# count = 0\n",
    "# for ind, bigram in enumerate(processed_bigram):\n",
    "#     for bi in bigram:\n",
    "#         if bi.find('_')>=0:\n",
    "#             print(bi)\n",
    "#     if any(bi.find('_')>=0 for bi in bigram) and count<10:\n",
    "#         print(bigram)\n",
    "#         count +=1\n",
    "\n",
    "# for ind, trigram in enumerate(processed_trigram):\n",
    "#     for bi in trigram:\n",
    "#         if bi.find('_')>=0:\n",
    "#             print(bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words on the data set\n",
    "- create a dictionary from 'processed_docs' containing the number of times a word appears in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "dictionary_made_by = processed_trigram\n",
    "dictionary_made_by_str = 'trigram'\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(dictionary_made_by)\n",
    "print('size of the dictionary %d' %len(dictionary))\n",
    "max_dic_size = len(dictionary)\n",
    "test_dic_sizes = [max_dic_size, int(max_dic_size*2/3), int(max_dic_size/2), 10000]\n",
    "\n",
    "# count = 0\n",
    "# for k, v in dictionary.iteritems():\n",
    "#     if v.find('_')>=0:\n",
    "#         print(k,v)\n",
    "#     count += 1\n",
    "#     if count > 200:\n",
    "#         break\n",
    "#     if v.find('tion')>=0:\n",
    "#         print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out tokens that appear in less than 0.05 documents \n",
    "# or more than 0.5 documents\n",
    "# after above, keep only the first 40000 most frequent tokens.\n",
    "\n",
    "#############\n",
    "dict_size = test_dic_sizes[0]\n",
    "dictionary.filter_extremes(no_below=0.08, no_above=0.8, keep_n=dict_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "# Bag-of-words representation of the documents\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in dictionary_made_by]\n",
    "\n",
    "bow_doc_100 = bow_corpus[100]\n",
    "for i in range(len(bow_doc_100)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} times.\".format(bow_doc_100[i][0],\n",
    "                                                     dictionary[bow_doc_100[i][0]],\n",
    "                                                     bow_doc_100[i][1]))\n",
    "\n",
    "processed_docs['bow_corpus'] = bow_corpus\n",
    "processed_docs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "- tf-idf model on bow_corpus\n",
    "\n",
    "- tf = count(word, document) / len(document)\n",
    "- idf = log( len(collection) / count(document_containing_term, collection)\n",
    "- tf-idf = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "\n",
    "tfidf_mod = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf_mod[bow_corpus]\n",
    "\n",
    "processed_docs['tfidf_corpus'] = tfidf_corpus\n",
    "tfidf_doc_100 = tfidf_corpus[100]\n",
    "for i in range(len(tfidf_doc_100)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} times.\".format(tfidf_doc_100[i][0],\n",
    "                                                     dictionary[tfidf_doc_100[i][0]],\n",
    "                                                     tfidf_doc_100[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents bow corpus: %d' % len(bow_corpus))\n",
    "print('Number of documents tfidf: %d' % len(tfidf_corpus))\n",
    "print('Number of documents : %d' % len(dictionary_made_by))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal number of topics\n",
    "- build many LDA models with different values of number of topics and pick the one that gives the highest coherence value based on corpus\n",
    "- 토픽 모델링 결과로 나온 주제들에 대해 각각의 주제에서 상위 N개의 단어를 뽑습니다. 모델링이 잘 되었을수록 한 주제 안에는 의미론적으로 유사한 단어가 많이 모여있게 마련입니다. 따라서 상위 단어 간의 유사도를 계산하면 실제로 해당 주제가 의미론적으로 일치하는 단어들끼리 모여있는지 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_coherence_values(model_type, dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    compute c_v coherence for various number of topics\n",
    "    \n",
    "    parameters:\n",
    "    dictionary : gensim dictionary\n",
    "    corpus : gensim corpus\n",
    "    texts : list of input texts\n",
    "    limit : max number of topics\n",
    "    \n",
    "    returns:\n",
    "    model_list : list of LDA topic models\n",
    "    coherence_values : coherence value\n",
    "    \"\"\"\n",
    "    \n",
    "    coherenece_values = []\n",
    "    u_mass_coherenece_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        if model_type == 'mallet':\n",
    "            model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, \\\n",
    "                                                     num_topics=num_topics, \\\n",
    "                                                     id2word=dictionary, workers=8, \\\n",
    "                                                     iterations=60)\n",
    "        elif model_type == 'online':\n",
    "            model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, \\\n",
    "                                                    num_topics=num_topics, update_every=0, \\\n",
    "                                                    passes=20)\n",
    "        else:\n",
    "            model = gensim.models.LdaMulticore(corpus, num_topics=num_topics, \\\n",
    "                                               id2word=dictionary, passes=2, \\\n",
    "                                               workers=8, iterations=60)\n",
    "        \n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        u_mass_coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        \n",
    "        coherenece_values.append(coherence_model.get_coherence())\n",
    "        u_mass_coherenece_values.append(u_mass_coherence_model.get_coherence())\n",
    "\n",
    "    return model_list, coherenece_values, u_mass_coherenece_values\n",
    "\n",
    "def find_optimal_topic_num(model_type, model_list, coherence_values, umass_co_val):   \n",
    "    #graph\n",
    "    print('======== '+model_type + ' ========')\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    x = range(start, limit, step)\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.plot(x, coherence_values)\n",
    "    ax1.set_xlabel(\"Number of Topics\")\n",
    "    ax1.set_ylabel(\"Coherence score\")\n",
    "    #     plt.legend((\"coherence_values\"), loc=\"best\")\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.plot(x, umass_co_val)\n",
    "    ax2.set_xlabel(\"Number of Topics\")\n",
    "    ax2.set_ylabel(\"u mass Coherence score\")\n",
    "    #     plt.legend((\"u mass coherence_values\"), loc=\"best\")\n",
    "\n",
    "    fig.savefig(directory+'/'+model_type+'_coherence.png')\n",
    "\n",
    "    # Print the coherence scores\n",
    "    # one that maximizes the topic coherence\n",
    "    print('Coherence c_v ================')\n",
    "    max_coherence_topic_num = 8 #default\n",
    "    prev_co = 0\n",
    "    first_max_cohe_found = False\n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        cur_coherence = round(cv, 4)\n",
    "        print(\"Num Topics =\", m, \" has Coherence cv Value of\", cur_coherence)\n",
    "#         if (cur_coherence > 0.32) and m > 5:\n",
    "        if m > 5:\n",
    "            if (prev_co <= cur_coherence) and not first_max_cohe_found:\n",
    "                max_coherence_topic_num = m\n",
    "            else:\n",
    "                first_max_cohe_found = True\n",
    "        prev_co = cur_coherence\n",
    "    print('Best number of topic is : %d' %max_coherence_topic_num)\n",
    "\n",
    "    print('Coherence umass ================')\n",
    "    #where score plateaus안정\n",
    "    for m, um in zip(x, umass_co_val):\n",
    "        print(\"Num Topics =\", m, \" has Coherence umass Value of\", round(um, 4))\n",
    "    \n",
    "    return max_coherence_topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config='dic_'+dictionary_made_by_str+'_size_'+str(len(dictionary))\n",
    "mallet_path = './mallet-2.0.8/bin/mallet'\n",
    "\n",
    "import os\n",
    "directory ='./stat_files/'+model_config\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "limit = 25; start=2; step=1;\n",
    "\n",
    "model_list_on, coherence_values_on, umass_co_val_on = compute_coherence_values(model_type='online', dictionary=dictionary, \\\n",
    "                                                                      corpus=bow_corpus, \\\n",
    "                                                                      texts=processed_trigram, \\\n",
    "                                                                      start=start, \\\n",
    "                                                                      limit=limit, \\\n",
    "                                                                      step=step)\n",
    "max_coherence_topic_num_on= find_optimal_topic_num('online', model_list_on, coherence_values_on, umass_co_val_on)\n",
    "\n",
    "model_list, coherence_values, umass_co_val = compute_coherence_values(model_type='mallet', dictionary=dictionary, \\\n",
    "                                                                      corpus=bow_corpus, \\\n",
    "                                                                      texts=processed_trigram, \\\n",
    "                                                                      start=start, \\\n",
    "                                                                      limit=limit, \\\n",
    "                                                                      step=step)\n",
    "max_coherence_topic_num = find_optimal_topic_num('mallet', model_list, coherence_values, umass_co_val)\n",
    "\n",
    "model_list_tfidf, coherence_values_tfidf, umass_co_val_tfidf = compute_coherence_values(model_type='tfidf', dictionary=dictionary, \\\n",
    "                                                                                              corpus=tfidf_corpus, \\\n",
    "                                                                                              texts=processed_trigram, \\\n",
    "                                                                                              start=start, \\\n",
    "                                                                                              limit=limit, \\\n",
    "                                                                                              step=step)\n",
    "max_coherence_topic_num_tfidf = find_optimal_topic_num('tfidf', model_list_tfidf, coherence_values_tfidf, umass_co_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using bag of words\n",
    "\n",
    "- train LDA using gensim.models.LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=8, id2word=dictionary, passes=2, workers=2)\n",
    "# for idx, topic in lda_model_bow.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n",
    "\n",
    "#is mallet better?\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "dir_to_check = Path(directory+'/data/')\n",
    "if not dir_to_check.is_dir():\n",
    "    os.makedirs(directory+'/data/')\n",
    "    \n",
    "def get_optimal_model(model_type, optimal_topic_num):\n",
    "#     optimal_topic_num = max_coherence_topic_num\n",
    "    if model_type=='mallet':\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path=mallet_path, \\\n",
    "                                                      corpus=bow_corpus, \\\n",
    "                                                      num_topics=optimal_topic_num, \\\n",
    "                                                      id2word=dictionary,\\\n",
    "                                                      iterations=100,\\\n",
    "                                                      prefix=directory+'/data/')\n",
    "    elif model_type=='online':\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus, id2word=dictionary, \\\n",
    "                                                    num_topics=optimal_topic_num, update_every=0, \\\n",
    "                                                    passes=20)\n",
    "    else:\n",
    "        model = gensim.models.LdaMulticore(tfidf_corpus,\\\n",
    "                                             num_topics=optimal_topic_num, \\\n",
    "                                             id2word=dictionary, \\\n",
    "                                             passes=2, \\\n",
    "                                             workers=8,\\\n",
    "                                            iterations=100)\n",
    "        \n",
    "    #show topics\n",
    "    topics = model.show_topics(formatted=False)\n",
    "    pprint(topics)\n",
    "    # lda_mallet.save('./data/mallet_topics_num_'+str(optimal_topic_num)+'_'+model_config+'.state.gz')\n",
    "#     model.load_word_topics()\n",
    "  \n",
    "    fig = plt.figure(figsize=(15,20))\n",
    "    fig.suptitle(model_type+'_topics_num_'+str(optimal_topic_num)+'_'+model_config)\n",
    "\n",
    "    for i in range(optimal_topic_num):\n",
    "        df=pd.DataFrame(model.show_topic(i), columns=['term','prob']).set_index('term')\n",
    "\n",
    "        axi = fig.add_subplot(math.ceil(optimal_topic_num/2),2,i+1)\n",
    "        axi.set_title('topic '+str(i+1))\n",
    "        sns.barplot(x='prob', y=df.index, data=df, palette='Reds_d')\n",
    "        axi.set_xlabel('probability')\n",
    "\n",
    "    # plt.show()\n",
    "    fig.savefig(directory+'/'+model_type+'_topics_num_'+str(optimal_topic_num)+'.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_coherence_topic_num = 8\n",
    "# max_coherence_topic_num_on = 8\n",
    "# max_coherence_topic_num_tfidf = 8\n",
    "\n",
    "mallet_model = get_optimal_model('mallet', max_coherence_topic_num)\n",
    "on_model = get_optimal_model('online', max_coherence_topic_num_on)\n",
    "tfidf_model = get_optimal_model('tfidf', max_coherence_topic_num_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def draw_duplicate_keywords_bar(model_type, optimal_topic_num, duplicate_keywords):\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    x = duplicate_keywords.keys()\n",
    "    x_values = [item[1] for item in duplicate_keywords.items()]\n",
    "    x_pos = [i for i in range(len(x))]\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    plt.bar(x_pos, x_values, color='green')\n",
    "    plt.xlabel(\"Duplicate Keywords\")\n",
    "    plt.ylabel(\"Num Duplication\")\n",
    "    plt.title(\"Duplicate Keywords_\" + model_type)\n",
    "\n",
    "    plt.xticks(x_pos, x)\n",
    "    plt.show()\n",
    "    fig.savefig(directory+'/'+model_type+'_duplicate_keywords'+str(optimal_topic_num)+'.png')\n",
    "    \n",
    "def check_duplicate_keywords(model_type, optimal_topic_num, topics):\n",
    "    all_keywords = {}\n",
    "    for topic in topics:\n",
    "        for t in topic[1]:\n",
    "            keyword = t[0]\n",
    "            if keyword in all_keywords.keys():\n",
    "                all_keywords[keyword] = all_keywords[keyword] + 1\n",
    "            else:\n",
    "                all_keywords[keyword] = 1\n",
    "    \n",
    "    duplicates= dict(filter(lambda x: x[1] > 1, \\\n",
    "           sorted(all_keywords.items(), key=operator.itemgetter(1), reverse=True)))\n",
    "    draw_duplicate_keywords_bar(model_type, optimal_topic_num, duplicates)\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "duplicate_keywords_mallet = check_duplicate_keywords(\"mallet\", max_coherence_topic_num, mallet_model.show_topics(formatted=False))\n",
    "duplicate_keywords_on = check_duplicate_keywords(\"online\", max_coherence_topic_num_on, on_model.show_topics(formatted=False))\n",
    "duplicate_keywords_tfidf = check_duplicate_keywords(\"tfidf\", max_coherence_topic_num_tfidf, tfidf_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_keywords(model_type, topics, duplicate_keywords):\n",
    "#     print(list(map(lambda x: x[1][0], topics)))\n",
    "#     for t in topics:\n",
    "#         print(list(map(lambda x: x[0], t[1])))\n",
    "#     print('=================')\n",
    "    keyword_df = pd.DataFrame(list(map(lambda s: [s[0], list(map(lambda x: x[0], s[1]))], \\\n",
    "                                 topics)))\n",
    "    keyword_df.columns = ['topic', 'keywords']\n",
    "    duplicate_removed = []\n",
    "    for indx, row in keyword_df.iterrows():\n",
    "        duplicate_removed.append(list(filter(lambda x: x not in duplicate_keywords, row.keywords)))\n",
    "\n",
    "    keyword_df['unique_keywords'] = pd.Series(duplicate_removed, index=keyword_df.index)\n",
    "    print(keyword_df)\n",
    "    keyword_df.to_html(directory+'/'+model_type+'_unique_keywords.html', index=False)\n",
    "    \n",
    "get_unique_keywords(\"mallet\", mallet_model.show_topics(formatted=False), duplicate_keywords_mallet)\n",
    "get_unique_keywords(\"online\", on_model.show_topics(formatted=False), duplicate_keywords_on)\n",
    "get_unique_keywords(\"tfidf\", tfidf_model.show_topics(formatted=False), duplicate_keywords_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check topic distribution of a sample data\n",
    "# for ind, score in sorted(lda_mallet[bow_corpus[45]], key=lambda x: -1*x[1]):\n",
    "#     print('\\nScore: {}\\t \\nTopic: {}'.format(score, lda_mallet.print_topic(ind, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "dataDir = directory+'/data/'\n",
    "\n",
    "def extract_params(statefile):\n",
    "    \"\"\"extract alpha and beta values from MALLET statefile by path to statfile\n",
    "    \n",
    "    Args:\n",
    "        statefile (str) : Path to statefile produced by MALLET\n",
    "    Returns:\n",
    "        tuple: alpha (list), beta\n",
    "    \"\"\"\n",
    "    \n",
    "#     with codecs.open(statefile, \"r\",encoding='utf-8') as state:\n",
    "    with gzip.open(statefile, 'r') as state:\n",
    "        params = [x.decode('utf-8').strip() for x in state.readlines()[1:3]]\n",
    "#         params = [x.strip() for x in state.readlines()[1:3]]\n",
    "    return (list(params[0].split(':')[1].split(\" \")), float(params[1].split(':')[1]))\n",
    "\n",
    "def state_to_df(statefile):\n",
    "    \"\"\"transform state file into pandas dataframe\n",
    "    the MALLET statefile is tab-separated, and the first two rows contain the alpha and beta parameters\n",
    "    \n",
    "    Args:\n",
    "        statefile (str): Path to statefile produced by MALLET\n",
    "    Returns:\n",
    "        dataframe: topic assignment for each token in each documnet of the model\n",
    "    \"\"\"\n",
    "    return pd.read_csv(statefile,\\\n",
    "                      compression='gzip',\\\n",
    "                      sep=' ',\\\n",
    "                      skiprows=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = extract_params(os.path.join(dataDir, 'state.mallet.gz'))\n",
    "alpha = [float(x) for x in params[0][1:]]\n",
    "beta = params[1]\n",
    "print(\"{}, {}\".format(alpha, beta))\n",
    "\n",
    "df = state_to_df(os.path.join(dataDir, 'state.mallet.gz'))\n",
    "df['type'] = df.type.astype(str)\n",
    "df[:5]\n",
    "#doc id, word position index, word index, topic assignmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the length of the document, group by the document id and count the tokens\n",
    "docs = df.groupby('#doc')['type'].count().reset_index(name='doc_length')\n",
    "docs[:10]\n",
    "\n",
    "#get vocab and term frequencies\n",
    "vocab = df['type'].value_counts().reset_index()\n",
    "vocab.columns = ['type', 'term_freq']\n",
    "vocab = vocab.sort_values(by='type', ascending=True)\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix file\n",
    "#need to normalize data so that each row sums to 1\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def pivot_and_smooth(df, smooth_values, rows_variable, cols_variable, values_variable):\n",
    "    \"\"\"\n",
    "    modify dataframe into matrix\n",
    "    Args:\n",
    "        df (dataframe) : \n",
    "        smooth_values (float) : value to add to the matrix to account for the priors\n",
    "        rows_variable (str) : title of rows\n",
    "        cols_variable (str) : title of columns\n",
    "        values_variable (str) : values\n",
    "    Returns:\n",
    "        dataframe : that has been normalized on the rows.\n",
    "    \"\"\"\n",
    "    matrix = df.pivot(index=rows_variable, columns=cols_variable, values=values_variable).fillna(value=0)\n",
    "    matrix = matrix.values + smooth_values\n",
    "    \n",
    "    normed = sklearn.preprocessing.normalize(matrix, norm='l1', axis=1)\n",
    "    \n",
    "    return pd.DataFrame(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the number of topic assingments for words in documents\n",
    "#phi - topic-term matrix and counted the number of times each word was assigned to each topic \n",
    "#and sorted by alphabetically to match the order of the vocabulary frame\n",
    "\n",
    "#beta as the smoothign value\n",
    "phi_df = df.groupby(['topic', 'type'])['type'].count().reset_index(name='token_count')\n",
    "phi_df = phi_df.sort_values(by='type', ascending=True)\n",
    "phi_df[:5]\n",
    "\n",
    "phi = pivot_and_smooth(phi_df, beta, 'topic', 'type', 'token_count')\n",
    "phi[:5]\n",
    "\n",
    "#theta document-topic matrix and use alpha as the smoothign value\n",
    "theta_df = df.groupby(['#doc', 'topic'])['topic'].count().reset_index(name='topic_count')\n",
    "theta_df[:5]\n",
    "\n",
    "theta = pivot_and_smooth(theta_df, alpha, '#doc', 'topic', 'topic_count')\n",
    "theta[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mallet_data = {\n",
    "    'topic_term_dists':phi,\n",
    "    'doc_topic_dists':theta,\n",
    "    'doc_lengths':list(docs['doc_length']),\n",
    "    'vocab':list(vocab['type']),\n",
    "    'term_frequency':list(vocab['term_freq'])\n",
    "}\n",
    "mallet_vis_data = pyLDAvis.prepare(**lda_mallet_data)\n",
    "pyLDAvis.display(mallet_vis_data)\n",
    "pyLDAvis.save_html(mallet_vis_data, directory+'/mallet_topics_num_'+str(max_coherence_topic_num)+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each bubble = topic, the larger the bubble, the more prevalent is that topic\n",
    "#good topic = fairly big, non-overlapping bubbles scattered throughout the chart\n",
    "#model with too many topics = typically have many overlaps, small sized bubbles in one region of the chart\n",
    "data_tfidf = pyLDAvis.gensim.prepare(tfidf_model, tfidf_corpus, dictionary)\n",
    "data_tfidf\n",
    "\n",
    "#bar = salient keywords that form the selected topic\n",
    "pyLDAvis.display(data_tfidf)\n",
    "pyLDAvis.save_html(data_tfidf, directory+'/tfidf_topics_num_'+str(max_coherence_topic_num_tfidf)+'.html')\n",
    "\n",
    "data_on = pyLDAvis.gensim.prepare(on_model, bow_corpus, dictionary)\n",
    "data_on\n",
    "\n",
    "pyLDAvis.display(data_on)\n",
    "pyLDAvis.save_html(data_on, directory+'/online_topics_num_'+str(max_coherence_topic_num_on)+'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity and Coherence score\n",
    "- model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute perplexity : a measure of how good the model is\n",
    "# lower the better\n",
    "# print('\\nPerplexit: ', lda_mallet.log_perplexity(bow_corpus))\n",
    "# print('\\nPerplexit: ', lda_model_tfidf.log_perplexity(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#compute coherence score\n",
    "coherence_model_lda = CoherenceModel(model=mallet_model, texts=processed_trigram, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#compute coherence score\n",
    "coherence_model_lda_tfidf = CoherenceModel(model=tfidf_model, texts=processed_trigram, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda_tfidf = coherence_model_lda_tfidf.get_coherence()\n",
    "print('\\nTF-IDF Coherence Score: ', coherence_lda_tfidf)\n",
    "\n",
    "#compute coherence score\n",
    "coherence_model_lda_on = CoherenceModel(model=on_model, texts=processed_trigram, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda_on = coherence_model_lda_on.get_coherence()\n",
    "print('\\nONLINE Coherence Score: ', coherence_lda_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find dominant topic\n",
    "- find the most contributed topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = processed_docs.merge(documents, on='id')\n",
    "# test = processed_docs[['id', 'title_x', 'bow_corpus', 'title_y']]\n",
    "# test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs.iloc[151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def format_topics_sentences(model, \\\n",
    "                            model_type='mallet', \\\n",
    "                            corpus=processed_docs, \\\n",
    "                            texts=dictionary_made_by):\n",
    "    sent_topic_df = pd.DataFrame()\n",
    "    if model_type=='tfidf':\n",
    "        target_corpus = corpus.tfidf_corpus\n",
    "    else:\n",
    "        target_corpus = corpus.bow_corpus\n",
    "    \n",
    "    for i, row in enumerate(model[target_corpus]):\n",
    "        origin_info = processed_docs.iloc[i]\n",
    "        text_vec = texts[i]\n",
    "        #get main topic in each document\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j==0: #dominant topic\n",
    "                wp = model.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "#                 print(pd.Series([origin_info.id,\\\n",
    "#                                 int(topic_num), \\\n",
    "#                                 round(prop_topic, 4), \\\n",
    "#                                 topic_keywords, \\\n",
    "#                                origin_info.title_y, \\\n",
    "#                                text_vec]))\n",
    "                sent_topic_df = sent_topic_df.append(pd.Series([origin_info.id,\\\n",
    "                                                                int(topic_num), \\\n",
    "                                                                round(prop_topic, 4), \\\n",
    "                                                                topic_keywords, \\\n",
    "                                                               origin_info.title_y, \\\n",
    "                                                               text_vec]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topic_df.columns = ['id', 'Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Origin_Text', 'Text_Vec']\n",
    "    \n",
    "    return (sent_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_mallet = format_topics_sentences(model=mallet_model, \\\n",
    "                                                  corpus=processed_docs, \\\n",
    "                                                  texts=dictionary_made_by, \\\n",
    "                                                 model_type='mallet')\n",
    "df_topic_sents_keywords_mallet.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_tfidf = format_topics_sentences(model=tfidf_model, \\\n",
    "                                                  corpus=processed_docs, \\\n",
    "                                                  texts=dictionary_made_by, \\\n",
    "                                                 model_type='tfidf')\n",
    "df_topic_sents_keywords_tfidf.head(max_coherence_topic_num_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_on = format_topics_sentences(model=on_model, \\\n",
    "                                                  corpus=processed_docs, \\\n",
    "                                                  texts=dictionary_made_by, \\\n",
    "                                                 model_type='online')\n",
    "df_topic_sents_keywords_on.head(max_coherence_topic_num_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_topic_sents_keywords_mallet.isna().any())\n",
    "print('=================')\n",
    "print(df_topic_sents_keywords_tfidf.isna().any())\n",
    "print('=================')\n",
    "print(df_topic_sents_keywords_on.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 토픽별로 가장 대표적인 문서 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_sents(keywords, model_type):\n",
    "    sent_topics_sorted_df_mallet = pd.DataFrame()\n",
    "    \n",
    "    sent_topics_groupby = keywords.groupby('Dominant_Topic')\n",
    "\n",
    "    keywords_top = pd.DataFrame()\n",
    "    for i, grp in sent_topics_groupby:\n",
    "        keywords_top = pd.concat([keywords_top, \\\n",
    "                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(20)], \\\n",
    "                            axis=0)\n",
    "        keywords = pd.concat([keywords, \\\n",
    "                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(20)], \\\n",
    "                            axis=0)\n",
    "\n",
    "    keywords_top.reset_index(drop=True, inplace=True)\n",
    "    keywords_top.columns = ['id', 'Topic_Num', 'Topic_Perc_Contribu', 'Topic_Keywords', 'Origin_Text', 'Text']\n",
    "    keywords_top.to_csv(directory+'/'+model_type+'_top_sen.tsv', sep='\\t')\n",
    "    \n",
    "    keywords.reset_index(drop=True, inplace=True)\n",
    "    keywords.columns = ['id', 'Topic_Num', 'Topic_Perc_Contribu', 'Topic_Keywords', 'Origin_Text', 'Text']\n",
    "    \n",
    "    return keywords\n",
    "    \n",
    "df_topic_sents_mallet = get_rep_sents(df_topic_sents_keywords_mallet, 'mallet')\n",
    "df_topic_sents_tfidf = get_rep_sents(df_topic_sents_keywords_tfidf, 'tfidf')\n",
    "df_topic_sents_on = get_rep_sents(df_topic_sents_keywords_on, 'online')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords_mallet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_mallet.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_tfidf.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서 전체적인 토픽 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df_topic_sents_mallet['Topic_Num'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "topic_num_keywords = df_topic_sents_mallet[['Topic_Num', 'Topic_Keywords']]\n",
    "\n",
    "df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "df_dominant_topics.columns = ['Topic_Num', 'Num_Documents', 'Perc_Documents']\n",
    "df_dominant_topics = df_dominant_topics.merge(topic_num_keywords.drop_duplicates(), on='Topic_Num')\n",
    "df_dominant_topics = df_dominant_topics.sort_values('Topic_Num')\n",
    "\n",
    "df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "df_dominant_topics.to_html(directory+'/mallet_distribution.html', index=False)\n",
    "df_dominant_topics.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df_topic_sents_tfidf['Topic_Num'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "topic_num_keywords = df_topic_sents_tfidf[['Topic_Num', 'Topic_Keywords']]\n",
    "\n",
    "df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "df_dominant_topics.columns = ['Topic_Num', 'Num_Documents', 'Perc_Documents']\n",
    "df_dominant_topics = df_dominant_topics.merge(topic_num_keywords.drop_duplicates(), on='Topic_Num')\n",
    "df_dominant_topics = df_dominant_topics.sort_values('Topic_Num')\n",
    "\n",
    "df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "df_dominant_topics.to_html(directory+'/tfidf_distribution.html', index=False)\n",
    "df_dominant_topics.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df_topic_sents_on['Topic_Num'].value_counts()\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "topic_num_keywords = df_topic_sents_on[['Topic_Num', 'Topic_Keywords']]\n",
    "\n",
    "df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "df_dominant_topics.reset_index(level=0, inplace=True)\n",
    "df_dominant_topics.columns = ['Topic_Num', 'Num_Documents', 'Perc_Documents']\n",
    "df_dominant_topics = df_dominant_topics.merge(topic_num_keywords.drop_duplicates(), on='Topic_Num')\n",
    "df_dominant_topics = df_dominant_topics.sort_values('Topic_Num')\n",
    "\n",
    "df_dominant_topics.reset_index(drop = True, inplace = True)\n",
    "df_dominant_topics.to_html(directory+'/on_distribution.html', index=False)\n",
    "df_dominant_topics.head(max_coherence_topic_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://ai.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html\n",
    "- http://projector.tensorflow.org/\n",
    "- http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb#loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
