{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster a set of documents using Python\n",
    "\n",
    "1. tokenizing\n",
    "2. stemming (*based on stemming lib, results change)- reduce a word to its stem or root form\n",
    "3. calculate cosine distance between each document = measure of similarity\n",
    "4. cluster documents using the k-means algorithm\n",
    "5. using multidimensional scaling to reduce dimensionality within the corpus\n",
    "6. conduct a hierarchical clustering on the corpus using Ward clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References :\n",
    "- https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import psycopg2\n",
    "# import db_conn\n",
    "from IPython.display import display\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = get_connection()\n",
    "\n",
    "def get_article_tables(is_file):\n",
    "    if is_file:\n",
    "        return pd.read_csv('../data/titles_condition_by_t.tsv', sep='\\t', header=None)\n",
    "#         return pd.read_csv('../topic_modeling/best_files/dic_unigram_size_6000/mallet_top_sen.tsv', sep='\\t')\n",
    "    else:\n",
    "        curs = conn.cursor()\n",
    "\n",
    "        select_sql = \"\"\"SELECT id, table_title, strip_tags(CONTENT) as content FROM article_tables order by id\"\"\" # limit 10000\n",
    "        curs.execute(select_sql)\n",
    "        return curs.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_article_tables(True)\n",
    "# train_data = train_data[['id', 'Origin_Text']]\n",
    "train_data.columns=['id', 'title']\n",
    "# clean_content = [x['content'].lower() for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>Analysis of efficacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>Comparisons of postoperative CA19-9 levels on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>Pattern of disease relapse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>Grade 1–5 adverse events with gemcitabine alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>Treatment with zoledronic acid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              title\n",
       "0  4106                               Analysis of efficacy\n",
       "1  4107  Comparisons of postoperative CA19-9 levels on ...\n",
       "2  4108                         Pattern of disease relapse\n",
       "3  4109  Grade 1–5 adverse events with gemcitabine alon...\n",
       "4  4112                     Treatment with zoledronic acid"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       False\n",
      "title    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "train_data.title = train_data.title.str.strip()\n",
    "train_data['title'].replace('', np.nan, inplace=True)\n",
    "print(train_data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       False\n",
      "title    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "train_data.loc[train_data.title.isna()]\n",
    "train_data.dropna(subset=['title'], inplace=True)\n",
    "print(train_data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep = {'nbsp':'', 'table':'', 'legend':'', 'mg/dl':'', 'g/l':'', 'yrs':'year', '\\n':' ', ';':'', 'kg/m2':'', 'n=':''}\n",
    "rep = {'nbsp':'', 'table':'', 'legend':'', 'yrs':'year', '\\n':' '}\n",
    "# clean_content = [pattern.sub(lambda m: rep[re.escape(m.group(0))], x['content']) for x in train_data]\n",
    "rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "train_data.title = [pattern.sub(lambda m: rep[re.escape(m.group(0))], str(x)) for x in train_data.title]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 Analysis of efficacy\n",
       "1    Comparisons of postoperative CA19-9 levels on ...\n",
       "2                           Pattern of disease relapse\n",
       "3    Grade 1–5 adverse events with gemcitabine alon...\n",
       "4                       Treatment with zoledronic acid\n",
       "5                             Treatment with docetaxel\n",
       "6    Treatments ever used at relapse, at the discre...\n",
       "7    Worst adverse event  (grade)  reported over en...\n",
       "8    Chemotherapy delivery and trial drug discontin...\n",
       "9                                       Adverse events\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.title[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/grace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords\n",
    "stemmer = SnowballStemmer('english')\n",
    "STOP_WORDS = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "STOP_WORDS.extend(['table', 'legend'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "# -porter stemmer\n",
    "# -lancaster stemmer\n",
    "# -snowball stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "#     deacc=True removes punctuations\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True):\n",
    "        if token not in STOP_WORDS and len(token)>1:\n",
    "#             result.append(lemmatize_stemming(strip_numeric(token)))\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def preprocess_token_only(text):\n",
    "    result = []\n",
    "#     deacc=True removes punctuations\n",
    "    for token in gensim.utils.simple_preprocess(text, deacc=True):\n",
    "        if token not in STOP_WORDS and len(token)>1:\n",
    "#             result.append(lemmatize_stemming(strip_numeric(token)))\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "#tokenizing\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    #filter tokens not containing letters\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token)>2:\n",
    "            filtered.append(token)\n",
    "#     stems = [stemmer.stem(t, pos='v') for t in filtered]\n",
    "    stems = [stemmer.stem(t) for t in filtered]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and len(token)>2:\n",
    "            filtered.append(token)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_stemmed = []\n",
    "total_vocab_tokenized = []\n",
    "\n",
    "for i in train_data.title.tolist():\n",
    "    all_stemmed = preprocess(i)\n",
    "    total_vocab_stemmed.extend(all_stemmed)\n",
    "    \n",
    "    all_tokenized = preprocess_token_only(i)\n",
    "    total_vocab_tokenized.extend(all_tokenized)\n",
    "\n",
    "processed_docs = pd.DataFrame()\n",
    "processed_docs = pd.concat([train_data.id, train_data.title.map(preprocess)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4106</td>\n",
       "      <td>[analys, eff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4107</td>\n",
       "      <td>[comparison, postop, ca, level, surv, espac, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108</td>\n",
       "      <td>[pattern, diseas, relaps]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4109</td>\n",
       "      <td>[grad, advers, ev, gemcitabin, gemcitabin, plu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4112</td>\n",
       "      <td>[tre, zoledron, acid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4113</td>\n",
       "      <td>[tre, docetaxel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4114</td>\n",
       "      <td>[tre, relaps, discret, tre, clin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4115</td>\n",
       "      <td>[worst, advers, ev, grad, report, entir, tim, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4117</td>\n",
       "      <td>[chemotherapy, delivery, tri, drug, discontinu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4118</td>\n",
       "      <td>[advers, ev]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              title\n",
       "0  4106                                      [analys, eff]\n",
       "1  4107  [comparison, postop, ca, level, surv, espac, c...\n",
       "2  4108                          [pattern, diseas, relaps]\n",
       "3  4109  [grad, advers, ev, gemcitabin, gemcitabin, plu...\n",
       "4  4112                              [tre, zoledron, acid]\n",
       "5  4113                                   [tre, docetaxel]\n",
       "6  4114                  [tre, relaps, discret, tre, clin]\n",
       "7  4115  [worst, advers, ev, grad, report, entir, tim, ...\n",
       "8  4117    [chemotherapy, delivery, tri, drug, discontinu]\n",
       "9  4118                                       [advers, ev]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>analys</th>\n",
       "      <td>analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eff</th>\n",
       "      <td>efficacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comparison</th>\n",
       "      <td>comparisons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postop</th>\n",
       "      <td>postoperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    words\n",
       "analys           analysis\n",
       "eff              efficacy\n",
       "comparison    comparisons\n",
       "postop      postoperative\n",
       "ca                     ca"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe with stemmed vocab and tokenized words (link)\n",
    "vocab_frame = pd.DataFrame({'words':total_vocab_tokenized}, index=total_vocab_stemmed)\n",
    "vocab_frame.drop_duplicates(inplace=True)\n",
    "vocab_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf and document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.62 s, sys: 73.7 ms, total: 9.69 s\n",
      "Wall time: 9.72 s\n",
      "(27960, 147)\n"
     ]
    }
   ],
   "source": [
    "#frequency-inverse document frequencey(tf-idf) vectorize parameters and convert the document list into tf-idf matrix\n",
    "# 1. count word occurrences by document\n",
    "# 2. transform into a document-term matrix = term frequency matrix\n",
    "\n",
    "#max_df = max frequency within the documents\n",
    "#min_idf = if 5, the term would have to be in at least 5 of the documents to be considered, 0.2 = 20% of documents\n",
    "#ngram_ranges\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                                   max_features=10000, \n",
    "                                   min_df=0.01, \n",
    "                                   stop_words='english', \n",
    "                                   use_idf=True, \n",
    "                                   lowercase=True, \n",
    "                                   tokenizer=preprocess)\n",
    "#                                    tokenizer=preprocess, ngram_range=(1,2))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(train_data.title)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accord',\n",
       " 'act',\n",
       " 'acut',\n",
       " 'adjust',\n",
       " 'advers',\n",
       " 'ag',\n",
       " 'analys',\n",
       " 'angiograph',\n",
       " 'artery',\n",
       " 'assess']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 58s, sys: 45 s, total: 16min 43s\n",
      "Wall time: 16min 46s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_tsne_result = TSNE(learning_rate=300, init='pca')\\\n",
    "                    .fit_transform(np.array(tfidf_matrix.toarray()))\n",
    "tfidf_vect = normalize(tfidf_tsne_result, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist = cosine similarity of each document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "         8.53883514e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.00000000e+00, -2.22044605e-16,  1.00000000e+00, ...,\n",
       "         5.88511646e-01,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
       "       ...,\n",
       "       [ 8.53883514e-01,  5.88511646e-01,  1.00000000e+00, ...,\n",
       "         0.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "         1.00000000e+00, -2.22044605e-16,  1.00000000e+00],\n",
       "       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
       "         1.00000000e+00,  1.00000000e+00, -2.22044605e-16]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer, CountVectorizer\n",
    "\n",
    "- 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩한 벡터를 만든다.\n",
    "- HashingVectorizer를 사용하면 해시 함수를 사용하여 단어에 대한 인덱스 번호를 생성하기 때문에 메모리 및 실행 시간을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accord': 0,\n",
       " 'act': 1,\n",
       " 'acut': 2,\n",
       " 'adjust': 3,\n",
       " 'advers': 4,\n",
       " 'ag': 5,\n",
       " 'analys': 6,\n",
       " 'angiograph': 7,\n",
       " 'artery': 8,\n",
       " 'assess': 9,\n",
       " 'assocy': 10,\n",
       " 'bas': 11,\n",
       " 'blood': 12,\n",
       " 'cardiac': 13,\n",
       " 'cardiovascul': 14,\n",
       " 'cas': 15,\n",
       " 'categ': 16,\n",
       " 'caus': 17,\n",
       " 'chang': 18,\n",
       " 'childr': 19,\n",
       " 'class': 20,\n",
       " 'clin': 21,\n",
       " 'cohort': 22,\n",
       " 'combin': 23,\n",
       " 'comp': 24,\n",
       " 'comparison': 25,\n",
       " 'comply': 26,\n",
       " 'cont': 27,\n",
       " 'control': 28,\n",
       " 'coron': 29,\n",
       " 'correl': 30,\n",
       " 'country': 31,\n",
       " 'cox': 32,\n",
       " 'dat': 33,\n",
       " 'day': 34,\n",
       " 'dea': 35,\n",
       " 'death': 36,\n",
       " 'diagnos': 37,\n",
       " 'diff': 38,\n",
       " 'diseas': 39,\n",
       " 'distribut': 40,\n",
       " 'dos': 41,\n",
       " 'drug': 42,\n",
       " 'eff': 43,\n",
       " 'effect': 44,\n",
       " 'end': 45,\n",
       " 'endpoint': 46,\n",
       " 'estim': 47,\n",
       " 'ev': 48,\n",
       " 'exerc': 49,\n",
       " 'fact': 50,\n",
       " 'fail': 51,\n",
       " 'flow': 52,\n",
       " 'follow': 53,\n",
       " 'frequ': 54,\n",
       " 'funct': 55,\n",
       " 'gen': 56,\n",
       " 'group': 57,\n",
       " 'hazard': 58,\n",
       " 'heal': 59,\n",
       " 'heart': 60,\n",
       " 'hemodynam': 61,\n",
       " 'high': 62,\n",
       " 'hospit': 63,\n",
       " 'incid': 64,\n",
       " 'ind': 65,\n",
       " 'independ': 66,\n",
       " 'index': 67,\n",
       " 'individ': 68,\n",
       " 'infarct': 69,\n",
       " 'infect': 70,\n",
       " 'interv': 71,\n",
       " 'leav': 72,\n",
       " 'level': 73,\n",
       " 'log': 74,\n",
       " 'maj': 75,\n",
       " 'mean': 76,\n",
       " 'meas': 77,\n",
       " 'med': 78,\n",
       " 'model': 79,\n",
       " 'mon': 80,\n",
       " 'month': 81,\n",
       " 'mort': 82,\n",
       " 'mult': 83,\n",
       " 'multivary': 84,\n",
       " 'myocard': 85,\n",
       " 'non': 86,\n",
       " 'numb': 87,\n",
       " 'od': 88,\n",
       " 'op': 89,\n",
       " 'outcom': 90,\n",
       " 'overal': 91,\n",
       " 'paramet': 92,\n",
       " 'particip': 93,\n",
       " 'paty': 94,\n",
       " 'period': 95,\n",
       " 'point': 96,\n",
       " 'pop': 97,\n",
       " 'post': 98,\n",
       " 'predict': 99,\n",
       " 'pres': 100,\n",
       " 'press': 101,\n",
       " 'prev': 102,\n",
       " 'prim': 103,\n",
       " 'proc': 104,\n",
       " 'proport': 105,\n",
       " 'random': 106,\n",
       " 'rat': 107,\n",
       " 'ratio': 108,\n",
       " 'reg': 109,\n",
       " 'regress': 110,\n",
       " 'rel': 111,\n",
       " 'report': 112,\n",
       " 'respons': 113,\n",
       " 'result': 114,\n",
       " 'risk': 115,\n",
       " 'saf': 116,\n",
       " 'scor': 117,\n",
       " 'second': 118,\n",
       " 'select': 119,\n",
       " 'sensit': 120,\n",
       " 'sev': 121,\n",
       " 'sex': 122,\n",
       " 'sign': 123,\n",
       " 'spec': 124,\n",
       " 'stat': 125,\n",
       " 'stratify': 126,\n",
       " 'stroke': 127,\n",
       " 'study': 128,\n",
       " 'subgroup': 129,\n",
       " 'subject': 130,\n",
       " 'sum': 131,\n",
       " 'surv': 132,\n",
       " 'test': 133,\n",
       " 'therapy': 134,\n",
       " 'tim': 135,\n",
       " 'tot': 136,\n",
       " 'tre': 137,\n",
       " 'tri': 138,\n",
       " 'typ': 139,\n",
       " 'valu': 140,\n",
       " 'vary': 141,\n",
       " 'ventricul': 142,\n",
       " 'vers': 143,\n",
       " 'week': 144,\n",
       " 'wom': 145,\n",
       " 'year': 146}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_df=0.8, \\\n",
    "                       min_df=0.01, \\\n",
    "                       max_features=10000,\\\n",
    "                       lowercase=True,\\\n",
    "                       tokenizer=preprocess,\\\n",
    "                       stop_words='english')\n",
    "# vect = HashingVectorizer(n_features=10000,\\\n",
    "#                        lowercase=True,\\\n",
    "#                        tokenizer=preprocess,\\\n",
    "#                        stop_words='english')\n",
    "\n",
    "# vect = CountVectorizer(max_df=0.8, min_df=0.02, ngram_range=(2,2))\n",
    "count_matrix = vect.fit_transform(train_data.title)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 1s, sys: 49.1 s, total: 16min 50s\n",
      "Wall time: 16min 52s\n"
     ]
    }
   ],
   "source": [
    "%time count_tsne_result = TSNE(learning_rate=300, init='pca')\\\n",
    "                    .fit_transform(np.array(count_matrix.toarray()))\n",
    "count_vect = normalize(count_tsne_result, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means- predetermined number of clusters\n",
    "nums =[8]\n",
    "from sklearn.cluster import KMeans\n",
    "from __future__ import print_function\n",
    "\n",
    "def run_kmeans(vect):\n",
    "    for num in nums:\n",
    "        print('cluster : %s' % str(num))\n",
    "        num_clusters = num\n",
    "        km = KMeans(n_clusters=num_clusters,\\\n",
    "                    random_state=0,\\\n",
    "                    init='random',\\\n",
    "                    algorithm='auto',\\\n",
    "                    max_iter=30000)\n",
    "        %time km.fit(vect)\n",
    "        clusters = km.labels_.tolist()\n",
    "\n",
    "        documents = {'id':[x for x in processed_docs.id],\n",
    "                    'content': train_data.title.tolist(),\n",
    "                     'title': processed_docs.title.tolist(),\n",
    "                    'cluster':clusters}\n",
    "\n",
    "        clu_docu = pd.DataFrame(documents, index=[clusters], columns=['id','content','title','cluster'])\n",
    "\n",
    "        print(clu_docu['cluster'].value_counts())\n",
    "\n",
    "        #top words nearest to the cluster centroid\n",
    "        print('Top terms per clusters')\n",
    "        print()\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "        for i in range(num_clusters):\n",
    "            print('Cluster %d words:' % i, end='')\n",
    "\n",
    "            for ind in order_centroids[i, :]:\n",
    "                print('%s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "            print()\n",
    "\n",
    "            print(\"Cluster %d titles:\" % i, end='')\n",
    "            adver_contents = [x for x in clu_docu.loc[i]['content'].tolist() if x.lower().find('advers')>=0]\n",
    "            print('count of adverse included in content %s' % str(len(adver_contents)))\n",
    "\n",
    "        return clu_docu, km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster : 8\n",
      "CPU times: user 677 ms, sys: 45.9 ms, total: 723 ms\n",
      "Wall time: 764 ms\n",
      "2    4303\n",
      "5    3545\n",
      "6    3515\n",
      "3    3495\n",
      "4    3373\n",
      "0    3320\n",
      "1    3281\n",
      "7    3128\n",
      "Name: cluster, dtype: int64\n",
      "Top terms per clusters\n",
      "\n",
      "Cluster 0 words:activation,according,\n",
      "Cluster 0 titles:count of adverse included in content 4\n",
      "Cluster 1 words:according,activation,\n",
      "Cluster 1 titles:count of adverse included in content 3\n",
      "Cluster 2 words:activation,according,\n",
      "Cluster 2 titles:count of adverse included in content 36\n",
      "Cluster 3 words:activation,according,\n",
      "Cluster 3 titles:count of adverse included in content 13\n",
      "Cluster 4 words:according,activation,\n",
      "Cluster 4 titles:count of adverse included in content 6\n",
      "Cluster 5 words:activation,according,\n",
      "Cluster 5 titles:count of adverse included in content 1020\n",
      "Cluster 6 words:according,activation,\n",
      "Cluster 6 titles:count of adverse included in content 108\n",
      "Cluster 7 words:according,activation,\n",
      "Cluster 7 titles:count of adverse included in content 7\n"
     ]
    }
   ],
   "source": [
    "tfidf_clu, tfidf_centers = run_kmeans(tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster : 8\n",
      "CPU times: user 756 ms, sys: 9.62 ms, total: 765 ms\n",
      "Wall time: 766 ms\n",
      "2    4117\n",
      "0    4098\n",
      "1    3484\n",
      "4    3479\n",
      "6    3418\n",
      "3    3314\n",
      "5    3093\n",
      "7    2957\n",
      "Name: cluster, dtype: int64\n",
      "Top terms per clusters\n",
      "\n",
      "Cluster 0 words:activation,according,\n",
      "Cluster 0 titles:count of adverse included in content 13\n",
      "Cluster 1 words:activation,according,\n",
      "Cluster 1 titles:count of adverse included in content 1098\n",
      "Cluster 2 words:activation,according,\n",
      "Cluster 2 titles:count of adverse included in content 53\n",
      "Cluster 3 words:activation,according,\n",
      "Cluster 3 titles:count of adverse included in content 10\n",
      "Cluster 4 words:according,activation,\n",
      "Cluster 4 titles:count of adverse included in content 3\n",
      "Cluster 5 words:according,activation,\n",
      "Cluster 5 titles:count of adverse included in content 0\n",
      "Cluster 6 words:according,activation,\n",
      "Cluster 6 titles:count of adverse included in content 5\n",
      "Cluster 7 words:activation,according,\n",
      "Cluster 7 titles:count of adverse included in content 15\n"
     ]
    }
   ],
   "source": [
    "count_clu = run_kmeans(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27960, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_clu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27960, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_clu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./km_result.pkl', 'wb') as f:\n",
    "    pickle.dump((tfidf_clu, tfidf_centers), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
